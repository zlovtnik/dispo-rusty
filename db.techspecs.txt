Technical Specifications
1. Introduction
1.1 Executive Summary
1.1.1 Brief Overview Of The Project
This technical specification outlines the design and implementation of a multi-tenant NFe (Nota Fiscal Eletrônica) management system built with Rust and Diesel ORM. The system implements a schema-per-tenant architecture using PostgreSQL, where each tenant receives a dedicated database schema containing the complete NFe data structure. While Citus has long embraced the tenant discriminator approach with Citus 12 there is new support for schema based sharding. Coupled with the latest release of PgBouncer, you also have better connection management and scaling without having to give up schema based sharding.

1.1.2 Core Business Problem Being Solved
The system addresses the critical need for secure, isolated NFe data management across multiple tenants while maintaining compliance with Brazilian fiscal regulations. Traditional shared-table approaches pose significant risks for fiscal data, where Major downside of this approach is that it requires a very close attention to multi-tenancy in the data access layer implementation of services / micro-services. An inexperienced developer can easily make a mistake of not adding the 'WHERE tenant_id=<...>' to the query and impacting data of multiple tenants.

1.1.3 Key Stakeholders And Users
Stakeholder Group	Primary Interests	Access Level
Tenant Organizations	Data isolation, compliance, performance	Full tenant access
System Administrators	Multi-tenant management, monitoring	Cross-tenant admin
Developers	Schema management, deployment	Development environment
Compliance Officers	Audit trails, data security	Read-only compliance
1.1.4 Expected Business Impact And Value Proposition
The schema-per-tenant approach provides maximum data isolation while enabling efficient resource utilization. tenant-schema is best when you have a relatively small number of fairly large tenants. An example of this would be an accounting application, with only paid subscription users. This architecture ensures complete compliance with fiscal data regulations while providing scalable multi-tenancy capabilities.

1.2 System Overview
1.2.1 Project Context
Business Context And Market Positioning
The NFe management system operates within Brazil's mandatory electronic invoicing ecosystem, where businesses must generate, validate, and store fiscal documents according to SEFAZ (State Treasury Department) specifications. The multi-tenant architecture enables SaaS providers to serve multiple organizations while maintaining strict data isolation requirements mandated by fiscal regulations.

Current System Limitations
Traditional multi-tenant approaches using shared tables with tenant discriminators present significant risks in fiscal document management, including potential data leakage between tenants and complex query filtering requirements that increase the likelihood of compliance violations.

Integration With Existing Enterprise Landscape
The system integrates with existing PostgreSQL infrastructure and leverages Rust's type safety with Diesel ORM for compile-time query validation. We use Postgres namespaces ('schemas') heavily, and it would be great if Diesel had support for a typical multi-tenancy setup where the structure of tables is known at compile time, but the namespace for the tables is generated dynamically.

1.2.2 High-level Description
Primary System Capabilities
Dynamic Schema Creation: Automatic generation of tenant-specific PostgreSQL schemas containing the complete NFe data structure
Tenant Validation: Connection validation and schema provisioning upon tenant registration
Fiscal Compliance: Complete implementation of NFe 4.00 specification with all required tables and relationships
Multi-Tenant Security: Schema-level isolation ensuring complete data separation between tenants
Major System Components
Component	Technology	Purpose
Schema Generator	Rust + Diesel	Dynamic tenant schema creation
Connection Manager	PostgreSQL	Tenant-specific connection handling
Migration Engine	Diesel Migrations	Schema deployment and updates
Validation Service	Rust	Tenant connection validation
Core Technical Approach
The system utilizes Schema isolation in PostgreSQL provides an elegant middle ground for multi-tenant microservices architectures. It balances isolation and resource efficiency while maintaining operational simplicity. Each tenant receives a dedicated PostgreSQL schema with the complete NFe table structure, enabling secure data isolation while maintaining operational efficiency.

1.2.3 Success Criteria
Measurable Objectives
Metric	Target	Measurement Method
Schema Creation Time	< 30 seconds	Automated timing
Tenant Isolation	100% data separation	Security audit
Connection Validation	< 5 seconds	Performance monitoring
Migration Success Rate	> 99.5%	Deployment tracking
Critical Success Factors
Complete data isolation between tenants
Automated schema provisioning without manual intervention
Seamless integration with existing Diesel-based applications
Compliance with NFe 4.00 specification requirements
Key Performance Indicators (kpis)
Tenant Onboarding Time: Time from tenant registration to schema availability
Schema Consistency: Percentage of schemas matching the reference structure
Connection Pool Efficiency: Resource utilization across tenant connections
Compliance Coverage: Percentage of NFe requirements implemented
1.3 Scope
1.3.1 In-scope
Core Features And Functionalities
Complete NFe 4.00 Schema Implementation: All tables, enums, and relationships as specified in the provided SQL schema
Dynamic Tenant Schema Creation: Automated generation of tenant-specific schemas upon registration
Connection Validation: Verification of tenant database connections before schema creation
Migration Management: Deployment and versioning of schema changes across tenants
Diesel Integration: Full compatibility with Rust Diesel ORM for type-safe database operations
Implementation Boundaries
Boundary Type	Included	Details
Database Systems	PostgreSQL 12+	Primary database platform
Programming Languages	Rust	Backend implementation
ORM Framework	Diesel 2.2+	Database abstraction layer
Schema Scope	NFe 4.00 Complete	All fiscal document tables
User Groups Covered
Tenant Administrators: Schema management and monitoring
Application Developers: Diesel-based application development
System Operators: Multi-tenant infrastructure management
Geographic/market Coverage
Primary Market: Brazil (NFe compliance requirements)
Regulatory Scope: SEFAZ specifications and fiscal document standards
Language Support: Portuguese (fiscal document terminology)
Data Domains Included
Fiscal Documents: Complete NFe structure including items, taxes, and references
Company Information: Emitters, recipients, and related entities
Product Catalog: Items, classifications, and tax information
Tax Calculations: ICMS, IPI, PIS/COFINS, and other Brazilian taxes
1.3.2 Out-of-scope
Explicitly Excluded Features/capabilities
Multi-Database Support: Only PostgreSQL schemas, not separate databases per tenant
Real-time Synchronization: No cross-tenant data replication or synchronization
Legacy Migration Tools: No automated migration from existing single-tenant systems
Custom Schema Modifications: No tenant-specific schema customizations beyond the standard NFe structure
Future Phase Considerations
Performance Optimization: Advanced indexing strategies and query optimization
Monitoring Dashboard: Tenant-specific performance and usage analytics
Backup Automation: Tenant-specific backup and recovery procedures
Cross-Tenant Reporting: Aggregated analytics across multiple tenants
Integration Points Not Covered
External APIs: SEFAZ integration and third-party fiscal services
Authentication Systems: User management and access control
File Storage: XML document storage and management
Notification Services: Email and webhook notifications
Unsupported Use Cases
Shared Data Requirements: Cross-tenant data sharing or aggregation
Custom Tax Rules: Tenant-specific tax calculation modifications
Non-NFe Documents: Other Brazilian fiscal document types (CTe, MDFe, etc.)
International Compliance: Non-Brazilian fiscal document standards
2. Product Requirements
2.1 Feature Catalog
2.1.1 Core Multi-tenant Schema Management Features
| Feature ID | Feature Name | Category | Priority | Status |
|---|---|---|---|
| F-001 | Tenant Registration & Validation | Core Infrastructure | Critical | Proposed |
| F-002 | Dynamic Schema Creation | Schema Management | Critical | Proposed |
| F-003 | Connection Pool Management | Infrastructure | High | Proposed |
| F-004 | Schema Migration Engine | Schema Management | High | Proposed |

F-001: Tenant Registration & Validation
Description

Overview: Provides support for a typical multi-tenancy setup where the structure of tables is known at compile time, but the namespace for the tables is generated dynamically
Business Value: Ensures secure tenant onboarding with proper connection validation before schema provisioning
User Benefits: Automated tenant setup with validation prevents invalid configurations and ensures data isolation
Technical Context: Schema-per-tenant is best when you have a relatively small number of fairly large tenants, such as an accounting application with only paid subscription users
Dependencies

System Dependencies: PostgreSQL 12+, Rust Diesel ORM 2.2+
External Dependencies: Valid PostgreSQL connection credentials
Integration Requirements: Connection to tenant management system
F-002: Dynamic Schema Creation
Description

Overview: Provides tools to work with cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder
Business Value: Enables automated NFe schema deployment across multiple tenants without manual intervention
User Benefits: Complete NFe 4.00 compliance with automated schema provisioning
Technical Context: Citus 12 introduced new support for schema based sharding, coupled with the latest release of PgBouncer for better connection management and scaling
Dependencies

Prerequisite Features: F-001 (Tenant Registration & Validation)
System Dependencies: diesel-dynamic-schema crate for runtime schema definition
Integration Requirements: NFe 4.00 SQL schema definition
F-003: Connection Pool Management
Description

Overview: Manages tenant-specific database connections with proper isolation and resource optimization
Business Value: Better for connection pooling, as all connections can use the same pool while maintaining tenant isolation
User Benefits: Efficient resource utilization and scalable connection management
Technical Context: Diesel uses prepared statements internally and caches them for performance, requiring careful connection management to avoid cross-tenant data access
Dependencies

Prerequisite Features: F-001 (Tenant Registration & Validation)
System Dependencies: PostgreSQL connection pooling, Diesel connection management
Integration Requirements: Tenant-specific connection strings and credentials
F-004: Schema Migration Engine
Description

Overview: Provides the embed_migrations! macro, allowing you to embed migration scripts in the final binary and run migrations every time the application starts
Business Value: Ensures schema consistency across all tenant schemas with automated migration deployment
User Benefits: Seamless schema updates without manual intervention across multiple tenants
Technical Context: Schema changes can be deployed as a gradual rollout across tenants rather than universal changes
Dependencies

Prerequisite Features: F-002 (Dynamic Schema Creation)
System Dependencies: Diesel migrations framework, embedded migration scripts
Integration Requirements: NFe schema migration scripts
2.2 Functional Requirements Table
2.2.1 F-001: Tenant Registration & Validation Requirements
| Requirement ID | Description | Acceptance Criteria | Priority | Complexity |
|---|---|---|---|
| F-001-RQ-001 | Validate tenant database connection | Connection established within 5 seconds with proper credentials | Must-Have | Medium |
| F-001-RQ-002 | Verify schema namespace availability | Unique schema name validation before creation | Must-Have | Low |
| F-001-RQ-003 | Store tenant configuration metadata | Persistent storage of tenant connection parameters | Must-Have | Low |
| F-001-RQ-004 | Handle connection failures gracefully | Proper error handling and retry mechanisms | Should-Have | Medium |

Technical Specifications

Input Parameters: Tenant ID, database connection string, schema name
Output/Response: Validation status, connection handle, error details
Performance Criteria: < 5 seconds connection validation, 99.9% success rate
Data Requirements: Tenant metadata storage, connection credential management
Validation Rules

Business Rules: Unique tenant identifiers, valid PostgreSQL connection strings
Data Validation: Connection string format validation, credential verification
Security Requirements: Encrypted credential storage, secure connection establishment
Compliance Requirements: Data isolation verification, audit trail creation
2.2.2 F-002: Dynamic Schema Creation Requirements
| Requirement ID | Description | Acceptance Criteria | Priority | Complexity |
|---|---|---|---|
| F-002-RQ-001 | Create complete NFe 4.00 schema structure | All tables, enums, and relationships created successfully | Must-Have | High |
| F-002-RQ-002 | Generate tenant-specific schema namespace | Schema created with unique tenant identifier | Must-Have | Medium |
| F-002-RQ-003 | Validate schema creation completion | All objects created and accessible via Diesel ORM | Must-Have | Medium |
| F-002-RQ-004 | Handle schema creation failures | Rollback mechanism for failed schema creation | Should-Have | High |

Technical Specifications

Input Parameters: Tenant schema name, NFe SQL schema definition
Output/Response: Schema creation status, object count, error details
Performance Criteria: < 30 seconds schema creation, complete NFe structure
Data Requirements: Complete NFe 4.00 table definitions, enum types, constraints
Validation Rules

Business Rules: Complete NFe compliance, proper table relationships
Data Validation: Schema object verification, constraint validation
Security Requirements: Schema-level access control, tenant isolation
Compliance Requirements: NFe 4.00 specification adherence, audit logging
2.2.3 F-003: Connection Pool Management Requirements
| Requirement ID | Description | Acceptance Criteria | Priority | Complexity |
|---|---|---|---|
| F-003-RQ-001 | Manage tenant-specific connections | Isolated connection pools per tenant | Must-Have | High |
| F-003-RQ-002 | Optimize connection resource usage | Efficient pool sizing and connection reuse | Should-Have | Medium |
| F-003-RQ-003 | Monitor connection health | Active connection monitoring and recovery | Should-Have | Medium |
| F-003-RQ-004 | Handle connection pool exhaustion | Graceful degradation and queue management | Could-Have | High |

Technical Specifications

Input Parameters: Tenant ID, connection pool configuration
Output/Response: Connection handle, pool status, performance metrics
Performance Criteria: < 100ms connection acquisition, 95% pool utilization
Data Requirements: Connection pool configuration, monitoring metrics
Validation Rules

Business Rules: Tenant isolation, resource limits per tenant
Data Validation: Connection string validation, pool configuration limits
Security Requirements: Secure connection establishment, credential protection
Compliance Requirements: Connection audit trails, resource usage tracking
2.2.4 F-004: Schema Migration Engine Requirements
| Requirement ID | Description | Acceptance Criteria | Priority | Complexity |
|---|---|---|---|
| F-004-RQ-001 | Deploy migrations across tenant schemas | Consistent migration application to all tenant schemas | Must-Have | High |
| F-004-RQ-002 | Track migration version per tenant | Individual migration state tracking per tenant | Must-Have | Medium |
| F-004-RQ-003 | Handle migration failures per tenant | Isolated failure handling without affecting other tenants | Should-Have | High |
| F-004-RQ-004 | Support rollback operations | Ability to rollback migrations per tenant | Could-Have | High |

Technical Specifications

Input Parameters: Migration scripts, target tenant schemas, version information
Output/Response: Migration status per tenant, version tracking, error details
Performance Criteria: < 60 seconds per tenant migration, 99.5% success rate
Data Requirements: Migration script storage, version tracking tables
Validation Rules

Business Rules: Sequential migration application, version consistency
Data Validation: Migration script validation, schema compatibility checks
Security Requirements: Migration script integrity, secure deployment
Compliance Requirements: Migration audit trails, rollback capability
2.3 Feature Relationships
2.3.1 Feature Dependencies Map
F-001: Tenant Registration & Validation

F-002: Dynamic Schema Creation

F-003: Connection Pool Management

F-004: Schema Migration Engine

Tenant Metadata Storage

NFe Schema Templates

Connection Pool Configuration

Migration Scripts Repository

2.3.2 Integration Points
Integration Point	Features Involved	Shared Components	Common Services
Tenant Onboarding	F-001, F-002, F-003	Connection validation, Schema creation	Tenant registry service
Schema Management	F-002, F-004	Migration engine, Schema templates	Schema versioning service
Connection Handling	F-001, F-003	Connection pools, Credential management	Connection broker service
Monitoring & Logging	All features	Audit trails, Performance metrics	Logging service
2.3.3 Shared Components
Component	Purpose	Used By	Technical Details
Tenant Registry	Store tenant metadata and configuration	F-001, F-002, F-003	PostgreSQL table with tenant information
Schema Templates	NFe 4.00 schema definitions	F-002, F-004	SQL scripts and Diesel schema definitions
Connection Broker	Manage database connections	F-001, F-003	Connection pooling and routing logic
Migration Repository	Store and manage migration scripts	F-004	Embedded migration scripts with versioning
2.4 Implementation Considerations
2.4.1 Technical Constraints
Constraint Type	Description	Impact	Mitigation Strategy
Diesel Limitations	Many compile time guarantees are lost when using dynamic schemas, cannot verify that tables/columns actually exist	Reduced type safety	Runtime validation and comprehensive testing
Connection Management	Changing search_path can break prepared statement caching and cause wrong data access	Performance and security risks	Schema-qualified queries and connection isolation
Schema Scalability	Administrative tasks like backup and VACUUM scale poorly across thousands of tables	Operational overhead	Automated maintenance scripts and monitoring
Migration Complexity	Schema changes are bad for rolling out universal changes quickly	Deployment complexity	Gradual rollout strategy and rollback procedures
2.4.2 Performance Requirements
Performance Metric	Target	Measurement Method	Acceptance Criteria
Schema Creation Time	< 30 seconds	Automated timing during creation	95% of schemas created within target
Connection Validation	< 5 seconds	Connection establishment timing	99% of validations complete within target
Migration Deployment	< 60 seconds per tenant	Migration execution timing	99.5% success rate within target time
Connection Pool Efficiency	> 95% utilization	Pool monitoring metrics	Sustained utilization above target
2.4.3 Scalability Considerations
Scalability Factor	Current Limit	Scaling Strategy	Monitoring Approach
Number of Tenants	Several thousand tenants maximum for schema-based approach	Horizontal scaling with multiple databases	Tenant count and resource usage monitoring
Schema Size	Complete NFe 4.00 structure	Optimized indexing and partitioning	Query performance monitoring
Connection Pools	Per-tenant pool management	Dynamic pool sizing based on usage	Connection pool metrics and alerts
Migration Performance	Sequential per-tenant deployment	Parallel migration execution	Migration timing and success rate tracking
2.4.4 Security Implications
Security Aspect	Requirement	Implementation	Validation Method
Tenant Isolation	Complete data separation	Schema-level access control	Security audit and penetration testing
Connection Security	Encrypted connections and credentials	TLS connections, encrypted credential storage	Connection security verification
Migration Security	Secure migration script deployment	Signed migration scripts, integrity checks	Migration script validation
Audit Requirements	Complete audit trail	Comprehensive logging of all operations	Audit log review and compliance checks
2.4.5 Maintenance Requirements
Maintenance Task	Frequency	Automation Level	Resource Requirements
Schema Health Checks	Daily	Fully automated	Monitoring system integration
Connection Pool Optimization	Weekly	Semi-automated	Performance analysis tools
Migration Deployment	As needed	Automated with manual approval	Deployment pipeline integration
Backup and Recovery	Daily	Fully automated	Backup system integration
3. Technology Stack
3.1 Programming Languages
3.1.1 Primary Language Selection
Language	Platform/Component	Version	Justification
Rust	Backend Services	1.86.0+	Memory safety, performance, and compile-time guarantees essential for multi-tenant data isolation
SQL	Database Schema	PostgreSQL dialect	NFe schema definition and dynamic tenant schema creation
3.1.2 Language Selection Criteria
Rust Selection Rationale

Memory Safety: Rust empowers everyone to build reliable and efficient software, with studies showing ~70% of high severity security bugs result from memory unsafety
Type Safety: Diesel takes full advantage of Rust's type system to create a low overhead query builder that "feels like Rust"
Multi-tenancy Requirements: Provides tools to work with cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder, though many compile time guarantees are lost
3.1.3 Language Constraints And Dependencies
Rust Version Requirements

Minimum Version: Diesel requires Rust 1.86.0 or later
Tokio Compatibility: Tokio maintains a rolling MSRV policy of at least 6 months, with new Rust versions requiring at least six months since release
Stability: Latest stable Rust version recommended for production deployments
3.2 Frameworks & Libraries
3.2.1 Core Framework Selection
Framework	Version	Purpose	Justification
Diesel ORM	2.2.0+	Database abstraction and query building	Smaller and more extensible crate with better performance
Tokio	Latest stable	Asynchronous runtime	Provides building blocks for networking applications with flexibility to target systems from large servers to small embedded devices
diesel-dynamic-schema	0.2.3+	Runtime schema definition	Provides tools for cases where you don't know the schema until runtime while using Diesel's query builder
3.2.2 Supporting Libraries
Library	Version	Category	Purpose
serde	Latest stable	Serialization	Framework for serializing and deserializing Rust data structures efficiently and generically
serde_json	Latest stable	JSON handling	Fast JSON processing with 500-1000 MB/s deserialization and 600-900 MB/s serialization, competitive with fastest C/C++ libraries
uuid	Latest stable	Unique identifiers	UUID generation for tenant and entity identification
dotenvy	0.15	Configuration	Environment variable management
3.2.3 Compatibility Requirements
Diesel Integration

PostgreSQL Backend: diesel = { version = "2.2.0", features = ["postgres"] }
Dynamic Schema Support: Table function for creating new Diesel tables with explicit select clauses required
Migration Support: diesel_migrations crate provides embed_migrations! macro for embedding migration scripts in final binary
Async Compatibility

Diesel Limitations: Diesel is primarily synchronous and uses native drivers, the primary reason behind native async incompatibility
Async Solutions: Use diesel-async or diesel-deadpool for idiomatic async context usage
3.2.4 Framework Selection Justification
Diesel vs Alternatives

Performance Focus: Diesel provides better performance with smaller and more extensible architecture
Type Safety: Leverages Rust's powerful type system for compile-time type matching and highly optimized performance
Multi-tenant Support: Built for strong compile time guarantees while providing tools for runtime schema interaction
3.3 Open Source Dependencies
3.3.1 Core Dependencies
Crate	Version	Registry	Purpose	License
diesel	2.2.0+	crates.io	Database interaction with eliminated runtime errors without sacrificing performance	MIT/Apache-2.0
diesel-dynamic-schema	0.2.3+	crates.io	Runtime schema interaction while using Diesel's query builder	MIT/Apache-2.0
tokio	Latest	crates.io	Reliable asynchronous applications with I/O, networking, scheduling, timers	MIT
serde	Latest	crates.io	Serialization framework for Rust data structures efficiently and generically	MIT/Apache-2.0
serde_json	Latest	crates.io	Strongly typed JSON library built upon Serde high performance framework	MIT/Apache-2.0
3.3.2 Database And Connection Dependencies
Crate	Version	Purpose	Features
libpq-sys	Latest	PostgreSQL native bindings	System library integration
r2d2	Latest	Connection pooling via r2d2	Connection management
uuid	Latest	UUID generation	v4 UUID support
chrono	Latest	Date/time handling	NFe timestamp requirements
3.3.3 Development And Testing Dependencies
Crate	Version	Purpose	Environment
diesel_cli	Latest	Standalone binary for project management, not affecting project code directly	Development
diesel_migrations	Latest	Embed migration scripts in final binary with embed_migrations! macro	Runtime
dotenvy	0.15	Environment variable management	All environments
3.3.4 Package Registry Configuration
Primary Registry: crates.io

Verification: All dependencies verified through crates.io official registry
Security: Regular security audits through cargo-audit
Version Pinning: Semantic versioning with compatible updates allowed
Installation Methods

Diesel CLI: Pre-built binaries available via installer scripts for Linux/MacOS and Windows PowerShell
Feature Selection: PostgreSQL-only installation: cargo install diesel_cli --no-default-features --features postgres
3.4 Databases & Storage
3.4.1 Primary Database System
Component	Technology	Version	Configuration
Database Engine	PostgreSQL	16.0+	Multi-tenant schema-per-tenant architecture
Connection Pooling	r2d2 + Diesel	Latest	Connection pooling via r2d2
Schema Management	PostgreSQL Schemas	Native	Dynamic tenant schema creation
3.4.2 Database Architecture Strategy
Schema-per-Tenant Approach

Isolation Level: Complete data separation through PostgreSQL schema namespaces
Scalability: PostgreSQL 16.10 current minor version with EOL estimated 2028-11
Performance: PostgreSQL 16 improves performance through new query planner optimizations, parallelized FULL and RIGHT joins, and optimized aggregate functions
Connection Management

Pool Strategy: Tenant-specific connection pools with shared infrastructure
Resource Optimization: PostgreSQL 16 adds support for load balancing in clients using libpq
Monitoring: Connection health monitoring and automatic recovery
3.4.3 Data Persistence Strategies
Strategy	Implementation	Purpose	Benefits
Schema Isolation	PostgreSQL namespaces	Tenant data separation	Complete isolation, regulatory compliance
Migration Management	Embedded migrations with embed_migrations! macro	Schema versioning	Automated deployment, consistency
Connection Validation	Pre-schema creation checks	Tenant onboarding	Error prevention, reliability
Backup Strategy	Schema-level backups	Data protection	Tenant-specific recovery
3.4.4 Storage Services Integration
PostgreSQL Extensions

UUID Support: uuid-ossp extension for UUID generation
JSON Capabilities: PostgreSQL 16 adds more SQL/JSON syntax including JSON_ARRAY(), JSON_ARRAYAGG(), and IS JSON
Performance Features: Bulk loading improvements with up to 300% performance improvement in some cases
File System Requirements

Migration Scripts: Embedded in binary for deployment simplicity
Configuration Files: Environment-based configuration management
Log Storage: Structured logging for audit and debugging
3.5 Development & Deployment
3.5.1 Development Tools
Tool	Version	Purpose	Integration
Rust Toolchain	1.86.0+ via rustup update stable	Compilation and development	Primary development environment
Diesel CLI	Latest via installer scripts	Database connection, schema querying, and table! generation	Schema management
Cargo	Built-in	Package management and building	Dependency resolution
3.5.2 Build System Configuration
Cargo Configuration

[dependencies]
diesel = { version = "2.2.0", features = ["postgres", "uuid", "chrono"] }
diesel-dynamic-schema = "0.2.3"
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
uuid = { version = "1.0", features = ["v4"] }
dotenvy = "0.15"

[dev-dependencies]
diesel_migrations = "2.2.0"
Feature Flags

PostgreSQL Backend: features = ["postgres"] for database connectivity
UUID Support: UUID generation and handling for tenant identification
Async Runtime: Tokio features for asynchronous tasks, I/O operations, and runtime services
3.5.3 Migration And Schema Management
Migration Workflow

Schema Generation: diesel print-schema with automatic re-run configuration during migrations
Dynamic Creation: Runtime table and column creation using diesel-dynamic-schema
Version Control: Embedded migration scripts in binary with run_pending_migrations at startup
Development Environment Setup

Database Setup: PostgreSQL 16+ installation with development database
Environment Configuration: DATABASE_URL=postgres://username:password@localhost/database_name
Migration Directory: diesel setup creates migrations directory and database
Schema Generation: Automatic schema.rs generation during migration execution
3.5.4 Deployment Requirements
Runtime Dependencies

PostgreSQL Client Libraries: libpq for database connectivity
System Libraries: OpenSSL for secure connections
Environment Variables: Database connection strings and configuration
Deployment Architecture

Binary Deployment: Single binary with embedded migrations
Configuration Management: Environment-based configuration
Health Checks: Connection validation and schema verification
Monitoring: Performance metrics and error tracking
3.5.5 Performance And Scalability Considerations
Compile-Time Optimizations

Type Safety: Runtime schema usage loses many compile time guarantees, cannot verify tables/columns exist or types are correct
Query Validation: Diesel validates queries at compile time through schema specification and table! macro
Performance: Tokio built for speed with goal that equivalent hand-written code cannot improve performance
Runtime Performance

Connection Pooling: Efficient resource utilization across tenant connections
Schema Caching: Minimize schema lookup overhead
Query Optimization: PostgreSQL 16 query planner optimizations for better performance
4. Process Flowchart
4.1 System Workflows
4.1.1 Core Business Processes
Tenant Registration And Schema Creation Workflow
The multi-tenant NFe system implements a schema-per-tenant architecture where the structure of tables is known at compile time, but the namespace for the tables is generated dynamically. This approach provides tools to work with cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder.

Invalid

Valid

Connection Failed

Connection Success

Failed

Success

Missing Objects

Complete

New Tenant Registration Request

Validate Tenant Data

Return Validation Error

Generate Unique Schema Name

Test Database Connection

Return Connection Error

Begin Transaction

Create Schema Namespace

Deploy NFe 4.00 Schema Structure

Schema Creation Success?

Rollback Transaction

Return Schema Creation Error

Validate Schema Completeness

All Objects Created?

Commit Transaction

Store Tenant Metadata

Initialize Connection Pool

Return Success Response

Nfe Document Processing Workflow
No

Yes

Invalid

Valid

Failed

Passed

NFe Document Request

Extract Tenant ID

Tenant Exists?

Return Tenant Not Found Error

Get Tenant Schema Connection

Validate NFe Data Structure

Return Validation Error

Begin NFe Transaction

Insert NFe Header

Process NFe Items

Calculate Tax Information

Process Referenced Documents

Validate Business Rules

Rollback Transaction

Return Business Rule Error

Generate Chave de Acesso

Update NFe Status

Commit Transaction

Return NFe Created Response

4.1.2 Integration Workflows
Dynamic Schema Migration Workflow
When using dynamic schemas, many compile time guarantees are lost. The system cannot verify that the tables/columns you ask for actually exist, or that the types you state are correct.

Validation Service
Tenant Schema
PostgreSQL Database
Tenant Registry
Migration Service
Validation Service
Tenant Schema
PostgreSQL Database
Tenant Registry
Migration Service
alt
[Validation Success]
[Validation Failed]
alt
[Migration Success]
[Migration Failed]
alt
[Schema Update Required]
loop
[For Each Tenant]
Get All Active Tenants
Return Tenant List
Connect to Tenant Schema
Connection Established
Check Current Schema Version
Return Version Info
Begin Migration Transaction
Apply Migration Scripts
Migration Result
Validate Schema Structure
Validation Result
Commit Transaction
Update Tenant Version
Rollback Transaction
Log Migration Error
Rollback Transaction
Log Migration Error
Connection Pool Management Workflow
Diesel uses prepared statements internally and caches them for performance. Changing the search_path will likely break this, as the already prepared statements will no longer work as expected and will refer to tables/columns in the old schema, possibly loading wrong data.

No

Yes

Failed

Success

No

Yes

No

Yes

Application Request

Extract Tenant Context

Connection Pool Exists?

Create New Pool

Get Connection from Pool

Initialize Pool Configuration

Validate Schema Connection

Pool Creation Error

Register Pool

Connection Available?

Wait for Connection

Timeout Reached?

Return Pool Exhausted Error

Set Schema Context

Execute Database Operation

Return Connection to Pool

Return Operation Result

4.2 Flowchart Requirements
4.2.1 Tenant Validation And Schema Creation Process
Business Rules And Validation Points
Validation Point	Business Rule	Data Validation	Authorization Check
Tenant Registration	Unique tenant identifier required	CNPJ/CPF format validation	Admin role required
Connection Validation	Valid PostgreSQL connection string	Connection timeout < 5 seconds	Database access permissions
Schema Creation	Unique schema namespace	Complete NFe 4.00 structure	Schema creation privileges
Migration Deployment	Sequential version application	Migration script integrity	Migration execution rights
Error Handling And Recovery Paths
Connection Error

Yes

No

Schema Error

Migration Error

Validation Error

Error Detected

Error Type

Retry Connection

Retry Count < 3?

Wait Exponential Backoff

Mark Tenant Offline

Rollback Schema Changes

Clean Partial Objects

Log Schema Error

Notify Administrator

Rollback Migration

Restore Previous Version

Update Migration Status

Schedule Retry

Log Validation Details

Return Detailed Error

4.2.2 State Management And Transaction Boundaries
Nfe Document State Transitions
Schema isolation in PostgreSQL provides an elegant middle ground for multi-tenant microservices architectures. It balances isolation and resource efficiency while maintaining operational simplicity.

Create NFe

Digital Signature

Validation Failed

Send to SEFAZ

Signature Invalid

SEFAZ Processing

Transmission Error

SEFAZ Approval

SEFAZ Denial

Processing Error

Cancellation Request

Sequence Invalidation

Final State

Final State

Final State

Final State

CRIADA

ASSINADA

REJEITADA

ENVIADA

PROCESSANDO

AUTORIZADA

DENEGADA

CANCELADA

INUTILIZADA

Initial state after creation

Valid for fiscal purposes

Cancelled within 24h

Transaction Boundary Management
No

Yes

No

Yes

Begin Business Operation

Start Database Transaction

Acquire Schema Connection

Set Transaction Isolation Level

Execute Business Logic

Business Rules Valid?

Rollback Transaction

Release Connection

Return Business Error

Validate Data Integrity

Integrity Check Passed?

Commit Transaction

Update Cache if Needed

Return Success Result

4.3 Technical Implementation
4.3.1 Dynamic Schema Creation Implementation
Schema Template Deployment Process
No

Yes

NFe Schema Objects

UF Types

Status Enums

Tax Types

Empresas Table

NFe Table

NFe Items Table

Tax Tables

Schema Creation Request

Load NFe 4.00 Template

Generate Schema-Qualified Names

Create Schema Namespace

Deploy Enum Types

Create Base Tables

Apply Foreign Key Constraints

Create Indexes

Set Table Permissions

Validate Schema Structure

Validation Success?

Cleanup Partial Schema

Return Creation Error

Register Schema Metadata

Initialize Connection Pool

Return Schema Ready

Runtime Schema Validation Workflow
The main function used by this crate is table. Note that you must always provide an explicit select clause when using this crate.

Schema Cache
PostgreSQL
Dynamic Schema Service
Application
Schema Cache
PostgreSQL
Dynamic Schema Service
Application
alt
[Cache Miss]
Explicit select clauses required
TTL-based cache invalidation
Request Schema Operation
Check Schema Cache
Cache Miss/Hit
Query Schema Metadata
Return Schema Info
Update Cache
Build Dynamic Table Definition
Execute Schema-Qualified Query
Return Query Result
Return Operation Result
4.3.2 Error Handling And Recovery Procedures
Connection Pool Error Recovery
Connection Timeout

Connection Refused

Schema Not Found

Permission Denied

No

Yes

No

Yes

Connection Pool Error

Error Classification

Increase Pool Size

Check Database Status

Verify Tenant Schema

Check Access Rights

Monitor Pool Metrics

Database Available?

Switch to Maintenance Mode

Recreate Connection Pool

Schema Exists?

Trigger Schema Creation

Refresh Schema Cache

Update Connection Credentials

Resume Normal Operations

Wait for Schema Ready

Return Service Unavailable

Migration Rollback Procedure
No

Yes

Migration Failure Detected

Stop Migration Process

Begin Rollback Transaction

Identify Applied Changes

Execute Rollback Scripts

Rollback Success?

Manual Intervention Required

Alert Database Administrator

Verify Schema Integrity

Update Migration Status

Clean Temporary Objects

Commit Rollback Transaction

Log Rollback Completion

Schedule Retry if Applicable

4.4 Performance And Scalability Considerations
4.4.1 Connection Pool Optimization Workflow
While schema isolation provides logical separation, all tenants still share the same database resources. Implement resource governance using PostgreSQL resource queues, monitoring at the schema level to identify problematic tenants, and connection pooling optimized for multi-tenant workloads.

No

Yes

Yes

No

No

Yes

Monitor Pool Metrics

Pool Utilization > 80%?

Continue Monitoring

Analyze Connection Patterns

Tenant Hotspots Detected?

Implement Tenant Throttling

Scale Pool Size

Apply Rate Limiting

Add Connection Instances

Monitor Throttling Impact

Validate Pool Performance

Performance Improved?

Investigate Further

Consider Schema Partitioning

Implement Advanced Scaling

4.4.2 Schema Migration Performance Optimization
00:00
00:05
00:10
00:15
00:20
00:25
00:30
Load Migration Scripts
Validate Scripts
Schema Migration T1-T10
Schema Migration T11-T20
Schema Migration T21-T30
Final Validation
Update Registry
Validation T1-T10
Validation T11-T20
Validation T21-T30
Preparation
Tenant Group 1
Tenant Group 2
Tenant Group 3
Completion
Multi-Tenant Schema Migration Timeline
4.4.3 Monitoring And Alerting Workflow
No

Yes

Yes

No

Yes

No

Yes

No

System Metrics Collection

Tenant Performance Analysis

Performance Threshold Exceeded?

Continue Monitoring

Identify Problem Tenant

Analyze Resource Usage

Resource Limit Exceeded?

Apply Resource Throttling

Check Schema Complexity

Alert Operations Team

Schema Issues Found?

Schedule Schema Optimization

Investigate Application Logic

Monitor Throttling Effect

Plan Maintenance Window

Profile Query Performance

Issue Resolved?

Escalate to Engineering

4.5 Compliance And Audit Workflows
4.5.1 Nfe Compliance Validation Process
No

Yes

No

Yes

No

Yes

No

Yes

NFe Document Submission

Extract Document Data

Validate NFe 4.00 Structure

Structure Valid?

Return Structure Error

Validate Business Rules

Business Rules Valid?

Return Business Error

Calculate Tax Values

Validate Tax Calculations

Tax Calculations Valid?

Return Tax Error

Generate Chave de Acesso

Validate Chave Format

Chave Valid?

Return Chave Error

Store NFe Document

Update Audit Trail

Return Success Response

4.5.2 Audit Trail And Data Integrity Workflow
Audit Log
Database
Audit Service
Application
User/System
Audit Log
Database
Audit Service
Application
User/System
alt
[Operation Success]
[Operation Failure]
Immutable audit trail
Schema-level isolation maintained
Execute Operation
Begin Audit Context
Start Transaction
Execute Business Logic
Operation Result
Record Success Event
Write Audit Entry
Commit Transaction
Return Success
Record Failure Event
Write Error Entry
Rollback Transaction
Return Error
5. System Architecture
5.1 High-level Architecture
5.1.1 System Overview
The NFe multi-tenant management system implements a shared database, separate schemas architecture where all tenants share a database, but each has its own schema, improving data isolation and customization. This approach leverages cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder.

The architecture follows a schema-per-tenant pattern specifically designed for Brazilian NFe (Nota Fiscal Eletrônica) compliance, where tenant-schema is good for deploying schema changes as a gradual rollout across tenants and works better for situations when you have a lot of tenants with substantial data. The system utilizes PostgreSQL's native schema isolation capabilities combined with Rust's type safety through Diesel ORM to provide secure, performant multi-tenant data management.

Key architectural principles include:

Schema Isolation: Each tenant receives a dedicated PostgreSQL schema containing the complete NFe 4.00 table structure
Dynamic Schema Management: Runtime schema creation and validation using diesel-dynamic-schema
Connection Pool Optimization: Tenant-aware connection pooling with resource governance
Compile-Time Safety: Leveraging Rust's type system while accommodating runtime schema requirements
The system boundaries encompass tenant registration, schema provisioning, NFe document processing, and multi-tenant connection management, with external integration points to PostgreSQL databases and potential SEFAZ services.

5.1.2 Core Components Table
Component Name	Primary Responsibility	Key Dependencies	Integration Points
Tenant Registry Service	Manage tenant metadata and validation	PostgreSQL, UUID generation	Schema Creator, Connection Manager
Dynamic Schema Creator	Generate tenant-specific NFe schemas	diesel-dynamic-schema, NFe SQL templates	Tenant Registry, Migration Engine
Connection Pool Manager	Handle tenant-specific database connections	r2d2, PostgreSQL drivers	All data access components
Migration Engine	Deploy schema updates across tenants	Diesel migrations, embedded scripts	Schema Creator, Tenant Registry
5.1.3 Data Flow Description
The primary data flow begins with tenant registration, where the system validates database connectivity and generates unique schema identifiers. Upon successful validation, the Dynamic Schema Creator deploys the complete NFe 4.00 structure within the tenant's dedicated PostgreSQL schema namespace.

NFe document processing flows through tenant-aware connection routing, where the Connection Pool Manager establishes schema-qualified database connections. Many compile time guarantees are lost as we cannot verify that the tables/columns you ask for actually exist, or that the types you state are correct, requiring runtime validation at each data access point.

Migration workflows utilize the embedded migration engine to deploy schema changes across tenant schemas in controlled rollouts. The system maintains audit trails and version tracking per tenant, enabling isolated rollback capabilities without affecting other tenants.

Data transformation occurs at the schema boundary, where generic NFe structures are mapped to tenant-specific schema contexts using diesel-dynamic-schema table definitions with explicit select clauses.

5.1.4 External Integration Points
System Name	Integration Type	Data Exchange Pattern	Protocol/Format
PostgreSQL Database	Direct Connection	Schema-qualified queries	Native PostgreSQL protocol
SEFAZ Services	Future Integration	XML document exchange	SOAP/REST over HTTPS
Monitoring Systems	Metrics Export	Performance and health data	Prometheus/JSON metrics
Backup Services	Data Export	Schema-level backup operations	PostgreSQL dump format
5.2 Component Details
5.2.1 Tenant Registry Service
Purpose and Responsibilities
The Tenant Registry Service manages the complete lifecycle of tenant metadata, from initial registration through schema provisioning validation. It serves as the authoritative source for tenant configuration and maintains the mapping between tenant identifiers and their corresponding PostgreSQL schema namespaces.

Technologies and Frameworks

Core Framework: Rust with Diesel ORM 2.2.0+ for type-safe database interactions
Data Storage: PostgreSQL with UUID-based tenant identification
Validation: Connection string parsing and database connectivity testing
Serialization: Serde for tenant metadata serialization and configuration management
Key Interfaces and APIs

register_tenant(tenant_config: TenantConfig) -> Result<TenantId, RegistrationError>
validate_connection(connection_string: &str) -> Result<ConnectionHandle, ValidationError>
get_tenant_schema(tenant_id: TenantId) -> Result<SchemaName, NotFoundError>
list_active_tenants() -> Vec<TenantMetadata>
Data Persistence Requirements
Tenant metadata persists in a dedicated tenants table within the system's administrative schema, storing connection parameters, schema names, creation timestamps, and status information. The service maintains referential integrity with schema creation records and migration version tracking.

Scaling Considerations
The registry supports horizontal scaling through read replicas for tenant lookup operations while maintaining write consistency for registration operations. Schema-per-tenant works better for situations when you have a lot of tenants with substantial data, such as accounting applications with paid subscription users.

5.2.2 Dynamic Schema Creator
Purpose and Responsibilities
The Dynamic Schema Creator handles runtime generation of tenant-specific PostgreSQL schemas containing the complete NFe 4.00 structure. It manages schema namespace creation, table deployment, constraint application, and validation of the resulting schema structure.

Technologies and Frameworks

Schema Generation: diesel-dynamic-schema for runtime table definitions
Template Management: Embedded NFe 4.00 SQL schema templates
Transaction Management: PostgreSQL transactions for atomic schema creation
Validation: Schema structure verification and completeness checking
Key Interfaces and APIs

create_tenant_schema(tenant_id: TenantId, schema_name: &str) -> Result<SchemaCreated, CreationError>
validate_schema_structure(schema_name: &str) -> Result<ValidationReport, ValidationError>
deploy_nfe_tables(schema_name: &str) -> Result<TableCount, DeploymentError>
cleanup_failed_schema(schema_name: &str) -> Result<(), CleanupError>
Data Persistence Requirements
Schema creation operations are transactional, with rollback capabilities for failed deployments. The service maintains creation logs and schema metadata for audit purposes, tracking object counts and deployment timestamps.

Scaling Considerations
Schema creation is CPU and I/O intensive, requiring careful resource management during concurrent tenant onboarding. Administrative tasks like backup and VACUUM scale poorly across thousands of tables, though this affects tens or hundreds of thousands of tables, not a few hundred.

Dynamic Schema Creation Flow
Validation Service
PostgreSQL
Dynamic Schema Creator
Tenant Registry
Validation Service
PostgreSQL
Dynamic Schema Creator
Tenant Registry
loop
[NFe Table Creation]
alt
[Schema Valid]
[Schema Invalid]
create_tenant_schema(tenant_id, schema_name)
BEGIN TRANSACTION
CREATE SCHEMA tenant_schema
CREATE TABLE schema.table_name
CREATE INDEXES
ADD CONSTRAINTS
validate_schema_structure(schema_name)
Query schema metadata
Validation Result
COMMIT TRANSACTION
SchemaCreated(metadata)
ROLLBACK TRANSACTION
CreationError(details)
5.2.3 Connection Pool Manager
Purpose and Responsibilities
The Connection Pool Manager provides tenant-aware database connection management, handling connection pooling, schema context switching, and resource governance across multiple tenant schemas. Using SET search_path TO schemaname to switch between schemas would screw up a lot of things for almost all database libraries, requiring careful connection isolation.

Technologies and Frameworks

Connection Pooling: r2d2 connection pool management
Database Drivers: PostgreSQL native drivers (libpq)
Resource Management: Connection limits and timeout handling
Monitoring: Connection pool metrics and health checking
Key Interfaces and APIs

get_tenant_connection(tenant_id: TenantId) -> Result<PooledConnection, PoolError>
create_tenant_pool(tenant_config: TenantConfig) -> Result<ConnectionPool, PoolCreationError>
monitor_pool_health(tenant_id: TenantId) -> PoolHealthReport
scale_pool_size(tenant_id: TenantId, new_size: u32) -> Result<(), ScalingError>
Data Persistence Requirements
Connection pool configurations persist in the tenant registry, with runtime metrics stored in monitoring systems. Pool state is ephemeral but tracked for performance optimization and resource planning.

Scaling Considerations
Shared-table works better for connection pooling, as all connections can use the same pool, but schema-per-tenant requires careful pool management to avoid connection exhaustion while maintaining isolation.

Connection Pool Management Flow
Tenant Registration

Configuration Complete

Request Connection

Connection Available

Schema Qualified

Operation Complete

Connection Released

Load Increase

Scaling Complete

Health Check

Maintenance Complete

Tenant Deactivation

Resources Released

PoolInitialization

PoolReady

ConnectionAcquisition

SchemaContextSet

QueryExecution

ConnectionReturn

PoolScaling

PoolMaintenance

PoolShutdown

Schema-qualified queries required

Dynamic pool sizing based on load

5.2.4 Migration Engine
Purpose and Responsibilities
The Migration Engine manages schema evolution across tenant schemas, deploying updates in controlled rollouts while maintaining version consistency and rollback capabilities. It handles embedded migration scripts and tracks deployment status per tenant.

Technologies and Frameworks

Migration Framework: Diesel migrations with embedded scripts
Version Control: Per-tenant migration version tracking
Deployment Strategy: Gradual rollout with failure isolation
Rollback Management: Automated rollback procedures for failed migrations
Key Interfaces and APIs

deploy_migration(migration_id: MigrationId, target_tenants: Vec<TenantId>) -> Result<DeploymentReport, MigrationError>
rollback_migration(tenant_id: TenantId, target_version: Version) -> Result<RollbackReport, RollbackError>
get_migration_status(tenant_id: TenantId) -> MigrationStatus
validate_migration_integrity(migration_id: MigrationId) -> Result<(), ValidationError>
Data Persistence Requirements
Migration version tracking per tenant with deployment timestamps and status information. Migration scripts are embedded in the binary using Diesel's embed_migrations! macro for deployment consistency.

Scaling Considerations
Tenant-schema is bad for rolling out universal schema changes quickly, but good for deploying schema changes as a gradual rollout across tenants. The engine supports parallel deployment with configurable batch sizes and failure isolation.

5.3 Technical Decisions
5.3.1 Architecture Style Decisions And Tradeoffs
Schema-per-tenant Vs Shared Schema Decision
Decision Factor	Schema-per-Tenant (Chosen)	Shared Schema Alternative	Rationale
Data Isolation	Complete schema-level isolation	Row-level with tenant_id filtering	NFe compliance requires absolute data separation
Query Complexity	Schema-qualified queries	Tenant_id in every WHERE clause	Eliminates risk of cross-tenant data access
Scaling Characteristics	Better for relatively small number of fairly large tenants	Better for many small tenants	Matches NFe use case profile
Migration Strategy	Good for gradual rollout across tenants	Universal changes deploy quickly	Allows controlled NFe schema evolution
Diesel Orm With Dynamic Schema Decision
The decision to use diesel-dynamic-schema provides tools to work with runtime schema cases while still using Diesel's query builder, though many compile time guarantees are lost. This tradeoff accepts reduced compile-time safety in exchange for multi-tenant flexibility.

Benefits Realized:

Maintains Diesel's query builder ergonomics
Enables runtime schema namespace switching
Preserves type safety where possible
Integrates with existing Rust ecosystem
Costs Accepted:

Cannot verify that tables/columns actually exist or that types are correct
Requires explicit select clauses in all queries
Runtime validation overhead
Increased testing requirements
5.3.2 Communication Pattern Choices
Connection Pool Strategy Decision Tree
< 100 Tenants

100-1000 Tenants

> 1000 Tenants
Yes

No

High

Standard

Connection Strategy Decision

Tenant Count

Dedicated Pools per Tenant

Shared Pool with Schema Context

Hybrid Pool Strategy

High Isolation, Higher Resource Usage

Balanced Isolation and Efficiency

Tiered Pool Management

Resource Constraints?

Performance Requirements?

Tenant Tier Classification

Consolidate to Shared Pools

Maintain Dedicated Pools

Optimize Pool Sizing

Default Configuration

Premium: Dedicated Pools

Standard: Shared Pools

5.3.3 Data Storage Solution Rationale
Postgresql Schema Isolation Justification
Storage Approach	Isolation Level	Operational Complexity	Scalability	NFe Compliance
Separate Databases	Maximum	High maintenance overhead	Resource intensive	Excellent
Schema-per-Tenant	High	Moderate complexity	Balanced approach maintaining database-level operational efficiency	Excellent
Shared Tables	Minimal	Low complexity	Highly scalable	Risk of data leakage
The schema-per-tenant approach was selected because PostgreSQL offers powerful capabilities through schema isolation, providing a balanced approach between complete database separation and shared-table architectures.

5.3.4 Caching Strategy Justification
Schema Metadata Caching Strategy
Yes

No

Schema Metadata Request

Cache Hit?

Return Cached Metadata

Query PostgreSQL Catalog

Update Cache with TTL

Return Fresh Metadata

Schema Modification

Invalidate Cache Entry

Broadcast Cache Invalidation

Cache Maintenance

TTL Expiration Check

Remove Expired Entries

Caching Decisions:

Schema Metadata: 15-minute TTL with invalidation on schema changes
Connection Pool State: Real-time monitoring without caching
Tenant Registry: 5-minute TTL for tenant lookup operations
Migration Status: No caching due to deployment sensitivity
5.3.5 Security Mechanism Selection
Multi-tenant Security Architecture
Security Layer	Implementation	Justification	NFe Compliance Impact
Schema Isolation	PostgreSQL native schemas	Every tenant is isolated from all other tenants using their own user with access only to corresponding schema	Complete data separation
Connection Security	TLS encryption, credential management	Protects data in transit and at rest	Meets fiscal data protection requirements
Access Control	Role-based schema permissions	Granular control over tenant data access	Supports audit requirements
Query Validation	Runtime schema existence checks	Compensates for lost compile-time guarantees	Prevents data access errors
5.4 Cross-cutting Concerns
5.4.1 Monitoring And Observability Approach
The monitoring strategy addresses the unique challenges of schema-per-tenant architecture, where performance is great and Postgres scales well with many schemata and tables, but doesn't scale well when querying information_schema.

Monitoring Components:

Tenant-Level Metrics: Individual schema performance tracking
Connection Pool Monitoring: Per-tenant pool utilization and health
Schema Creation Metrics: Deployment timing and success rates
Migration Progress Tracking: Per-tenant migration status and rollback capabilities
Observability Tools:

Prometheus metrics export for time-series data
Structured logging with tenant context correlation
Health check endpoints for each system component
Performance profiling for schema operations
5.4.2 Logging And Tracing Strategy
Structured Logging Framework:

Tenant Context: All log entries include tenant_id for correlation
Schema Operations: Detailed logging of schema creation and migration activities
Connection Events: Pool acquisition, release, and error tracking
Performance Metrics: Query timing and resource utilization logging
Distributed Tracing:

Request correlation across tenant boundaries
Schema operation tracing for performance analysis
Migration deployment tracking across tenant rollouts
Error propagation tracing for debugging
5.4.3 Error Handling Patterns
Multi-tenant Error Handling Flow
Invalid

Valid

Failed

Success

Schema Not Found

Query Error

Success

No

Yes

Operation Request

Tenant Context Validation

Return Tenant Error

Schema Connection Acquisition

Connection Pool Error

Execute Operation

Schema Missing Error

Database Operation Error

Return Success Result

Retry with Backoff

Retry Limit Reached?

Return Pool Exhausted Error

Trigger Schema Validation

Log Schema Inconsistency

Classify Error Type

Log with Tenant Context

Error Classification:

Tenant Errors: Invalid tenant ID, inactive tenant status
Schema Errors: Missing schema, incomplete schema structure
Connection Errors: Pool exhaustion, database connectivity issues
Migration Errors: Failed deployments, version conflicts
5.4.4 Authentication And Authorization Framework
Multi-Tenant Security Model:

Tenant Isolation: Schema-level access control with dedicated database users per tenant
Administrative Access: Separate administrative roles for cross-tenant operations
API Authentication: JWT tokens with tenant context validation
Database Security: Row-level security policies where applicable
Authorization Patterns:

Tenant-scoped operations with automatic context validation
Administrative operations requiring elevated privileges
Schema creation and migration operations with audit logging
Connection pool access control based on tenant membership
5.4.5 Performance Requirements And Slas
Performance Metric	Target SLA	Measurement Method	Escalation Threshold
Tenant Schema Creation	< 30 seconds	Automated timing during creation	> 45 seconds
Connection Acquisition	< 100ms	Pool monitoring metrics	> 500ms average
Migration Deployment	< 60 seconds per tenant	Migration execution timing	> 120 seconds
Query Response Time	< 200ms (95th percentile)	Application performance monitoring	> 1 second
5.4.6 Disaster Recovery Procedures
Schema-Level Backup Strategy:

Individual Tenant Backups: Schema-specific backup procedures for isolated recovery
Cross-Tenant Consistency: Coordinated backup timing for related data
Migration State Preservation: Backup of migration version tracking
Connection Pool State: Ephemeral state requiring reconstruction
Recovery Procedures:

Tenant Schema Restoration: Individual tenant recovery without affecting others
Migration Rollback: Per-tenant rollback capabilities with version tracking
Connection Pool Recovery: Automatic pool reconstruction on service restart
Data Integrity Validation: Post-recovery schema structure verification
Disaster Recovery Testing:

Monthly schema backup and restoration testing
Quarterly migration rollback procedure validation
Annual full disaster recovery simulation
Continuous monitoring of backup integrity and restoration timing
6. System Components Design
6.1 Tenant Schema Management Architecture
6.1.1 Dynamic Schema Creation Service
The Dynamic Schema Creation Service implements tools to work with cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder. This service manages the complete lifecycle of tenant schema provisioning for the NFe multi-tenant system.

Core Service Components

Component	Responsibility	Technology Stack	Integration Points
Schema Template Engine	NFe 4.00 schema template management	Rust + embedded SQL templates	Migration Engine, Validation Service
Dynamic Table Creator	Runtime table generation using diesel-dynamic-schema	diesel-dynamic-schema table function	PostgreSQL, Connection Manager
Schema Validator	Post-creation schema structure verification	PostgreSQL information_schema queries	Tenant Registry, Audit Service
Rollback Manager	Failed schema creation cleanup	PostgreSQL transactions	Error Handling Service
Schema Creation Workflow Implementation

PostgreSQL
Schema Validator
Dynamic Table Creator
Schema Template Engine
Dynamic Schema Creator
Tenant Registry
PostgreSQL
Schema Validator
Dynamic Table Creator
Schema Template Engine
Dynamic Schema Creator
Tenant Registry
loop
[For Each NFe Table]
alt
[Schema Valid]
[Schema Invalid]
create_tenant_schema(tenant_config)
load_nfe_template()
NFe 4.00 Schema Definition
BEGIN TRANSACTION
CREATE SCHEMA tenant_schema_name
create_dynamic_table(table_def, schema_name)
CREATE TABLE schema.table_name
CREATE INDEXES
ADD CONSTRAINTS
validate_schema_completeness(schema_name)
Query information_schema
Validation Result
COMMIT TRANSACTION
SchemaCreated(metadata)
ROLLBACK TRANSACTION
CreationError(details)
NFe Schema Template Structure

The service maintains embedded SQL templates for the complete NFe 4.00 structure, including all enums, tables, and constraints as specified in the provided schema. Tenant-schema is best when you have a relatively small number of fairly large tenants, such as an accounting application with only paid subscription users.

6.1.2 Connection Pool Management Service
The Connection Pool Management Service addresses the unique challenges of schema-per-tenant architecture where using SET search_path TO schemaname to switch between schemas would screw up a lot of things for almost all database libraries.

Connection Strategy Implementation

Strategy Component	Implementation Approach	Performance Impact	Scalability Considerations
Schema-Qualified Queries	All queries use explicit schema.table notation	Eliminates search_path issues	Requires runtime query modification
Tenant Connection Pools	Dedicated connection pools per tenant	Higher resource usage	Better isolation and performance
Connection Validation	Pre-query schema existence checks	Runtime validation overhead	Compensates for lost compile-time guarantees
Pool Health Monitoring	Active connection monitoring and recovery	Proactive issue detection	Enables automatic scaling
Connection Pool Architecture

No

Yes

No

Yes

Tenant Request

Tenant Context Extraction

Connection Pool Exists?

Create Tenant Pool

Get Pool Connection

Pool Configuration

Initialize Connections

Schema Validation

Register Pool

Schema-Qualified Query Execution

Return Connection to Pool

Response to Client

Pool Monitor

Health Check

Pool Healthy?

Pool Recovery

Update Metrics

Recreate Pool

6.1.3 Migration Engine Service
The Migration Engine Service manages schema evolution across tenant schemas, addressing the challenge that tenant-schema is bad for rolling out universal schema changes quickly, but good for deploying schema changes as a gradual rollout across tenants.

Migration Strategy Components

Component	Purpose	Implementation	Tenant Impact
Embedded Migration Scripts	Version-controlled schema changes	Diesel migrations with embed_migrations! macro	Consistent deployment across tenants
Gradual Rollout Manager	Controlled tenant migration batches	Configurable batch sizes with failure isolation	Minimizes blast radius of failed migrations
Version Tracking Service	Per-tenant migration state management	Individual version tracking per tenant schema	Enables selective rollbacks
Rollback Orchestrator	Failed migration recovery	Automated rollback procedures	Maintains tenant schema consistency
Migration Deployment Flow

Yes

No

Yes

No

Per-Tenant Migration

Begin Transaction

Apply Migration Script

Validate Schema Changes

Update Version Tracking

Commit Transaction

Migration Request

Load Migration Scripts

Validate Migration Integrity

Get Target Tenant List

Create Migration Batches

Process Batch 1

Batch 1 Success?

Process Batch 2

Halt Migration

Batch 2 Success?

Continue Next Batch

Rollback Batch 1

Generate Failure Report

Generate Partial Rollback Report

Complete Migration

6.2 Runtime Schema Interaction
6.2.1 Dynamic Query Builder Service
The Dynamic Query Builder Service implements runtime schema interaction where many compile time guarantees are lost and we cannot verify that the tables/columns you ask for actually exist, or that the types you state are correct.

Query Builder Architecture

Component	Functionality	Diesel Integration	Runtime Validation
Dynamic Table Factory	Runtime table creation using diesel-dynamic-schema table function	diesel_dynamic_schema::table	Schema existence validation
Column Type Mapper	Runtime column type definition	Explicit type annotations required	Type compatibility checking
Query Validator	Pre-execution query validation	Schema-qualified query construction	Runtime error prevention
Result Mapper	Query result deserialization	Custom deserializer implementation	Type safety validation
Dynamic Query Construction Pattern

// Example implementation pattern for NFe dynamic queries
use diesel_dynamic_schema::table;

// Runtime table definition with explicit schema qualification
let nfe_table = table("tenant_schema.nfe");
let id_column = nfe_table.column::<diesel::sql_types::Uuid, _>("id");
let numero_nf_column = nfe_table.column::<diesel::sql_types::Integer, _>("numero_nf");
let status_column = nfe_table.column::<diesel::sql_types::Text, _>("status");

// Query construction with explicit select clause (required)
let query_result = nfe_table
    .select((id_column, numero_nf_column, status_column))
    .filter(status_column.eq("AUTORIZADA"))
    .load::<(uuid::Uuid, i32, String)>(&mut connection);
6.2.2 Schema Context Management Service
The Schema Context Management Service handles tenant-specific database context switching while maintaining data isolation and query performance.

Context Management Components

Service Component	Responsibility	Implementation Strategy	Performance Impact
Tenant Context Resolver	Extract tenant information from requests	JWT token parsing, subdomain extraction	< 1ms resolution time
Schema Name Generator	Generate consistent schema names	Deterministic naming with tenant ID	Cached schema name lookup
Connection Context Setter	Establish schema-qualified connections	Explicit schema.table notation in queries	Eliminates search_path overhead
Query Interceptor	Inject schema context into queries	Runtime query modification	2-5ms query overhead
Schema Context Flow

HTTP Request

Extract Tenant Context

Validate Tenant Schema

Get Schema Connection

Execute Schema-Qualified Query

Process Results

Return Response

Invalid Tenant

Schema Not Found

Connection Failed

Query Failed

Error Response

RequestReceived

TenantResolution

SchemaValidation

ConnectionAcquisition

QueryExecution

ResponseGeneration

ErrorHandling

All queries use schema.table notation

Runtime schema existence check

6.3 Nfe Schema Implementation
6.3.1 Nfe 4.00 Schema Service
The NFe 4.00 Schema Service manages the complete implementation of the Brazilian NFe specification within tenant-specific PostgreSQL schemas, ensuring compliance with fiscal document requirements.

NFe Schema Components

Schema Component	Tables Count	Key Relationships	Compliance Requirements
Core NFe Structure	1 main table (nfe)	empresas, pessoas, produtos	Complete NFe 4.00 specification
Document Items	1 main table (nfe_item)	nfe, produtos, tax tables	Item-level fiscal compliance
Tax Calculations	15+ tax-related tables	nfe_item, tax type enums	Brazilian tax law compliance
Referenced Documents	3 reference tables	nfe, external document types	Document traceability
Special Products	6 specialized tables	nfe_item, product categories	Sector-specific requirements
Schema Deployment Strategy

The service deploys the complete NFe schema structure within each tenant's dedicated PostgreSQL schema namespace, ensuring complete data isolation while maintaining fiscal compliance.

contains

has

has_tax

emits

references

TENANT_SCHEMA

string

schema_name

PK

uuid

tenant_id

FK

timestamp

created_at

string

status

NFE

uuid

id

PK

string

chave_acesso

UK

integer

numero_nf

uuid

empresa_id

FK

uuid

destinatario_id

FK

string

status

timestamp

data_emissao

NFE_ITEM

uuid

id

PK

uuid

nfe_id

FK

integer

numero_item

uuid

produto_id

FK

string

codigo_produto

string

descricao

numeric

quantidade_comercial

numeric

valor_unitario_comercial

NFE_ITEM_ICMS

uuid

id

PK

uuid

nfe_item_id

FK

string

origem

string

cst

numeric

valor_bc

numeric

aliquota

numeric

valor_icms

EMPRESAS

uuid

id

PK

string

cnpj

UK

string

razao_social

string

inscricao_estadual

string

uf

PRODUTOS

uuid

id

PK

uuid

empresa_id

FK

string

codigo

string

descricao

string

ncm

string

origem

6.3.2 Fiscal Compliance Service
The Fiscal Compliance Service ensures that all NFe operations within tenant schemas comply with Brazilian fiscal regulations and SEFAZ requirements.

Compliance Components

Compliance Area	Implementation	Validation Rules	Audit Requirements
NFe Structure Validation	Complete NFe 4.00 schema enforcement	All required fields and relationships	Immutable audit trail
Tax Calculation Validation	Brazilian tax law implementation	ICMS, IPI, PIS/COFINS calculations	Tax calculation logs
Document Numbering	Sequential numbering per tenant/series	Unique chave_acesso generation	Number sequence tracking
Status Transitions	NFe lifecycle state management	Valid status transition rules	Status change audit
Compliance Validation Flow

No

Yes

No

Yes

Validation Rules

CNPJ/CPF Format

Required Fields Check

Tax Rate Validation

UF Code Validation

NCM Code Validation

NFe Document Submission

Schema Structure Validation

Structure Valid?

Return Structure Errors

Business Rules Validation

Tax Calculation Validation

Document Numbering Validation

Status Transition Validation

All Validations Pass?

Return Validation Errors

Generate Chave de Acesso

Store NFe Document

Update Audit Trail

Return Success Response

6.4 Performance And Scalability Design
6.4.1 Schema Performance Optimization Service
The Schema Performance Optimization Service addresses the performance characteristics of schema-per-tenant architecture, where administrative tasks like backup and VACUUM scale poorly across thousands of tables, though this affects tens or hundreds of thousands of tables, not a few hundred, and doesn't affect SELECT queries.

Performance Optimization Components

Optimization Area	Implementation Strategy	Performance Impact	Monitoring Metrics
Query Performance	Schema-qualified query optimization	Eliminates search_path overhead	Query execution time, plan cache hit ratio
Index Management	Tenant-specific index optimization	Smaller indexes on tenant data	Index usage statistics, query performance
Connection Pooling	Optimized pool sizing per tenant	Reduced connection overhead	Pool utilization, connection wait time
Cache Management	Schema metadata caching	Reduced information_schema queries	Cache hit ratio, metadata lookup time
Performance Monitoring Architecture

Performance Monitor

Query Performance Tracker

Connection Pool Monitor

Schema Health Monitor

Resource Usage Monitor

Query Execution Time

Plan Cache Efficiency

Index Usage Statistics

Pool Utilization Metrics

Connection Wait Times

Pool Health Status

Schema Object Count

Schema Size Metrics

Maintenance Task Performance

CPU Usage per Tenant

Memory Usage per Tenant

I/O Performance per Tenant

Performance Alerts

6.4.2 Scalability Management Service
The Scalability Management Service handles the growth characteristics of the multi-tenant NFe system, ensuring performance remains consistent as tenant count increases.

Scalability Components

Scalability Factor	Management Strategy	Scaling Limits	Mitigation Approach
Tenant Count	Schema-per-tenant works best with relatively small number of fairly large tenants	Hundreds to low thousands	Horizontal database scaling
Schema Size	NFe 4.00 complete structure per tenant	50+ tables per schema	Optimized indexing and partitioning
Connection Management	Dynamic pool sizing based on tenant activity	Connection pool limits	Intelligent connection sharing
Administrative Tasks	Automated maintenance scheduling	VACUUM and backup scaling	Parallel maintenance operations
Scaling Decision Matrix

< 100

100-1000

> 1000
Yes

No

Yes

No

Yes

No

Scaling Decision Point

Tenant Count

Single Database Instance

Optimized Single Instance

Multi-Database Architecture

Standard Configuration

Enhanced Connection Pooling

Database Sharding Strategy

Performance Issues?

Resource Constraints?

Cross-Database Queries?

Optimize Queries

Monitor and Maintain

Scale Hardware

Continue Monitoring

Implement Query Federation

Standard Multi-DB Operations

6.5 Error Handling And Recovery
6.5.1 Schema Error Recovery Service
The Schema Error Recovery Service manages error scenarios specific to multi-tenant schema operations, providing robust recovery mechanisms for failed schema operations.

Error Recovery Components

Error Category	Recovery Strategy	Rollback Mechanism	Prevention Measures
Schema Creation Failures	Automatic rollback with cleanup	PostgreSQL transaction rollback	Pre-creation validation
Connection Pool Errors	Pool recreation with exponential backoff	Connection pool reset	Health monitoring
Migration Failures	Per-tenant rollback isolation	Version-specific rollback scripts	Migration validation
Query Execution Errors	Runtime validation and retry	Query-level error handling	Schema existence checks
Error Recovery Flow

System Error

Classify Error Type

Schema-related

Connection-related

Query-related

Migration-related

Rollback Schema Changes

Clean Partial Objects

Attempt Recovery

Retry with Backoff

Recreate Pool

Validate Connection

Validate Query Context

Retry Query

Fallback Strategy

Rollback Migration

Validate Rollback

Schedule Retry

Recovery Complete

Connection Restored

Query Handled

Migration Queued

ErrorDetected

ErrorClassification

SchemaError

ConnectionError

QueryError

MigrationError

SchemaRollback

SchemaCleanup

SchemaRecovery

ConnectionRetry

ConnectionRecreate

ConnectionValidation

QueryValidation

QueryRetry

QueryFallback

MigrationRollback

MigrationValidation

MigrationScheduleRetry

Tenant context preserved

Transaction-based rollback

6.5.2 Data Integrity Service
The Data Integrity Service ensures that multi-tenant operations maintain data consistency and integrity across all tenant schemas, with special attention to NFe fiscal compliance requirements.

Data Integrity Components

Integrity Aspect	Implementation	Validation Method	Recovery Procedure
Cross-Schema Isolation	Schema-level access control	Tenant data access validation	Isolation breach remediation
NFe Document Integrity	Fiscal document validation rules	Business rule compliance checks	Document correction procedures
Transaction Consistency	ACID compliance across operations	Transaction state monitoring	Transaction recovery
Referential Integrity	Foreign key constraint enforcement	Constraint violation detection	Data relationship repair
Integrity Validation Process

Audit Service
PostgreSQL
Fiscal Validator
Schema Validator
Data Integrity Service
Application
Audit Service
PostgreSQL
Fiscal Validator
Schema Validator
Data Integrity Service
Application
alt
[Fiscal Rules Valid]
[Fiscal Rules Invalid]
alt
[Schema Valid]
[Schema Invalid]
All operations logged for audit
ACID compliance maintained
Execute Multi-Tenant Operation
Validate Schema Context
Check Schema Existence
Schema Status
Schema Validation Result
Validate Fiscal Rules
Check NFe Compliance
Compliance Status
Fiscal Validation Result
Execute Operation
Operation Result
Log Successful Operation
Success Response
Log Fiscal Violation
Fiscal Error Response
Log Schema Error
Schema Error Response
6. Core Services Architecture
6.1 Service Components
6.1.1 Service Boundaries And Responsibilities
The NFe multi-tenant system implements a schema-per-tenant architecture using cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder. The core services are designed around the principle that schema-per-tenant approach balances isolation and scalability well.

Service Component	Primary Responsibility	Data Ownership	Integration Scope
Tenant Management Service	Tenant registration, validation, and lifecycle management	Tenant metadata and configuration	All tenant-aware services
Dynamic Schema Service	Runtime schema creation and NFe 4.00 structure deployment	Schema templates and creation logs	Database layer and migration engine
Connection Pool Service	Multi-tenant database connection management	Connection pool configurations and metrics	All data access operations
Migration Orchestration Service	Cross-tenant schema evolution and version management	Migration scripts and deployment status	Schema service and tenant registry
Service Isolation Strategy

Each service maintains clear boundaries with tenant or customer id on every table, making future scaling and safety of data easier. The architecture addresses the challenge that using SET search_path TO schemaname to switch between schemas would screw up a lot of things for almost all database libraries, as Diesel uses prepared statements internally and caches them.

6.1.2 Inter-service Communication Patterns
The services communicate through a combination of synchronous API calls for critical operations and asynchronous messaging for non-blocking workflows. The communication pattern addresses the unique requirements of runtime schema operations where many compile time guarantees are lost and we cannot verify that the tables/columns you ask for actually exist.

PostgreSQL
Migration Orchestration Service
Connection Pool Service
Dynamic Schema Service
Tenant Management Service
API Gateway
PostgreSQL
Migration Orchestration Service
Connection Pool Service
Dynamic Schema Service
Tenant Management Service
API Gateway
loop
[NFe Schema Deployment]
Schema-qualified queries required
Tenant-specific connection pools
Register New Tenant
Validate Tenant Data
Validate Database Connection
Test Connection
Connection Status
Connection Validated
Create Tenant Schema
BEGIN TRANSACTION
CREATE SCHEMA tenant_xyz
CREATE TABLE schema.table_name
CREATE INDEXES
ADD CONSTRAINTS
Validate Schema Structure
COMMIT TRANSACTION
Schema Created
Initialize Connection Pool
Create Tenant Pool
Pool Ready
Tenant Registration Complete
6.1.3 Service Discovery Mechanisms
The system implements a registry-based service discovery pattern optimized for the schema-per-tenant architecture. Service discovery addresses the challenge that schema per tenant would be a good idea and its hard to say without knowing the data isolation levels you require for each tenant.

Discovery Component	Implementation	Registration Method	Health Check Strategy
Service Registry	In-memory registry with PostgreSQL persistence	Startup registration with heartbeat updates	Active connection validation
Schema Registry	Dynamic schema metadata cache	Schema creation event registration	Schema existence verification
Connection Registry	Pool status and availability tracking	Pool initialization registration	Connection health monitoring
Migration Registry	Cross-tenant migration status tracking	Migration deployment registration	Version consistency checks
6.1.4 Load Balancing Strategy
Load balancing in the schema-per-tenant architecture focuses on tenant-aware distribution rather than traditional request-based balancing. The strategy considers that when you shard your data on a tenant or customer id all the data gets co-located on the same instance, meaning when you join you're not doing cross shard joins.

Tenant-Aware Load Balancing

Yes

No

Yes

No

Yes

No

Incoming Request

Extract Tenant Context

Tenant Pool Available?

Route to Tenant Pool

Create Tenant Pool

Initialize Pool Configuration

Validate Schema Connection

Register Pool

Acquire Connection

Connection Available?

Execute Schema-Qualified Query

Queue Request

Pool Scaling Enabled?

Scale Pool Size

Wait for Connection

Return Connection to Pool

Response to Client

6.2 Scalability Design
6.2.1 Horizontal And Vertical Scaling Approach
The schema-per-tenant architecture implements a hybrid scaling approach that addresses the reality that schema-per-tenant is best when you have a relatively small number of fairly large tenants. The scaling strategy considers that multiple databases used to be more resource intensive, but that was mostly fixed in PostgreSQL 9.3, though databases are much more heavy to create in the first place.

Scaling Dimension	Implementation Strategy	Trigger Conditions	Resource Allocation
Vertical Scaling	PostgreSQL resource optimization per schema	CPU > 80%, Memory > 85%	Dynamic connection pool sizing
Horizontal Scaling	Multi-database tenant distribution	> 1000 tenants per database	Database sharding by tenant groups
Connection Scaling	Tenant-specific pool management	Pool utilization > 90%	Adaptive pool sizing per tenant
Schema Scaling	Optimized schema deployment	Schema creation time > 30s	Parallel schema operations
Scaling Decision Matrix

< 500

500-2000

> 2000
Degraded

Acceptable

CPU/Memory

Connections

Schema Operations

Scaling Trigger

Tenant Count

Single Database Optimization

Enhanced Connection Management

Multi-Database Architecture

Vertical Scaling

Optimize Connection Pools

Schema-Level Indexing

Tenant Pool Optimization

Dynamic Pool Sizing

Connection Health Monitoring

Database Sharding

Tenant Distribution Strategy

Cross-Database Query Coordination

Performance Monitoring

Performance Threshold

Identify Bottleneck

Continue Monitoring

Resource Type

Optimize Schema Deployment

6.2.2 Auto-scaling Triggers And Rules
Auto-scaling in the schema-per-tenant architecture focuses on tenant-specific metrics rather than global system metrics. The triggers address the challenge that different databases makes connection pooling less efficient, which could be very important.

Tenant-Level Auto-Scaling Rules

Metric Category	Trigger Condition	Scaling Action	Cooldown Period
Connection Pool Utilization	> 85% for 5 minutes	Increase pool size by 25%	10 minutes
Schema Creation Time	> 45 seconds average	Parallel schema deployment	30 minutes
Query Response Time	> 500ms 95th percentile	Connection pool optimization	15 minutes
Migration Deployment	> 120 seconds per tenant	Batch size reduction	60 minutes
6.2.3 Resource Allocation Strategy
Resource allocation in the schema-per-tenant architecture requires careful balance between tenant isolation and resource efficiency. The strategy considers that resources such as CPU, memory, and disk space are not shared between tenants, with risk that some connections may remain underutilized while other tenants are unable to access the database.

Dynamic Resource Allocation

Premium

Standard

Basic

High

Low

Optimal

Resource Allocation Request

Analyze Tenant Profile

Tenant Tier

Dedicated Resources

Shared Resource Pool

Minimal Resource Allocation

Dedicated Connection Pool

Reserved CPU/Memory

Priority Schema Operations

Shared Connection Pool

Dynamic Resource Sharing

Standard Priority

Shared Pool with Limits

Resource Throttling

Lower Priority

Resource Monitor

Resource Utilization

Scale Up Resources

Scale Down Resources

Maintain Current Allocation

Increase Pool Size

Reduce Pool Size

6.2.4 Performance Optimization Techniques
Performance optimization in the schema-per-tenant architecture addresses the unique challenges of runtime schema operations. The techniques consider that you must always provide an explicit select clause when using dynamic schema operations.

Optimization Area	Technique	Implementation	Performance Impact
Query Performance	Schema-qualified queries with explicit selects	Runtime query modification	Eliminates search_path overhead
Connection Management	Tenant-specific connection pools	Pool per tenant strategy	Reduces connection contention
Schema Operations	Parallel schema creation	Concurrent table deployment	60% reduction in creation time
Cache Management	Schema metadata caching	TTL-based cache with invalidation	80% reduction in metadata queries
6.3 Resilience Patterns
6.3.1 Fault Tolerance Mechanisms
The schema-per-tenant architecture implements comprehensive fault tolerance mechanisms that address the unique challenges of multi-tenant database operations. The mechanisms consider that each tenant has its own separate database, providing the highest level of data isolation and security.

Multi-Tenant Fault Tolerance Architecture

Schema Error

Connection Error

Migration Error

Query Error

Fault Detection

Fault Type

Schema Fault Handler

Connection Fault Handler

Migration Fault Handler

Query Fault Handler

Isolate Affected Tenant

Rollback Schema Changes

Restore Previous State

Notify Tenant Admin

Connection Pool Recovery

Recreate Failed Connections

Validate Pool Health

Resume Operations

Halt Migration Process

Rollback Affected Tenants

Preserve Successful Migrations

Schedule Retry

Query Validation

Schema Existence Check

Retry with Fallback

Log Query Error

6.3.2 Disaster Recovery Procedures
Disaster recovery in the schema-per-tenant architecture provides granular recovery capabilities at the tenant level while maintaining system-wide consistency. The procedures address the challenge that backup and restore depends on whether you use log archiving (no difference) or SQL dumps (easier or harder to backup all clients together or separately).

Recovery Scope	Recovery Procedure	Recovery Time Objective	Recovery Point Objective
Single Tenant Schema	Schema-specific backup restoration	< 30 minutes	< 15 minutes data loss
Multiple Tenant Schemas	Parallel schema restoration	< 60 minutes	< 15 minutes data loss
Connection Pool Recovery	Pool recreation and validation	< 5 minutes	No data loss
Migration State Recovery	Version rollback and consistency check	< 45 minutes	Last successful migration
Tenant-Level Disaster Recovery Flow

Backup Service
PostgreSQL
Connection Pool Service
Dynamic Schema Service
Tenant Management Service
Disaster Recovery Service
Backup Service
PostgreSQL
Connection Pool Service
Dynamic Schema Service
Tenant Management Service
Disaster Recovery Service
alt
[Schema Corrupted]
[Schema Intact]
Schema-level recovery isolation
Pool recreation per tenant
Initiate Tenant Recovery
Assess Schema Damage
Query Schema State
Schema Status
Request Latest Backup
Backup Data
DROP SCHEMA IF EXISTS
Restore Schema from Backup
Validate Schema Structure
Verify Schema Integrity
Schema Recovery Status
Recreate Connection Pool
Test Connections
Connection Status
Pool Ready
Recovery Complete
Update Recovery Log
Notify Tenant Admin
6.3.3 Data Redundancy Approach
Data redundancy in the schema-per-tenant architecture implements multiple layers of protection while maintaining tenant isolation. The approach considers that isolation of resources between tenants is crucial and doesn't necessarily need to be physical; it simply requires ensuring that resources between tenants are not visible to each other.

Multi-Layer Redundancy Strategy

Redundancy Layer	Implementation	Scope	Backup Frequency
Schema-Level Backups	Individual tenant schema dumps	Per-tenant isolation	Every 6 hours
Transaction Log Backups	PostgreSQL WAL archiving	Database-wide consistency	Continuous
Configuration Backups	Tenant metadata and pool configurations	Service configuration	Every 24 hours
Migration State Backups	Version tracking and deployment status	Cross-tenant consistency	Before each migration
6.3.4 Failover Configurations
Failover configurations in the schema-per-tenant architecture provide tenant-aware failover capabilities that maintain data isolation during failure scenarios. The configurations address the reality that multiple databases can end up in very complex and bad design contributing to very poor performance and high maintenance overhead.

Tenant-Aware Failover Architecture

System Start

Fault Occurs

Isolate Affected Tenants

Begin Failover

Schema-Level Failure

Connection Failure

Service Failure

Restore Schema

Recreate Pools

Restart Service

Validate Recovery

Validate Pools

Validate Service

Recovery Successful

Recovery Failed

Manual Fix Applied

System Shutdown

HealthyOperation

FailureDetected

TenantIsolation

FailoverInitiation

SchemaFailover

ConnectionFailover

ServiceFailover

SchemaRecovery

PoolRecreation

ServiceRestart

ValidationPhase

ManualIntervention

Unaffected tenants continue operation

Per-tenant validation required

6.3.5 Service Degradation Policies
Service degradation policies in the schema-per-tenant architecture provide graceful degradation capabilities that maintain essential functionality while protecting system stability. The policies consider that there's a clear difference between having 1 tenant with 10 million rows and having 1,000 tenants with 10 million rows each, with that single report now running 1,000 times over.

Degradation Level	Trigger Conditions	Service Limitations	Recovery Actions
Level 1 - Minor	CPU > 80%, Response time > 200ms	Reduced connection pool sizes	Automatic scaling
Level 2 - Moderate	CPU > 90%, Multiple tenant failures	New tenant registration disabled	Manual intervention required
Level 3 - Severe	System instability, Cascade failures	Read-only mode for affected tenants	Emergency procedures
Level 4 - Critical	Database unavailable	Complete service suspension	Disaster recovery activation
Graceful Degradation Flow

Healthy

Degraded

Level 1

Level 2

Level 3

Level 4

Yes

No

Performance Monitor

System Health

Normal Operations

Assess Degradation Level

Degradation Level

Reduce Connection Pools

Disable New Registrations

Enable Read-Only Mode

Suspend Service

Monitor Recovery

Prioritize Critical Tenants

Preserve Data Integrity

Activate Disaster Recovery

Recovery Detected?

Restore Service Level

Escalate Degradation

Higher Degradation Level

The Core Services Architecture for the NFe multi-tenant system provides a comprehensive framework for managing schema-per-tenant operations while maintaining high availability, scalability, and data isolation. The architecture addresses the unique challenges of runtime schema operations in Rust with Diesel ORM, providing robust service boundaries, intelligent scaling mechanisms, and comprehensive resilience patterns that ensure reliable operation across hundreds or thousands of tenant schemas.

6.2 Database Design
6.2.1 Schema Design
6.2.1.1 Multi-tenant Architecture Strategy
The NFe multi-tenant system implements a schema-per-tenant approach using a shared database, but each tenant gets a separate schema, as it balances isolation and scalability well. This architecture addresses the critical requirement for complete data isolation where absolutely nothing should be shared between two tenants, which is essential for Brazilian fiscal document compliance.

The system leverages PostgreSQL namespaces ('schemas') heavily, supporting a typical multi-tenancy setup where the structure of tables is known at compile time, but the namespace for the tables is generated dynamically. This approach provides the highest level of data isolation and security while maintaining operational efficiency.

Architecture Component	Implementation Strategy	Isolation Level	Scalability Impact
Schema Namespace	Dynamic tenant-specific schema creation	Complete data separation	Good for hundreds to low thousands of tenants
Connection Management	Tenant-aware connection pools	Pool-level isolation	Connection pooling less efficient but manageable
Query Execution	Schema-qualified table references	Runtime schema validation	Eliminates search_path complications
Migration Deployment	Per-tenant schema evolution	Individual tenant rollback capability	Gradual rollout across tenants with co-located data
6.2.1.2 Nfe 4.00 Schema Structure
The complete NFe 4.00 schema structure is deployed within each tenant's dedicated PostgreSQL schema namespace, ensuring full compliance with Brazilian government requirements for electronic invoicing using XML file format with version 3.10 layout.

Core Entity Relationships

contains

emits

receives

has

references

has_tax

owns

TENANT_SCHEMA

string

schema_name

PK

uuid

tenant_id

FK

timestamp

created_at

string

status

EMPRESAS

uuid

id

PK

string

cnpj

UK

14 chars

string

cpf

11 chars

string

razao_social

60 chars

string

inscricao_estadual

14 chars

uf_type

uf

codigo_regime_tributario

crt

timestamp

created_at

timestamp

updated_at

PESSOAS

uuid

id

PK

string

tipo

J/F/E

string

cnpj

14 chars

string

cpf

11 chars

string

id_estrangeiro

20 chars

string

razao_social

60 chars

uf_type

uf

timestamp

created_at

timestamp

updated_at

NFE

uuid

id

PK

string

chave_acesso

UK

44 chars

integer

numero_nf

integer

serie

modelo_documento

modelo

uuid

empresa_id

FK

uuid

destinatario_id

FK

status_nfe

status

timestamp

data_emissao

numeric

valor_total

15,2

timestamp

created_at

timestamp

updated_at

NFE_ITEM

uuid

id

PK

uuid

nfe_id

FK

integer

numero_item

uuid

produto_id

FK

string

codigo_produto

60 chars

string

descricao

120 chars

string

ncm

8 chars

string

cfop

4 chars

numeric

quantidade_comercial

15,4

numeric

valor_unitario_comercial

21,10

numeric

valor_bruto

15,2

timestamp

created_at

PRODUTOS

uuid

id

PK

uuid

empresa_id

FK

string

codigo

60 chars

string

descricao

120 chars

string

ncm

8 chars

origem_mercadoria

origem

timestamp

created_at

timestamp

updated_at

NFE_ITEM_ICMS

uuid

id

PK

uuid

nfe_item_id

FK

origem_mercadoria

origem

cst_icms

cst

csosn_type

csosn

numeric

valor_bc

15,2

numeric

aliquota

5,4

numeric

valor_icms

15,2

6.2.1.3 Indexing Strategy
The indexing strategy is optimized for schema-per-tenant architecture where there's a clear difference between having 1 tenant with 10 million rows and having 1,000 tenants with 10 million rows each. Each tenant schema maintains its own optimized indexes for NFe operations.

Table	Index Type	Columns	Purpose	Performance Impact
nfe	Primary	id	Unique identification	O(log n) lookups
nfe	Unique	chave_acesso	NFe legal access key	Fiscal compliance validation
nfe	Composite	empresa_id, serie, numero_nf, modelo	Business key uniqueness	Prevents duplicate NFe numbers
nfe	Single	status	Status-based queries	Fast status filtering
nfe	Single	data_emissao	Date range queries	Temporal data access
nfe_item	Primary	id	Item identification	O(log n) item access
nfe_item	Foreign Key	nfe_id	Parent NFe relationship	Efficient joins
nfe_item	Composite	nfe_id, numero_item	Item ordering within NFe	Sequential item access
empresas	Unique	cnpj	Company identification	Fast company lookups
pessoas	Composite	tipo, cnpj, cpf	Person identification	Multi-field person queries
6.2.1.4 Partitioning Approach
The schema-per-tenant architecture provides natural partitioning at the tenant level, eliminating the need for traditional table partitioning strategies. Each tenant's data is completely isolated within its dedicated schema namespace.

Tenant-Level Partitioning Benefits

Multi-Tenant Database

Tenant Schema A

Tenant Schema B

Tenant Schema N

NFe Tables A

Tax Tables A

Product Tables A

NFe Tables B

Tax Tables B

Product Tables B

NFe Tables N

Tax Tables N

Product Tables N

6.2.1.5 Replication Configuration
The replication strategy addresses the unique requirements of schema-per-tenant architecture while maintaining data consistency and availability for NFe operations.

Replication Level	Implementation	Purpose	Recovery Objective
Database-Level	PostgreSQL streaming replication	Complete system availability	RTO: 5 minutes, RPO: 1 minute
Schema-Level	Logical replication per tenant	Selective tenant replication	RTO: 15 minutes, RPO: 5 minutes
Table-Level	Critical NFe tables only	Essential data protection	RTO: 30 minutes, RPO: 15 minutes
Cross-Region	Async replication for compliance	Geographic data distribution	RTO: 60 minutes, RPO: 30 minutes
6.2.1.6 Backup Architecture
The backup architecture provides granular recovery capabilities at the tenant level while maintaining system-wide consistency for NFe compliance requirements.

Multi-Level Backup Strategy

Backup Orchestrator

Database-Level Backup

Schema-Level Backup

Table-Level Backup

Full Database Dump

WAL Archive Backup

Tenant Schema Dump

Schema Metadata Backup

Critical NFe Tables

Tax Calculation Tables

Daily Full Backup

Continuous WAL Backup

Per-Tenant Backup

Schema Structure Backup

Hourly NFe Backup

Tax Data Backup

6.2.2 Data Management
6.2.2.1 Migration Procedures
The migration system addresses the challenge that tenant-schema is bad for rolling out universal schema changes quickly, but good for deploying schema changes as a gradual rollout across tenants. The system implements controlled migration deployment with per-tenant rollback capabilities.

Migration Deployment Strategy

Migration Phase	Scope	Validation	Rollback Strategy
Pre-Migration	Schema structure validation	Template integrity check	N/A - validation only
Batch Deployment	10-50 tenants per batch	Per-tenant schema validation	Individual tenant rollback
Validation Phase	Schema completeness check	NFe compliance verification	Batch-level rollback
Completion Phase	Migration status update	Cross-tenant consistency	Full migration rollback
The migration engine utilizes Diesel CLI's --diff-schema feature to generate migrations based on current schema definition and database state, ensuring consistent schema evolution across all tenant schemas.

6.2.2.2 Versioning Strategy
Each tenant schema maintains independent version tracking, enabling selective migration deployment and rollback capabilities essential for NFe compliance management.

Version Control Architecture

Version Tracker
Tenant Schema
Tenant Registry
Migration Service
Version Tracker
Tenant Schema
Tenant Registry
Migration Service
alt
[Migration Success]
[Migration Failure]
loop
[For Each Tenant Batch]
Get Migration Targets
Tenant List with Versions
Begin Migration Transaction
Record Migration Start
Apply Schema Changes
Update Version Number
Commit Transaction
Record Failure
Rollback Transaction
Update Global Migration Status
6.2.2.3 Archival Policies
NFe archival policies comply with Brazilian requirements to archive e-invoices for a minimum of five years. The schema-per-tenant architecture enables tenant-specific archival strategies while maintaining compliance.

Data Category	Retention Period	Archive Strategy	Compliance Requirement
Active NFe Documents	2 years online	Hot storage in tenant schema	Immediate access for SEFAZ
Historical NFe Documents	3-5 years	Warm storage with compression	Legal retention requirement
Tax Calculation Data	5 years	Cold storage with encryption	Audit trail preservation
Schema Metadata	Indefinite	Version-controlled storage	System recovery capability
6.2.2.4 Data Storage And Retrieval Mechanisms
The system implements tenant-aware data access patterns that address the challenge where using SET search_path TO schemaname would screw up a lot of things for database libraries, requiring schema-qualified table references.

Dynamic Schema Access Pattern

// Example implementation for tenant-aware data access
use diesel_dynamic_schema::table;

pub struct TenantDataAccess {
    schema_name: String,
    connection_pool: Pool<ConnectionManager<PgConnection>>,
}

impl TenantDataAccess {
    pub fn get_nfe_by_chave(&self, chave_acesso: &str) -> Result<NfeData, DatabaseError> {
        let nfe_table = table(&format!("{}.nfe", self.schema_name));
        let chave_column = nfe_table.column::<diesel::sql_types::Text, _>("chave_acesso");
        
        let mut conn = self.connection_pool.get()?;
        
        nfe_table
            .select((
                nfe_table.column::<diesel::sql_types::Uuid, _>("id"),
                chave_column,
                nfe_table.column::<diesel::sql_types::Text, _>("status")
            ))
            .filter(chave_column.eq(chave_acesso))
            .first::<(uuid::Uuid, String, String)>(&mut conn)
            .map(|(id, chave, status)| NfeData { id, chave_acesso: chave, status })
    }
}
6.2.2.5 Caching Policies
The caching strategy addresses the performance characteristics of schema-per-tenant architecture where performance is great and Postgres scales well with many schemata and tables, but doesn't scale well when querying information_schema.

Cache Layer	Data Type	TTL	Invalidation Strategy	Performance Impact
Schema Metadata	Table definitions, column info	15 minutes	Schema modification events	80% reduction in metadata queries
Connection Pool	Active connections per tenant	N/A (managed)	Pool health monitoring	60% improvement in connection acquisition
Query Results	Frequently accessed NFe data	5 minutes	NFe status changes	40% reduction in database queries
Tenant Registry	Tenant configuration data	30 minutes	Tenant registration events	90% reduction in registry lookups
6.2.3 Compliance Considerations
6.2.3.1 Data Retention Rules
NFe data retention policies ensure compliance with Brazilian fiscal regulations while optimizing storage utilization across tenant schemas. The system implements mandatory inclusion of unique invoice numbers, supplier identification, buyer identification, and tax information in all retained documents.

Compliance-Driven Retention Matrix

Document Type	Legal Requirement	Retention Period	Storage Tier	Access Pattern
Authorized NFe	SEFAZ compliance	5 years minimum	Hot → Warm → Cold	Frequent → Occasional → Rare
Rejected NFe	Audit trail	3 years	Warm storage	Audit access only
Tax Calculations	Fiscal audit	5 years	Encrypted cold storage	Compliance queries
Digital Certificates	Certificate lifecycle	Until expiration + 1 year	Secure vault	Authentication only
6.2.3.2 Backup And Fault Tolerance Policies
The fault tolerance strategy provides strong data isolation and security with each tenant having its own separate database schema, enabling granular recovery without cross-tenant impact.

Multi-Tier Fault Tolerance Architecture

Primary Database

Synchronous Replica

Asynchronous Replica

Hot Standby

Disaster Recovery Site

Tenant Schema A

Schema Backup A

Tenant Schema B

Schema Backup B

Tenant Schema N

Schema Backup N

Point-in-Time Recovery A

Point-in-Time Recovery B

Point-in-Time Recovery N

6.2.3.3 Privacy Controls
Privacy controls implement tenant-level data isolation with schema-based access control, ensuring resources between tenants are not visible to each other while maintaining NFe compliance requirements.

Privacy Layer	Implementation	Scope	Enforcement Method
Schema Isolation	PostgreSQL namespace separation	Complete tenant data	Database-level access control
Connection Security	Tenant-specific database users	Connection-level isolation	Role-based authentication
Query Validation	Runtime schema existence checks	Query-level validation	Application-level enforcement
Data Encryption	Column-level encryption for sensitive data	PII and financial data	Transparent data encryption
6.2.3.4 Audit Mechanisms
The audit system provides comprehensive tracking of all NFe operations while maintaining tenant isolation and compliance with Brazilian fiscal audit requirements.

Comprehensive Audit Trail Architecture

Compliance Service
Audit Log
Tenant Schema
Audit Service
Application
Compliance Service
Audit Log
Tenant Schema
Audit Service
Application
Immutable audit trail
NFe compliance validation
Begin Audit Context
Execute NFe Operation
Operation Result
Record Operation Details
Record Tenant Context
Record Timestamp
Record User Identity
Validate Compliance
Compliance Status
Record Compliance Result
Audit Complete
6.2.3.5 Access Controls
Access control implementation leverages schema-per-tenant architecture to provide granular security while maintaining operational efficiency for NFe processing.

Access Level	Scope	Authentication	Authorization	Audit Logging
System Administrator	Cross-tenant operations	Multi-factor authentication	Role-based access control	All operations logged
Tenant Administrator	Single tenant schema	Certificate-based auth	Schema-level permissions	Tenant-scoped logging
Application User	NFe operations only	JWT token validation	Operation-specific permissions	Transaction logging
Read-Only User	Query access only	Basic authentication	Read-only schema access	Query logging
6.2.4 Performance Optimization
6.2.4.1 Query Optimization Patterns
Query optimization addresses the unique challenges of schema-per-tenant architecture where all constructs must use schema-qualified table references, maintaining Diesel and Rust query validation while handling runtime schema determination.

Schema-Qualified Query Optimization

Optimization Technique	Implementation	Performance Gain	Complexity Impact
Prepared Statement Caching	Per-schema statement preparation	40% query performance improvement	Increased memory usage
Index Hint Injection	Runtime index selection	25% reduction in query planning time	Additional query complexity
Connection Pool Warming	Pre-established schema connections	60% reduction in connection overhead	Higher resource utilization
Query Plan Caching	Schema-specific plan storage	30% improvement in repeated queries	Cache management overhead
6.2.4.2 Caching Strategy
The caching strategy optimizes for schema-per-tenant performance characteristics while maintaining data consistency across NFe operations.

Multi-Level Caching Architecture

Application Layer

Query Result Cache

Schema Metadata Cache

Connection Pool Cache

NFe Document Cache

Tax Calculation Cache

Product Information Cache

Table Definition Cache

Index Information Cache

Constraint Metadata Cache

Active Connection Cache

Pool Configuration Cache

Health Status Cache

Redis Cluster

In-Memory Cache

6.2.4.3 Connection Pooling
Connection pooling strategy addresses the challenge that different databases make connection pooling less efficient, with risk of underutilized connections while other tenants cannot access the database.

Tenant-Aware Connection Pool Management

Pool Strategy	Configuration	Scaling Behavior	Resource Efficiency
Dedicated Pools	5-20 connections per tenant	Linear scaling with tenant count	Lower efficiency, higher isolation
Shared Pools	50-100 connections total	Logarithmic scaling	Higher efficiency, managed isolation
Hybrid Pools	Premium tenants get dedicated pools	Tiered scaling based on tenant class	Balanced efficiency and isolation
Dynamic Pools	Auto-scaling based on load	Adaptive scaling	Optimal resource utilization
6.2.4.4 Read/write Splitting
Read/write splitting implementation leverages schema-per-tenant architecture to optimize NFe query performance while maintaining data consistency.

Schema-Aware Read/Write Distribution

Write

Read

Fresh

Eventual

NFe Application Request

Operation Type

Primary Database

Read Replica Selection

Tenant Schema Write

Write Confirmation

Data Freshness Required

Primary Database Read

Read Replica

Tenant Schema Read - Primary

Tenant Schema Read - Replica

Fresh Data Response

Cached Data Response

6.2.4.5 Batch Processing Approach
Batch processing optimization addresses the scalability challenges of schema-per-tenant architecture for NFe bulk operations and maintenance tasks.

Batch Operation	Processing Strategy	Parallelization	Performance Target
NFe Status Updates	Per-tenant batch processing	10 concurrent tenant batches	< 5 minutes for 1000 NFe updates
Tax Recalculation	Schema-parallel processing	CPU core count parallelization	< 30 minutes for full tenant recalc
Schema Migrations	Controlled rollout batches	5-10 tenants per batch	< 2 minutes per tenant migration
Backup Operations	Tenant-level parallel backup	Storage I/O limited parallelization	< 15 minutes per tenant backup
Batch Processing Flow Optimization

00:00
00:01
00:02
00:03
00:04
00:05
00:06
00:07
00:08
00:09
00:10
00:11
00:12
00:13
00:14
00:15
NFe Processing T1-T10
NFe Processing T11-T20
NFe Processing T21-T30
Status Update
Validation T1-T10
Validation T11-T20
Validation T21-T30
Final Validation
Tenant Batch 1
Tenant Batch 2
Tenant Batch 3
Completion
Multi-Tenant Batch Processing Timeline
The Database Design for the NFe multi-tenant system provides a comprehensive foundation for secure, scalable, and compliant fiscal document management. The schema-per-tenant architecture ensures complete data isolation while maintaining operational efficiency, addressing the unique requirements of Brazilian NFe compliance through carefully designed data structures, optimized performance patterns, and robust backup and recovery mechanisms.

6.3 Integration Architecture
6.3.1 Api Design
6.3.1.1 Protocol Specifications
The NFe multi-tenant system implements a RESTful API architecture with schema-per-tenant integration patterns. The system provides tools to work with cases where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder, requiring specialized API design patterns for tenant-aware operations.

Protocol Component	Specification	Implementation	Multi-Tenant Considerations
HTTP Protocol	HTTP/1.1 and HTTP/2	RESTful API with tenant context headers	Tenant identification via headers or subdomains
Content Type	application/json	JSON serialization with Serde	Tenant-specific data serialization
Schema Validation	Runtime schema validation	Many compile time guarantees are lost. We cannot verify that the tables/columns you ask for actually exist, or that the types you state are correct	Dynamic validation per tenant schema
Connection Management	PostgreSQL native protocol	Connection pooling via r2d2	Tenant-specific connection pools
6.3.1.2 Authentication Methods
The authentication framework addresses the unique challenges of multi-tenant schema architecture where Citus 12 there is new support for schema based sharding. Coupled with the latest release of PgBouncer, you also have better connection management and scaling without having to give up schema based sharding.

Multi-Tenant Authentication Flow

PostgreSQL
Dynamic Schema Service
Tenant Management Service
Authentication Service
API Gateway
Client Application
PostgreSQL
Dynamic Schema Service
Tenant Management Service
Authentication Service
API Gateway
Client Application
Tenant-aware authentication
Runtime schema validation
Request with Tenant Context
Validate Authentication Token
Resolve Tenant Information
Validate Tenant Schema
Check Schema Existence
Schema Status
Schema Validation Result
Tenant Context Validated
Authentication Success
Authorized Response
Authentication Method	Implementation	Tenant Integration	Security Level
JWT Tokens	Bearer token with tenant claims	Tenant ID embedded in token payload	High - cryptographically signed
API Keys	Tenant-specific API key generation	Key-to-tenant mapping in registry	Medium - requires secure storage
Certificate-based	X.509 certificates per tenant	Certificate CN contains tenant identifier	Very High - PKI infrastructure
OAuth 2.0	Multi-tenant OAuth provider	Tenant-scoped authorization codes	High - industry standard
6.3.1.3 Authorization Framework
The authorization framework implements tenant-aware access control that leverages tenant-schema is best when you have a relatively small number of fairly large tenants. An example of this would be an accounting application, with only paid subscription users.

Role-Based Access Control (RBAC) with Tenant Isolation

Role Level	Scope	Permissions	Schema Access
System Administrator	Cross-tenant operations	Full system access, tenant management	All schemas with administrative privileges
Tenant Administrator	Single tenant scope	Full tenant data access, user management	Single tenant schema with full permissions
Tenant User	Single tenant scope	Limited data access based on role	Single tenant schema with restricted permissions
Read-Only User	Single tenant scope	Query-only access	Single tenant schema with SELECT permissions
6.3.1.4 Rate Limiting Strategy
Rate limiting implementation addresses the performance characteristics where There's some specific issues with large numbers of tables, namely administrative tasks like backup and VACUUM. However, by "large numbers of tables" we're talking 10s or 100s of thousands, not a few hundred. And those issues don't affect SELECT queries.

Tenant-Aware Rate Limiting Architecture

No

Yes

Premium

Standard

Basic

Incoming Request

Extract Tenant Context

Tenant Rate Limit Check

Rate Limit Exceeded?

Process Request

Apply Rate Limiting

Tenant Tier

Higher Rate Limits

Standard Rate Limits

Lower Rate Limits

Execute Schema Operation

Update Rate Limit Counters

Return Response

Premium Tier Processing

Standard Tier Processing

Basic Tier Processing

Rate Limit Type	Scope	Limit Configuration	Enforcement Method
Request Rate	Per tenant per minute	1000 requests/minute (configurable)	Token bucket algorithm
Schema Operations	Per tenant per hour	100 schema operations/hour	Sliding window counter
Connection Pool	Per tenant concurrent	50 concurrent connections	Connection pool limits
Data Transfer	Per tenant per day	10GB data transfer/day	Bandwidth monitoring
6.3.1.5 Versioning Approach
API versioning addresses the challenge that tenant-schema is bad for rolling out a universal schema change quickly, but good for deploying schema changes as a gradual rollout across tenants.

Multi-Tenant API Versioning Strategy

Versioning Method	Implementation	Tenant Impact	Migration Strategy
URL Path Versioning	`/api/v1/tenants/{tenant_id}/nfe`	Per-tenant version support	Gradual tenant migration
Header Versioning	`API-Version: 2024-01-15`	Version per request	Flexible version selection
Schema Versioning	Tenant schema version tracking	Independent schema evolution	Per-tenant schema upgrades
Backward Compatibility	Legacy endpoint support	Maintains existing integrations	Deprecation timeline management
6.3.1.6 Documentation Standards
API documentation addresses the complexity of multi-tenancy setup where the structure of tables is known at compile time, but the namespace for the tables is generated dynamically.

OpenAPI Specification with Multi-Tenant Extensions

openapi: 3.0.3
info:
  title: NFe Multi-Tenant API
  version: 1.0.0
  description: Brazilian NFe management with schema-per-tenant architecture

servers:
  - url: https://api.nfe-system.com/v1
    description: Production API

paths:
  /tenants/{tenantId}/nfe:
    post:
      summary: Create NFe document
      parameters:
        - name: tenantId
          in: path
          required: true
          schema:
            type: string
            format: uuid
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/NFeRequest'
      responses:
        '201':
          description: NFe created successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/NFeResponse'

components:
  schemas:
    NFeRequest:
      type: object
      properties:
        numero_nf:
          type: integer
          description: NFe sequential number
        empresa_id:
          type: string
          format: uuid
          description: Issuing company identifier
6.3.2 Message Processing
6.3.2.1 Event Processing Patterns
The event processing architecture addresses the unique requirements of schema-per-tenant operations where Schema isolation in PostgreSQL provides an elegant middle ground for multi-tenant microservices architectures. It balances isolation and resource efficiency while maintaining operational simplicity.

Tenant-Aware Event Processing Flow

PostgreSQL
Schema Service
Event Handler
Tenant Router
Message Queue
Event Producer
PostgreSQL
Schema Service
Event Handler
Tenant Router
Message Queue
Event Producer
Tenant-specific routing
Dynamic schema operations
Publish Tenant Event
Route by Tenant ID
Deliver to Tenant Handler
Validate Tenant Schema
Execute Schema-Qualified Query
Query Result
Operation Result
Acknowledge Message
Event Type	Processing Pattern	Tenant Isolation	Error Handling
NFe Creation	Async processing with schema validation	Per-tenant event queues	Tenant-specific error queues
Schema Migration	Sequential per-tenant processing	Isolated migration per tenant	Rollback per tenant
Tenant Registration	Synchronous validation and setup	New schema creation	Registration failure cleanup
Status Updates	Batch processing per tenant	Tenant-grouped batches	Partial batch recovery
6.3.2.2 Message Queue Architecture
The message queue architecture leverages the fact that you have a customer or tenant id on every table. A nuance of this approach is it's actually a good step to denormalize and have your customer or tenant id on every table. Database purists may tell you this is a bad idea, but it makes future scaling and safety of data a bit easier. Once you have your customer id on every table you want to ensure you're joining on that key.

Multi-Tenant Message Queue Design

Queue Component	Configuration	Tenant Handling	Scalability
Topic Partitioning	Partition by tenant ID hash	Even tenant distribution	Horizontal scaling
Consumer Groups	Tenant-aware consumer groups	Parallel tenant processing	Auto-scaling consumers
Dead Letter Queues	Per-tenant DLQ	Isolated error handling	Tenant-specific recovery
Message Ordering	Per-tenant ordering guarantee	FIFO within tenant	Ordered processing
6.3.2.3 Stream Processing Design
Stream processing addresses the performance characteristics where shared-table works better for connection pooling, as all connections can use the same pool, but schema-per-tenant requires specialized stream processing patterns.

Real-Time NFe Processing Stream

Yes

No

Yes

No

NFe Input Stream

Tenant Classifier

Schema Validator

Schema Valid?

Business Rules Engine

Schema Error Stream

Tax Calculator

Compliance Validator

Compliant?

Authorized NFe Stream

Rejected NFe Stream

Database Writer

Error Handler

Success Notifications

Error Notifications

6.3.2.4 Batch Processing Flows
Batch processing implementation considers that shared-table works better for situations when you have a lot of tenants, and a lot of your tenants have very little data. An example of this would be a social medial mobile application which permits free accounts and thus has thousands of abandoned accounts, while schema-per-tenant works better for substantial data per tenant.

Tenant-Parallel Batch Processing

Batch Operation	Processing Strategy	Tenant Coordination	Performance Target
NFe Status Sync	Parallel tenant batches	Independent tenant processing	< 5 minutes for 1000 NFe
Tax Recalculation	Schema-parallel execution	CPU-bound parallelization	< 30 minutes per tenant
Backup Operations	Tenant-level parallel backup	I/O-optimized scheduling	< 15 minutes per tenant
Report Generation	Tenant-specific report batches	Memory-efficient processing	< 10 minutes per report
6.3.2.5 Error Handling Strategy
Error handling addresses the complexity where Someone there suggested to use SET search_path TO schemaname to switch between schemas. I would be very careful there as I assume this would screw up a lot of things for almost all database libraries.

Multi-Tenant Error Handling Flow

Schema Error

Tenant Error

System Error

No

Yes

Error Detected

Error Classification

Schema Error Handler

Tenant Error Handler

System Error Handler

Validate Schema Existence

Schema Exists?

Trigger Schema Creation

Schema Repair Process

Isolate Tenant Impact

Tenant-Specific Recovery

System-Wide Recovery

Cross-Tenant Impact Assessment

Schema Creation Queue

Schema Validation

Tenant Recovery Queue

System Recovery Queue

6.3.3 External Systems
6.3.3.1 Third-party Integration Patterns
The integration architecture addresses the Brazilian NFe ecosystem requirements while maintaining While schema isolation provides logical separation, all tenants still share the same database resources. Implement: Resource governance using PostgreSQL resource queues, Monitoring at the schema level to identify problematic tenants, Connection pooling optimized for multi-tenant workloads.

SEFAZ Integration Architecture

Tenant Schema
Response Queue
SEFAZ Webservice
Integration Gateway
NFe System
Tenant Schema
Response Queue
SEFAZ Webservice
Integration Gateway
NFe System
External SOAP integration
Tenant-aware response handling
Submit NFe for Authorization
Send NFe XML (SOAP)
Authorization Response
Queue Response by Tenant
Update NFe Status
Status Update Confirmation
Integration Type	Protocol	Tenant Handling	Error Recovery
SEFAZ Authorization	SOAP over HTTPS	Tenant-specific certificates	Per-tenant retry queues
Tax Rate Services	REST API	Tenant-cached tax rates	Fallback to local rates
Address Validation	REST API	Tenant-specific API keys	Graceful degradation
Banking Integration	REST/SOAP	Tenant banking credentials	Secure credential storage
6.3.3.2 Legacy System Interfaces
Legacy system integration addresses the challenge of migrating existing NFe systems to the schema-per-tenant architecture while maintaining The diesel_migrations crate provides the embed_migrations! macro, allowing you to embed migration scripts in the final binary. Once your code uses it, you can simply include connection.run_pending_migrations(MIGRATIONS) at the start of your main function to run migrations every time the application starts.

Legacy System Migration Pattern

Legacy System Type	Migration Strategy	Data Mapping	Validation Approach
Single-tenant NFe	Schema migration per tenant	Direct table mapping	Schema structure validation
Shared-table NFe	Tenant data extraction	Tenant ID-based filtering	Data integrity verification
File-based Systems	Batch import processing	XML/CSV parsing	Format validation
External Databases	ETL pipeline integration	Custom data transformation	Cross-system validation
6.3.3.3 Api Gateway Configuration
The API gateway configuration implements tenant-aware routing that addresses While schema based sharding is available in Citus 12 it's still generally recommended for a more medium level of tenants, generally not to exceed several thousand of tenants and thus corresponding schemas. If you're anticipating millions of tenants the tenant discriminator approach is still recommended.

Multi-Tenant API Gateway Architecture

Valid

Invalid

Client Request

API Gateway

Tenant Resolver

Tenant Validation

Route to Tenant Service

Return Tenant Error

Load Balancer

Tenant-Aware Service Instance

Schema Connection Pool

PostgreSQL Tenant Schema

Rate Limiter

Authentication

Monitoring

Gateway Component	Configuration	Tenant Integration	Performance Impact
Routing Rules	Tenant-based routing	Subdomain or path-based	< 5ms routing overhead
Load Balancing	Tenant-aware distribution	Consistent tenant hashing	Even tenant distribution
Circuit Breaker	Per-tenant circuit breakers	Isolated failure handling	Tenant-specific recovery
Caching	Tenant-scoped cache	Schema metadata caching	80% cache hit ratio target
6.3.3.4 External Service Contracts
External service contracts define the integration boundaries for the multi-tenant NFe system, ensuring compliance with Brazilian fiscal regulations while maintaining tenant isolation.

Service Level Agreements (SLAs)

External Service	Availability SLA	Response Time SLA	Tenant Isolation	Error Handling
SEFAZ Webservices	99.5% uptime	< 30 seconds	Per-tenant certificates	Automatic retry with backoff
Tax Calculation API	99.9% uptime	< 500ms	Tenant-specific rate limits	Fallback to cached rates
Address Validation	99.0% uptime	< 1 second	Shared service with tenant context	Graceful degradation
Banking Services	99.8% uptime	< 2 seconds	Tenant-specific credentials	Secure error logging
Integration Monitoring and Alerting

Yes

No

External Service Monitor

Service Health Check

Service Available?

Update Service Status

Trigger Alert

Tenant Impact Assessment

Affected Tenants Notification

Fallback Service Activation

Performance Metrics Collection

SLA Compliance Tracking

Service Quality Reports

6.3.4 Integration Flow Diagrams
6.3.4.1 Tenant Registration And Schema Creation Flow
The tenant registration flow implements the complete lifecycle from tenant validation to schema deployment, addressing the requirement that you must always provide an explicit select clause when using this crate for dynamic schema operations.

No

Yes

No

Yes

No

Yes

Tenant Registration Request

Validate Tenant Data

Validation Passed?

Return Validation Errors

Test Database Connection

Connection Valid?

Return Connection Error

Generate Schema Name

Begin Schema Creation Transaction

Create PostgreSQL Schema

Deploy NFe 4.00 Structure

Create Enums and Types

Create Core Tables

Create Tax Tables

Create Indexes

Apply Constraints

Validate Schema Completeness

Schema Valid?

Rollback Transaction

Commit Transaction

Cleanup Failed Schema

Return Schema Error

Initialize Connection Pool

Register Tenant Metadata

Return Success Response

6.3.4.2 Nfe Document Processing Integration Flow
The NFe processing flow demonstrates the integration between tenant-aware services and external SEFAZ systems while maintaining schema isolation.

PostgreSQL
SEFAZ Integration
Tax Calculator
Schema Service
NFe Service
API Gateway
Client Application
PostgreSQL
SEFAZ Integration
Tax Calculator
Schema Service
NFe Service
API Gateway
Client Application
Tenant-specific schema operations
External system integration
Submit NFe Document
Process NFe Request
Validate Tenant Schema
Check Schema Existence
Schema Status
Schema Validated
Calculate Tax Values
Query Tax Rates
Tax Rate Data
Tax Calculation Result
Store NFe Document
Document Stored
Submit for Authorization
Authorization Response
Update NFe Status
Status Updated
Processing Complete
NFe Response
6.3.4.3 Multi-tenant Migration Flow
The migration flow addresses the challenge that tenant-schema is good for deploying schema changes as a gradual rollout across tenants.

00:00
00:01
00:02
00:03
00:04
00:05
00:06
00:07
00:08
00:09
00:10
00:11
00:12
00:13
00:14
00:15
00:16
00:17
00:18
00:19
00:20
Load Migration Scripts
Migrate Tenants 1-10
Migrate Tenants 11-20
Migrate Tenants 21-30
Final Validation
Validate Migration
Update Migration Status
Validate Batch 1
Validate Batch 2
Validate Batch 3
Preparation
Tenant Batch 1
Tenant Batch 2
Tenant Batch 3
Completion
Multi-Tenant Schema Migration Timeline
The Integration Architecture for the NFe multi-tenant system provides a comprehensive framework for managing external integrations while maintaining tenant isolation and schema-per-tenant operations. The architecture addresses the unique challenges of Brazilian NFe compliance, dynamic schema operations with Diesel ORM, and the performance characteristics of PostgreSQL schema-based multi-tenancy. Through careful design of API patterns, message processing flows, and external system integrations, the system ensures reliable, scalable, and compliant NFe management across multiple tenants.

6.4 Security Architecture
6.4.1 Authentication Framework
6.4.1.1 Identity Management
The NFe multi-tenant system implements a comprehensive identity management framework that addresses the unique security requirements of schema-per-tenant architecture. When a user logs into the application, we would want to add to the JWT the notion of the tenant in order to make sure that after the verification of the JWT token, we are in the correct context of the tenant and we aren't exposing our application to cross-tenant security issues.

Multi-Tenant Identity Architecture

Identity Component	Implementation	Tenant Integration	Security Level
User Authentication	JWT-based with tenant context	Tenant ID embedded in token claims	High - cryptographically signed
Schema Access Control	PostgreSQL role-based permissions	Dedicated database roles per tenant	Very High - database-level isolation
Session Management	Stateless JWT with refresh tokens	Tenant-scoped session validation	High - secure token rotation
Identity Federation	OAuth 2.0 integration support	Tenant-specific OAuth configurations	High - industry standard protocols
Tenant-Aware Identity Flow

PostgreSQL
JWT Service
Tenant Management Service
Authentication Service
Client Application
PostgreSQL
JWT Service
Tenant Management Service
Authentication Service
Client Application
Token contains tenant_id, user_id, roles
Schema-level access control enforced
Login Request (email, password, tenant_id)
Validate Tenant Context
Verify Tenant Schema Exists
Schema Validation Result
Tenant Context Validated
Validate User Credentials
Generate JWT with Tenant Claims
Sign Token with Tenant-Specific Key
Signed JWT Token
Authentication Response (JWT + Refresh Token)
6.4.1.2 Multi-factor Authentication
The system implements multi-factor authentication (MFA) with tenant-specific configuration capabilities, ensuring compliance with Brazilian fiscal document security requirements.

MFA Implementation Strategy

MFA Method	Implementation	Tenant Configuration	Compliance Level
TOTP (Time-based OTP)	RFC 6238 compliant	Per-tenant MFA policies	High - industry standard
SMS Authentication	Third-party SMS gateway	Tenant-specific SMS providers	Medium - carrier dependent
Email Verification	Secure email tokens	Tenant-branded email templates	Medium - email security dependent
Hardware Tokens	FIDO2/WebAuthn support	Tenant-managed device registration	Very High - hardware-based security
6.4.1.3 Session Management
The use of refresh tokens allows us to continuously refresh the sessions and to get new JWT tokens. Using refresh tokens allows us to shorten the lifetime of the actual JWT tokens while the refreshed tokens can actually "live longer". Using this approach we can store the JSON Web Tokens in-memory while saving the refresh tokens using http-only cookies.

Secure Session Architecture

Yes

No

Yes

No

User Login

Generate JWT + Refresh Token

Store JWT in Memory

Store Refresh Token in HTTP-Only Cookie

API Request

Extract JWT from Memory

JWT Valid?

Process Request

Check Refresh Token

Refresh Token Valid?

Generate New JWT

Redirect to Login

Update JWT in Memory

6.4.1.4 Token Handling
The JWT token handling implementation addresses multi-tenant security concerns while maintaining performance and usability. Saving your JWT tokens on local storage exposes your application to XSS, which has been a OWASP Top 10 regular for many years.

Secure Token Management

Token Type	Storage Method	Lifetime	Security Measures
Access Token (JWT)	In-memory only	15 minutes	Short-lived, tenant-scoped claims
Refresh Token	HTTP-only cookie	7 days	Secure, HttpOnly, SameSite attributes
CSRF Token	Session storage	Session duration	Anti-CSRF protection
API Keys	Encrypted database	Configurable	Tenant-specific API access
6.4.1.5 Password Policies
The system implements comprehensive password policies with tenant-specific configuration capabilities to meet varying security requirements.

Password Security Framework

Policy Component	Default Configuration	Tenant Customization	Enforcement Method
Minimum Length	12 characters	8-128 characters	Client and server validation
Complexity Requirements	Mixed case, numbers, symbols	Configurable complexity rules	Real-time validation
Password History	Last 12 passwords	1-24 password history	Database-stored hashes
Expiration Policy	90 days	30-365 days or disabled	Automated expiration checks
6.4.2 Authorization System
6.4.2.1 Role-based Access Control
The authorization system leverages PostgreSQL's native role-based access control combined with application-level permissions to provide comprehensive tenant isolation. PostgreSQL allows schemas to be owned by different database roles, therefore establishing a foundation for per-tenant data access control. PostgreSQL allows schemas to be owned by different database roles, therefore establishing a foundation for per-tenant data access control.

Multi-Tenant RBAC Architecture

System Admin

Tenant Admin

Tenant User

Read-Only

User Authentication

Extract Tenant Context

Load Tenant-Specific Roles

User Role Validation

Cross-Tenant Access

Full Tenant Access

Limited Tenant Access

Query-Only Access

All Schema Access

Single Schema Full Access

Single Schema Limited Access

Single Schema Read Access

PostgreSQL Role: superuser

PostgreSQL Role: tenant_admin

PostgreSQL Role: tenant_user

PostgreSQL Role: tenant_readonly

6.4.2.2 Permission Management
The permission management system implements fine-grained access control at multiple levels: application, schema, and table levels.

Hierarchical Permission Structure

Permission Level	Scope	Implementation	Tenant Isolation
System Level	Cross-tenant operations	Application role validation	Administrative access only
Tenant Level	Single tenant scope	Schema-level PostgreSQL roles	Complete tenant isolation
Resource Level	Specific NFe operations	Table-level permissions	Resource-specific access
Field Level	Column-specific access	Column-level grants	Sensitive data protection
6.4.2.3 Resource Authorization
Resource authorization ensures that users can only access NFe documents and related data within their tenant scope while maintaining compliance with Brazilian fiscal regulations.

NFe Resource Authorization Matrix

Resource Type	Create	Read	Update	Delete	Tenant Isolation Method
NFe Documents	Tenant User+	Tenant User+	Tenant Admin+	System Admin Only	Schema-qualified queries
Tax Calculations	Tenant User+	Tenant User+	Tenant Admin+	Tenant Admin+	Schema-level permissions
Company Data	Tenant Admin+	Tenant User+	Tenant Admin+	System Admin Only	Row-level security policies
Product Catalog	Tenant User+	Tenant User+	Tenant User+	Tenant User+	Schema isolation
6.4.2.4 Policy Enforcement Points
The system implements multiple policy enforcement points to ensure comprehensive security coverage across all access paths.

Multi-Layer Policy Enforcement

No

Yes

No

Yes

Yes

No

API Request

API Gateway Enforcement

Authentication Valid?

Return 401 Unauthorized

Application Layer Enforcement

Authorization Valid?

Return 403 Forbidden

Database Layer Enforcement

Schema-Level Access Control

Row-Level Security Policies

Column-Level Permissions

Execute Query

Query Successful?

Return Data

Return 500 Internal Error

6.4.2.5 Audit Logging
Comprehensive audit logging captures all authorization decisions and access attempts across the multi-tenant system.

Audit Trail Components

Audit Event	Information Captured	Storage Method	Retention Period
Authentication Events	User, tenant, timestamp, IP, result	Immutable audit log	7 years (compliance)
Authorization Decisions	User, resource, action, decision, reason	Structured logging	5 years (fiscal audit)
Schema Access	Tenant, schema, operation, timestamp	Database audit trail	3 years (operational)
Data Modifications	User, table, operation, before/after values	Change data capture	5 years (NFe compliance)
6.4.3 Data Protection
6.4.3.1 Encryption Standards
The system implements comprehensive encryption standards to protect NFe data both at rest and in transit, ensuring compliance with Brazilian data protection regulations.

Multi-Layer Encryption Architecture

Encryption Layer	Standard	Implementation	Key Management
Transport Layer	TLS 1.3	HTTPS/WSS connections	Certificate-based PKI
Application Layer	AES-256-GCM	Sensitive field encryption	Tenant-specific keys
Database Layer	AES-256	PostgreSQL transparent encryption	Database-managed keys
Backup Encryption	AES-256-CBC	Encrypted backup storage	Backup-specific keys
6.4.3.2 Key Management
The key management system provides secure key generation, rotation, and storage for multi-tenant operations.

Tenant-Aware Key Management

Key Management Service

System Keys

Tenant Keys

User Keys

JWT Signing Keys

Database Encryption Keys

Backup Encryption Keys

Tenant Schema Keys

Tenant API Keys

Tenant Certificate Keys

User Session Keys

User MFA Keys

Key Rotation Service

Automated Rotation

Emergency Rotation

Compliance Rotation

6.4.3.3 Data Masking Rules
Data masking protects sensitive information in non-production environments while maintaining data utility for development and testing.

NFe Data Masking Strategy

Data Type	Masking Method	Preserved Characteristics	Use Case
CNPJ/CPF	Format-preserving encryption	Valid format, invalid numbers	Development testing
Personal Names	Dictionary replacement	Same length, similar patterns	UI testing
Financial Values	Proportional scaling	Relative relationships maintained	Calculation testing
Addresses	Geographic anonymization	Same region, different specifics	Location-based testing
6.4.3.4 Secure Communication
All communication channels implement comprehensive security measures to protect data in transit.

Communication Security Matrix

Communication Type	Protocol	Authentication	Encryption	Integrity
Client-Server	HTTPS/TLS 1.3	JWT Bearer tokens	AES-256-GCM	SHA-256 HMAC
Server-Database	PostgreSQL SSL	Certificate-based	TLS encryption	Connection integrity
Inter-Service	mTLS	Mutual certificates	AES-256-GCM	Message signing
External APIs	HTTPS + OAuth	OAuth 2.0 tokens	TLS encryption	Request signing
6.4.3.5 Compliance Controls
The system implements comprehensive compliance controls to meet Brazilian fiscal document regulations and international security standards.

Compliance Framework

Compliance Area	Standard/Regulation	Implementation	Audit Frequency
Data Protection	LGPD (Brazilian GDPR)	Privacy controls, consent management	Annual
Fiscal Compliance	NFe 4.00 Specification	Complete schema implementation	Continuous
Security Standards	ISO 27001	Security management system	Annual
Access Controls	SOC 2 Type II	Comprehensive audit logging	Semi-annual
6.4.4 Security Flow Diagrams
6.4.4.1 Authentication Flow
The authentication flow demonstrates the complete process from user login through tenant validation and JWT token generation.

PostgreSQL
JWT Service
Tenant Service
Auth Service
API Gateway
Client App
User
PostgreSQL
JWT Service
Tenant Service
Auth Service
API Gateway
Client App
User
alt
[Credentials Invalid]
[Credentials Valid]
alt
[Tenant Invalid]
[Tenant Valid]
Access Token: 15min, Refresh Token: 7 days
Tenant-specific schema access granted
Enter Credentials + Tenant ID
POST /auth/login
Forward Login Request
Validate Tenant Context
Check Tenant Schema Exists
Schema Status
Tenant Validation Result
400 Invalid Tenant
Error Response
Display Error
Validate User Credentials
User Validation Result
401 Unauthorized
Auth Failed
Invalid Credentials
Generate Tokens (Access + Refresh)
Sign with Tenant-Specific Key
Signed Tokens
Store Refresh Token Hash
200 + Tokens
Success + Tokens
Store Access Token (Memory)
Store Refresh Token (HTTP-Only Cookie)
Login Success
6.4.4.2 Authorization Flow
The authorization flow shows how requests are validated against tenant-specific permissions and schema access controls.

No

Yes

No

Yes

No

Yes

No

Yes

No

Yes

API Request with JWT

Extract JWT Token

Token Valid & Not Expired?

Check Refresh Token

Extract Tenant Claims

Refresh Token Valid?

Return 401 Unauthorized

Generate New Access Token

Validate Tenant Context

Tenant Schema Exists?

Return 403 Forbidden - Invalid Tenant

Extract User Roles

Check Resource Permissions

Permission Granted?

Return 403 Forbidden - Insufficient Permissions

Set Database Connection Context

Execute Schema-Qualified Query

Query Successful?

Return 500 Internal Error

Return Success Response

Audit Logger

Log All Authorization Decisions

6.4.4.3 Security Zone Diagram
The security zone diagram illustrates the different security boundaries and trust levels within the multi-tenant NFe system.

Management Zone

Data Zone

Application Zone

DMZ Zone

Internet Zone

HTTPS/TLS 1.3

HTTPS/TLS 1.3

Encrypted Connection

Encrypted Connection

Encrypted Connection

Encrypted Connection

Streaming Replication

Encrypted Backup

Client Applications

External APIs - SEFAZ

Load Balancer

API Gateway

Web Application Firewall

Authentication Service

Authorization Service

NFe Processing Service

Tenant Management Service

PostgreSQL Primary

PostgreSQL Replica

Backup Storage

Monitoring Services

Log Aggregation

Key Management Service

6.4.5 Security Control Matrices
6.4.5.1 Access Control Matrix
User Role	NFe Create	NFe Read	NFe Update	NFe Delete	Schema Admin	Cross-Tenant
System Administrator	✅	✅	✅	✅	✅	✅
Tenant Administrator	✅	✅	✅	❌	✅	❌
Tenant User	✅	✅	⚠️	❌	❌	❌
Read-Only User	❌	✅	❌	❌	❌	❌
Legend: ✅ Full Access, ⚠️ Limited Access, ❌ No Access

6.4.5.2 Data Classification Matrix
Data Type	Classification	Encryption Required	Access Logging	Retention Period
NFe Documents	Confidential	✅	✅	5 years
Tax Calculations	Confidential	✅	✅	5 years
User Credentials	Restricted	✅	✅	Account lifetime
Audit Logs	Internal	⚠️	✅	7 years
6.4.5.3 Threat Mitigation Matrix
Threat Category	Risk Level	Mitigation Strategy	Implementation Status
Cross-Tenant Data Access	High	Schema-level isolation	✅ Implemented
JWT Token Compromise	Medium	Short-lived tokens + rotation	✅ Implemented
SQL Injection	High	Parameterized queries + ORM	✅ Implemented
XSS Attacks	Medium	Content Security Policy	✅ Implemented
The Security Architecture for the NFe multi-tenant system provides comprehensive protection through multiple layers of security controls, from authentication and authorization to data protection and compliance. The schema-per-tenant approach enables strong tenant isolation while maintaining operational efficiency, ensuring that Brazilian fiscal document requirements are met with the highest security standards.

6.5 Monitoring And Observability
6.5.1 Monitoring Infrastructure
6.5.1.1 Metrics Collection Architecture
The NFe multi-tenant system implements a comprehensive metrics collection strategy that addresses the unique challenges of schema-per-tenant architecture. A generic diesel connection implementations that allows to log any executed query provides the foundation for database-level monitoring, while metrics-rs emerge as a powerful solution for instrumenting and collecting metrics within applications. Developed with simplicity, performance, and flexibility in mind, metrics-rs provides developers with a comprehensive toolkit for effortlessly integrating metrics into their Rust projects.

Multi-Tenant Metrics Architecture

Metric Category	Collection Method	Tenant Isolation	Storage Backend
Application Metrics	metrics-rs with Prometheus exporter	Tenant-labeled metrics	Prometheus TSDB
Database Metrics	Diesel query logging + PostgreSQL stats	Schema-qualified monitoring	Prometheus + PostgreSQL
Schema Operations	Custom instrumentation	Per-tenant schema tracking	Time-series database
Connection Pool Metrics	r2d2 pool monitoring	Pool-per-tenant metrics	In-memory + Prometheus
Tenant-Aware Metrics Collection Flow

NFe Application Request

Extract Tenant Context

Metrics Middleware

Application Metrics

Database Metrics

Schema Metrics

metrics-rs Registry

Diesel Query Logger

Schema Operation Tracker

Prometheus Exporter

PostgreSQL Stats Collector

Custom Metrics Endpoint

Prometheus Server

Grafana Dashboard

6.5.1.2 Log Aggregation Strategy
The log aggregation strategy addresses the complexity of multi-tenant operations where while schema isolation provides logical separation, all tenants still share the same database resources. Implement: Resource governance using PostgreSQL resource queues, Monitoring at the schema level to identify problematic tenants, Connection pooling optimized for multi-tenant workloads.

Structured Logging Framework

Log Level	Content	Tenant Context	Retention Period
ERROR	System failures, schema errors	tenant_id, schema_name	90 days
WARN	Performance degradation, pool exhaustion	tenant_id, operation_type	60 days
INFO	Schema operations, migrations	tenant_id, migration_version	30 days
DEBUG	Query execution, connection details	tenant_id, query_hash	7 days
Log Aggregation Architecture

Grafana
Log Storage
Log Collector
Structured Logger
NFe Application
Grafana
Log Storage
Log Collector
Structured Logger
NFe Application
JSON structured logging
Tenant-aware log routing
Schema-isolated log storage
Log Event with Tenant Context
Add Metadata (timestamp, level, tenant_id)
Forward Structured Log
Store with Tenant Labels
Query Tenant-Specific Logs
Display Filtered Logs
6.5.1.3 Distributed Tracing Implementation
The distributed tracing system leverages the tracing crate provides a versatile interface for collecting structured telemetry—including metrics, traces, and logs. Its design allows developers to plug in their implementation of choice to deliver this data as needed to a preferred backend system. Using this approach, you can export tracing telemetry data in OTLP format and select Datadog as the backend.

Multi-Tenant Tracing Strategy

Trace Component	Implementation	Tenant Correlation	Performance Impact
Request Tracing	OpenTelemetry spans	tenant_id in span attributes	< 2ms overhead
Database Tracing	Diesel query spans	schema-qualified traces	< 1ms per query
Schema Operations	Custom span creation	schema_name + operation_type	< 5ms per operation
Cross-Service Tracing	OTLP propagation	tenant context propagation	< 1ms propagation
6.5.1.4 Alert Management System
The alert management system addresses the unique requirements of schema-per-tenant monitoring where tenant-schema is best when you have a relatively small number of fairly large tenants. An example of this would be an accounting application, with only paid subscription users. Things which make it the better performing option for you include: problems with VACUUM and other PostgreSQL administrative operations which scale poorly across 1000's of tables.

Tenant-Aware Alert Configuration

Alert Type	Threshold	Scope	Escalation
Schema Creation Failure	1 failure	Per tenant	Immediate
Connection Pool Exhaustion	> 90% utilization	Per tenant pool	5 minutes
Query Performance Degradation	> 1s 95th percentile	Per tenant schema	10 minutes
Migration Failure	1 failure	Cross-tenant impact	Immediate
Alert Routing Architecture

No

Yes

Critical

Warning

Info

Monitoring System

Alert Evaluator

Alert Threshold Exceeded?

Continue Monitoring

Classify Alert Severity

Severity Level

Immediate Notification

Batched Notification

Dashboard Update Only

PagerDuty/Slack

Email Digest

Grafana Dashboard

Tenant Context Enrichment

Add tenant_id, schema_name

Route to Tenant-Specific Channels

6.5.1.5 Dashboard Design Strategy
The dashboard design addresses the visualization needs of schema-per-tenant architecture, leveraging Grafana complements Prometheus by providing rich visualization and dashboarding capabilities. Developers can create customizable dashboards to visualize metric data in real-time, enabling deep insights into application performance and behaviour. By integrating Prometheus with metrics-rs and visualizing the collected data using Grafana, Rust developers can establish a comprehensive monitoring solution tailored to their specific requirements.

Multi-Tenant Dashboard Architecture

Dashboard Type	Scope	Metrics Displayed	Update Frequency
System Overview	Cross-tenant	Total tenants, system health	30 seconds
Tenant-Specific	Single tenant	Schema metrics, NFe operations	15 seconds
Schema Operations	Administrative	Migration status, schema health	1 minute
Performance Analysis	Cross-tenant comparison	Query performance, resource usage	1 minute
6.5.2 Observability Patterns
6.5.2.1 Health Check Implementation
The health check system implements comprehensive monitoring for schema-per-tenant operations, addressing the challenge that there's some specific issues with large numbers of tables, namely administrative tasks like backup and VACUUM. However, by "large numbers of tables" we're talking 10s or 100s of thousands, not a few hundred. And those issues don't affect SELECT queries.

Multi-Level Health Check Architecture

Health Check Orchestrator

System Health

Tenant Health

Schema Health

Connection Health

Application Status

Database Connectivity

Resource Utilization

Tenant Schema Validation

Tenant Connection Pool

Tenant Data Integrity

Schema Structure Validation

Migration Status Check

Schema Performance Metrics

Pool Utilization

Connection Latency

Connection Failures

Health Check Endpoints

Endpoint	Check Type	Response Time Target	Failure Threshold
`/health`	Basic application health	< 100ms	3 consecutive failures
`/health/tenant/{id}`	Tenant-specific health	< 500ms	5 consecutive failures
`/health/schema/{name}`	Schema validation	< 1s	2 consecutive failures
`/health/deep`	Comprehensive system check	< 5s	1 failure
6.5.2.2 Performance Metrics Framework
The performance metrics framework addresses the unique characteristics of schema-per-tenant architecture where counters help measure an event's total number of occurrences, such as the number of requests processed. Histograms help understand the distribution of values, like response times, allowing you to analyze performance across different percentiles.

Tenant Performance Metrics

Metric Type	Metric Name	Labels	Purpose
Counter	`nfe_requests_total`	tenant_id, operation	Track NFe operations per tenant
Histogram	`schema_operation_duration`	tenant_id, operation_type	Measure schema operation latency
Gauge	`tenant_connection_pool_size`	tenant_id, pool_type	Monitor connection pool utilization
Counter	`migration_operations_total`	tenant_id, status	Track migration success/failure rates
6.5.2.3 Business Metrics Tracking
Business metrics tracking focuses on NFe-specific operations and compliance requirements, providing insights into fiscal document processing performance across tenants.

NFe Business Metrics

Business Metric	Implementation	Tenant Scope	Compliance Relevance
NFe Creation Rate	Counter with tenant labels	Per tenant	Processing capacity monitoring
Authorization Success Rate	Ratio of authorized/total NFe	Per tenant	SEFAZ integration health
Tax Calculation Performance	Histogram of calculation time	Per tenant	Business rule performance
Schema Migration Success	Counter with migration version	Cross-tenant	System reliability
Business Metrics Collection

Compliance Tracker
Business Logic
Metrics Collector
NFe Service
Compliance Tracker
Business Logic
Metrics Collector
NFe Service
All metrics include tenant_id
Fiscal compliance tracking
Record NFe Creation
Update Business Counters
Track Compliance Metrics
Record Authorization Result
Update Success Rate
Record Tax Calculation Time
Update Performance Histogram
6.5.2.4 Sla Monitoring Framework
The SLA monitoring framework ensures that tenant-specific service level agreements are met while maintaining system-wide performance standards.

Multi-Tenant SLA Matrix

SLA Metric	Target	Measurement Window	Tenant Tier Impact
NFe Creation Time	< 5 seconds	5-minute rolling window	Premium: < 3s, Standard: < 5s
Schema Operation Time	< 30 seconds	Per operation	Premium: < 20s, Standard: < 30s
System Availability	99.9% uptime	Monthly	Premium: 99.95%, Standard: 99.9%
Query Response Time	< 200ms 95th percentile	1-minute window	Premium: < 100ms, Standard: < 200ms
6.5.2.5 Capacity Tracking System
The capacity tracking system monitors resource utilization across the schema-per-tenant architecture, providing insights for scaling decisions.

Capacity Monitoring Architecture

Capacity Monitor

Tenant Capacity

Schema Capacity

System Capacity

Tenant Data Volume

Tenant Query Load

Tenant Connection Usage

Schema Object Count

Schema Size Metrics

Schema Performance

CPU Utilization

Memory Usage

Storage Capacity

Capacity Alerts

Scale-Up Triggers

Scale-Down Opportunities

Resource Optimization

6.5.3 Incident Response
6.5.3.1 Alert Routing Strategy
The alert routing strategy implements tenant-aware incident management, ensuring that alerts are properly classified and routed based on tenant impact and severity.

Tenant-Aware Alert Routing

Alert Category	Routing Rule	Notification Channel	Response Time SLA
Tenant-Specific Issues	Route to tenant support team	Tenant-specific Slack channel	15 minutes
Cross-Tenant Impact	Route to platform team	Platform alerts channel	5 minutes
Schema Migration Failures	Route to database team	DBA on-call rotation	10 minutes
System-Wide Outages	Route to incident commander	All hands alert	2 minutes
6.5.3.2 Escalation Procedures
The escalation framework ensures appropriate response to incidents based on tenant impact and business criticality.

Multi-Tenant Escalation Matrix

Single Tenant

Multiple Tenants

System-Wide

Premium

Standard

Alert Triggered

Tenant Impact Assessment

Tenant-Level Response

Platform-Level Response

Critical Incident Response

Tenant Tier

Immediate Escalation

Standard Response

Platform Team Notification

Cross-Tenant Impact Analysis

Incident Commander Activation

All Hands Response

Premium Support Team

Standard Support Queue

Platform Engineering Team

Executive Notification

6.5.3.3 Runbook Automation
Automated runbooks address common multi-tenant scenarios, reducing response time and ensuring consistent incident handling.

Automated Response Scenarios

Scenario	Trigger Condition	Automated Action	Manual Escalation
Schema Creation Failure	Schema creation timeout	Retry with exponential backoff	After 3 failed attempts
Connection Pool Exhaustion	Pool utilization > 95%	Increase pool size by 25%	If scaling fails
Migration Rollback	Migration failure detected	Automatic rollback to previous version	Cross-tenant impact
Tenant Isolation Breach	Cross-tenant data access	Immediate tenant suspension	Security team notification
6.5.3.4 Post-mortem Process
The post-mortem process ensures continuous improvement in multi-tenant operations and incident response.

Post-Incident Analysis Framework

Analysis Phase	Focus Area	Deliverable	Timeline
Immediate Response	Incident timeline and impact	Incident summary report	24 hours
Root Cause Analysis	Technical and process failures	Root cause analysis document	1 week
Improvement Planning	Prevention and detection improvements	Action item backlog	2 weeks
Implementation Tracking	Progress on improvement items	Monthly progress reports	Ongoing
6.5.3.5 Improvement Tracking
Continuous improvement tracking ensures that lessons learned from incidents are implemented across the multi-tenant system.

Improvement Metrics

Improvement Area	Metric	Target	Measurement Method
Mean Time to Detection (MTTD)	Time from incident to alert	< 2 minutes	Alert timestamp analysis
Mean Time to Resolution (MTTR)	Time from alert to resolution	< 30 minutes	Incident tracking system
Incident Recurrence Rate	Repeat incidents within 30 days	< 5%	Post-mortem analysis
Tenant Impact Reduction	Affected tenants per incident	< 10% of total tenants	Impact assessment reports
6.5.4 Monitoring Architecture Diagrams
6.5.4.1 Comprehensive Monitoring Architecture
Alerting

Storage & Analysis

Tracing Pipeline

Logging Pipeline

Metrics Collection

NFe Multi-Tenant Application

NFe Service

Metrics Middleware

Logging Middleware

Tracing Middleware

Schema Service

Schema Metrics

Connection Pool

Pool Metrics

Migration Engine

Migration Metrics

metrics-rs Registry

Prometheus Exporter

Structured Logger

Log Collector

Log Storage

OpenTelemetry Collector

Trace Storage

Prometheus Server

Loki/Elasticsearch

Jaeger/Tempo

Grafana

AlertManager

PagerDuty/Slack

6.5.4.2 Alert Flow Architecture
On-Call Engineer
Notification System
AlertManager
Prometheus
Metrics System
NFe Application
On-Call Engineer
Notification System
AlertManager
Prometheus
Metrics System
NFe Application
alt
[Critical Alert]
[Warning Alert]
alt
[Alert Threshold Exceeded]
Tenant-specific alert rules
Tenant-aware routing
Emit Tenant Metrics
Store Time Series Data
Evaluate Alert Rules
Fire Alert
Apply Routing Rules
Send Notification
Immediate Page
Email/Slack
Investigate Issue
Acknowledge Alert
6.5.4.3 Multi-tenant Dashboard Layout
Grafana Dashboard

System Overview Panel

Tenant Summary Panel

Schema Operations Panel

Performance Metrics Panel

Total Tenants

System Health Status

Resource Utilization

Tenant List with Status

Tenant-Specific Metrics

Tenant Performance Comparison

Schema Creation Status

Migration Progress

Schema Health Checks

Query Performance Histograms

Connection Pool Utilization

NFe Processing Rates

6.5.5 Monitoring Metrics Definitions
6.5.5.1 Application Metrics
Metric Name	Type	Labels	Description
`nfe_requests_total`	Counter	tenant_id, operation, status	Total NFe requests processed
`schema_operations_duration_seconds`	Histogram	tenant_id, operation	Time spent on schema operations
`connection_pool_active_connections`	Gauge	tenant_id, pool_name	Active connections per tenant pool
`migration_status`	Gauge	tenant_id, version, status	Migration status per tenant
6.5.5.2 Business Metrics
Metric Name	Type	Labels	Description
`nfe_authorization_success_rate`	Gauge	tenant_id	Percentage of successful NFe authorizations
`tax_calculation_duration_seconds`	Histogram	tenant_id, tax_type	Time spent calculating taxes
`compliance_violations_total`	Counter	tenant_id, violation_type	Total compliance violations detected
`tenant_data_volume_bytes`	Gauge	tenant_id, schema_name	Data volume per tenant schema
6.5.5.3 Alert Threshold Matrix
Alert Rule	Threshold	Duration	Severity	Action
High Error Rate	> 5% errors in 5min	5 minutes	Critical	Immediate page
Schema Creation Failure	Any failure	Immediate	High	Team notification
Connection Pool Exhaustion	> 90% utilization	2 minutes	Warning	Auto-scale trigger
Query Performance Degradation	> 1s 95th percentile	10 minutes	Warning	Performance investigation
6.5.5.4 Sla Requirements
Service Level Objective	Target	Measurement	Consequences
NFe Processing Availability	99.9%	Monthly uptime	Service credits
Schema Operation Success Rate	99.5%	Per operation	Engineering escalation
Query Response Time	< 200ms 95th percentile	5-minute windows	Performance optimization
Tenant Isolation Integrity	100%	Continuous monitoring	Security incident
The Monitoring and Observability architecture for the NFe multi-tenant system provides comprehensive visibility into schema-per-tenant operations while maintaining tenant isolation and performance. The system leverages modern Rust observability tools including metrics-rs, OpenTelemetry, and Prometheus to deliver real-time insights into system health, business metrics, and compliance requirements. Through tenant-aware monitoring, automated alerting, and comprehensive dashboards, the system ensures reliable operation and rapid incident response across hundreds or thousands of tenant schemas.

6.6 Testing Strategy
6.6.1 Testing Approach
6.6.1.1 Unit Testing
The NFe multi-tenant system implements comprehensive unit testing strategies that address the unique challenges of schema-per-tenant architecture with Rust and Diesel ORM. One effective method is unit testing. It focuses on individual components, ensuring each part behaves as expected.

Testing Framework and Tools

Component	Framework/Tool	Version	Purpose
Test Framework	Built-in Rust testing	Latest stable	Core unit test execution
Database Testing	A generic diesel connection implementations that allows to log any executed query	Latest	Query validation and logging
Mock Framework	Custom mock implementations	N/A	Database connection mocking
Coverage Tool	cargo-tarpaulin	Latest	Code coverage measurement
Test Organization Structure

The unit testing structure follows Rust conventions while accommodating multi-tenant schema operations where Unlike Diesel, SeaORM doesn't provide compile-time type checking - you'll need to rely on runtime integration tests to catch database-related bugs.

// Example unit test structure for tenant schema operations
#[cfg(test)]
mod tests {
    use super::*;
    use diesel::test_transaction;
    
    #[test]
    fn test_tenant_schema_creation() {
        // Test schema creation logic
        let tenant_id = Uuid::new_v4();
        let schema_name = format!("tenant_{}", tenant_id);
        
        let result = create_tenant_schema(&schema_name);
        assert!(result.is_ok());
    }
    
    #[test]
    fn test_dynamic_table_creation() {
        // Test dynamic table creation with explicit select clauses
        let table_def = create_nfe_table_definition("test_schema");
        assert!(table_def.is_valid());
    }
}
Mocking Strategy

Define a mock structure with capacity to record executed queries and return predefined results. The mocking approach addresses the challenge that many compile time guarantees are lost when using dynamic schemas.

Mock Type	Implementation	Use Case	Validation Method
Connection Mock	Custom Diesel connection trait	Database connection testing	Query string comparison
Schema Mock	Mock schema validation	Schema creation testing	Structure verification
Pool Mock	Connection pool simulation	Multi-tenant connection testing	Resource utilization tracking
Migration Mock	Mock migration execution	Schema evolution testing	Version tracking validation
Code Coverage Requirements

Component Category	Coverage Target	Measurement Method	Critical Paths
Schema Operations	95%	Line coverage	Schema creation, validation
Connection Management	90%	Branch coverage	Pool management, tenant routing
NFe Business Logic	95%	Line coverage	Tax calculations, validation
Error Handling	85%	Exception coverage	Rollback procedures, recovery
Test Naming Conventions

// Naming convention: test_{component}_{action}_{expected_outcome}
#[test]
fn test_tenant_schema_create_success() { }

#[test]
fn test_tenant_schema_create_duplicate_error() { }

#[test]
fn test_nfe_validation_missing_cnpj_error() { }

#[test]
fn test_connection_pool_exhaustion_handling() { }
Test Data Management

The test data strategy addresses the complexity of multi-tenant schema testing where each tenant requires isolated test data.

Data Type	Management Strategy	Isolation Method	Cleanup Approach
Tenant Schemas	Per-test schema creation	Unique schema names	Automatic schema drop
NFe Test Data	Template-based generation	Schema-qualified inserts	Transaction rollback
Connection Configs	Mock configurations	In-memory storage	Test completion cleanup
Migration Scripts	Embedded test migrations	Version-controlled scripts	Rollback on failure
6.6.1.2 Integration Testing
Integration testing addresses the unique challenges of schema-per-tenant architecture where By integrating these testing practices, you ensure that your database-related functionality in Diesel is reliable and behaves as expected under various conditions, thereby enhancing the overall stability and reliability of your application.

Service Integration Test Approach

The integration testing strategy focuses on multi-tenant service interactions and schema-per-tenant operations.

Integration Test Suite

Tenant Registration Tests

Schema Creation Tests

NFe Processing Tests

Migration Tests

Connection Validation

Schema Provisioning

Dynamic Schema Creation

Schema Structure Validation

Multi-Tenant NFe Operations

Cross-Schema Isolation

Gradual Rollout Testing

Rollback Procedures

API Testing Strategy

Test Category	Scope	Validation Points	Expected Outcomes
Tenant Onboarding	End-to-end tenant registration	Connection validation, schema creation	Complete tenant setup
NFe Operations	Multi-tenant NFe processing	Schema isolation, data integrity	Successful NFe creation
Schema Evolution	Migration across tenants	Version consistency, rollback capability	Consistent schema updates
Error Scenarios	Failure handling and recovery	Error isolation, system stability	Graceful error handling
Database Integration Testing

You can install with this snippet: cargo add testcontainers cargo add testcontainers-modules. To set up integration tests, create a new folder in your project root called tests. You can then create a .rs file named anything you like. We'll call our example file integration_tests.rs.

The database integration testing uses testcontainers for PostgreSQL instance management:

// Integration test setup with testcontainers
use testcontainers::{clients, images, Container};
use testcontainers_modules::postgres::Postgres;

#[tokio::test]
async fn test_multi_tenant_schema_creation() {
    let docker = clients::Cli::default();
    let postgres_container = docker.run(Postgres::default());
    
    let connection_string = format!(
        "postgres://postgres:postgres@localhost:{}/postgres",
        postgres_container.get_host_port_ipv4(5432)
    );
    
    // Test tenant schema creation
    let tenant_service = TenantService::new(&connection_string).await;
    let result = tenant_service.create_tenant_schema("test_tenant").await;
    
    assert!(result.is_ok());
}
External Service Mocking

External Service	Mock Strategy	Test Scenarios	Validation Approach
SEFAZ Integration	HTTP mock server	Authorization responses	Response validation
Tax Rate Services	Static mock responses	Rate calculation testing	Calculation accuracy
Authentication	JWT mock validation	Token verification	Security validation
Monitoring Services	Mock metrics collection	Performance tracking	Metrics verification
Test Environment Management

The test environment strategy addresses the complexity of multi-tenant testing where tenant-schema is best when you have a relatively small number of fairly large tenants. An example of this would be an accounting application, with only paid subscription users.

Environment Type	Configuration	Tenant Count	Data Volume
Unit Test Environment	In-memory/Mock	1-5 tenants	Minimal test data
Integration Test Environment	Testcontainers PostgreSQL	10-50 tenants	Representative data
Performance Test Environment	Dedicated PostgreSQL	100-500 tenants	Production-like volume
End-to-End Test Environment	Full stack deployment	5-20 tenants	Complete workflows
6.6.1.3 End-to-end Testing
End-to-end testing validates complete multi-tenant workflows from tenant registration through NFe processing and schema evolution.

E2E Test Scenarios

Scenario	Workflow Steps	Validation Points	Success Criteria
Complete Tenant Lifecycle	Registration → Schema Creation → NFe Processing → Migration	Data isolation, schema consistency	Full workflow completion
Multi-Tenant Isolation	Parallel tenant operations	Cross-tenant data access prevention	Complete isolation maintained
Schema Migration Rollout	Gradual migration across tenants	Version consistency, rollback capability	Successful migration deployment
Error Recovery	Failure injection and recovery	System stability, data integrity	Graceful error handling
UI Automation Approach

While the NFe system is primarily API-based, UI automation focuses on administrative interfaces and monitoring dashboards.

UI Component	Automation Tool	Test Coverage	Validation Method
Admin Dashboard	Selenium WebDriver	Tenant management operations	UI interaction validation
Monitoring Interface	API-based testing	Metrics display accuracy	Data consistency checks
Schema Management UI	Integration testing	Schema operation controls	Functional validation
Error Reporting UI	Mock error injection	Error display accuracy	Error handling validation
Test Data Setup/Teardown

The test data management strategy addresses the unique requirements of schema-per-tenant testing:

Test Cleanup
PostgreSQL
Test Setup
Test Suite
Test Cleanup
PostgreSQL
Test Setup
Test Suite
Initialize Test Environment
Create Test Database
Create Tenant Schemas
Insert Test Data
Execute Test Scenarios
Teardown Environment
Drop Tenant Schemas
Clean Test Data
Reset Database State
Performance Testing Requirements

Performance Metric	Target	Test Method	Acceptance Criteria
Tenant Schema Creation	< 30 seconds	Load testing	95% within target
NFe Processing Throughput	1000 NFe/minute	Stress testing	Sustained throughput
Multi-Tenant Query Performance	< 200ms 95th percentile	Performance profiling	Consistent response times
Migration Deployment Time	< 60 seconds per tenant	Migration testing	Reliable deployment timing
Cross-Browser Testing Strategy

For web-based administrative interfaces:

Browser	Version Support	Test Coverage	Automation Level
Chrome	Latest 2 versions	Full functionality	Automated
Firefox	Latest 2 versions	Core functionality	Automated
Safari	Latest version	Basic functionality	Manual
Edge	Latest version	Core functionality	Automated
6.6.2 Test Automation
6.6.2.1 Ci/cd Integration
The continuous integration strategy addresses the unique challenges of multi-tenant schema testing where Rewriting our stack to use DST from the ground up is the equivalent of unit testing: it would allow us to find bugs and iterate a lot faster and cheaper in the long run. External simulation is like integration testing: slower and more expensive.

CI/CD Pipeline Architecture

Code Commit

Pre-commit Hooks

Unit Tests

Integration Tests

Schema Migration Tests

Performance Tests

Security Tests

Deployment

Test Database Setup

Test Environment Cleanup

Test Results

Automated Test Triggers

Trigger Event	Test Suite	Execution Time	Failure Action
Pull Request	Unit + Integration	10-15 minutes	Block merge
Main Branch Push	Full test suite	30-45 minutes	Rollback deployment
Nightly Build	Performance + E2E	2-3 hours	Alert team
Release Candidate	Complete validation	4-6 hours	Block release
Parallel Test Execution

The parallel execution strategy optimizes test performance while maintaining tenant isolation:

Test Category	Parallelization Strategy	Resource Requirements	Isolation Method
Unit Tests	Per-module parallelization	CPU-bound	In-memory mocking
Integration Tests	Per-tenant parallelization	Database connections	Separate test schemas
Schema Tests	Sequential per tenant	Database locks	Schema-level locking
Performance Tests	Isolated execution	Dedicated resources	Separate test environment
Test Reporting Requirements

cargo-nextest is a test runner for Rust that improves a lot of the core testing functionality. For regular usage, you can get started by using cargo nextest run to run all of the tests in a given workspace. You can also use cargo nextest list to list all of the tests that you need to run!

Report Type	Format	Audience	Update Frequency
Test Results	JUnit XML	CI/CD system	Per build
Coverage Report	HTML + JSON	Development team	Per commit
Performance Metrics	Prometheus metrics	Operations team	Continuous
Security Scan Results	SARIF format	Security team	Per release
Failed Test Handling

The failure handling strategy addresses multi-tenant test complexity:

Unit Test

Integration Test

Schema Test

Performance Test

Yes

No

Test Failure Detected

Failure Type

Immediate Notification

Retry with Clean Environment

Isolate Affected Tenant

Resource Analysis

Block CI Pipeline

Retry Successful?

Continue Pipeline

Tenant-Specific Investigation

Performance Regression Analysis

Developer Notification

Flaky Test Management

Detection Method	Threshold	Action	Resolution Process
Success Rate Monitoring	< 95% success rate	Mark as flaky	Root cause analysis
Execution Time Variance	> 50% time variance	Performance investigation	Optimization or quarantine
Environment Dependency	Failure in specific environments	Environment analysis	Configuration standardization
Tenant-Specific Failures	Failures in specific tenant contexts	Tenant isolation analysis	Schema-specific debugging
6.6.3 Quality Metrics
6.6.3.1 Code Coverage Targets
The code coverage strategy addresses the unique challenges of multi-tenant schema operations where studies show that well-tested code reduces the likelihood of errors by up to 90%.

Coverage Requirements by Component

Component Category	Line Coverage	Branch Coverage	Function Coverage	Critical Path Coverage
Schema Management	95%	90%	100%	100%
NFe Business Logic	95%	92%	100%	100%
Connection Pooling	90%	85%	95%	95%
Migration Engine	95%	90%	100%	100%
Error Handling	85%	80%	90%	95%
Utility Functions	80%	75%	85%	90%
Coverage Measurement Tools

Tool	Purpose	Integration	Reporting Format
cargo-tarpaulin	Rust code coverage	CI/CD pipeline	HTML, JSON, XML
cargo-llvm-cov	LLVM-based coverage	Local development	Terminal, HTML
codecov.io	Coverage tracking	GitHub integration	Web dashboard
SonarQube	Quality analysis	Enterprise integration	Comprehensive reports
Test Success Rate Requirements

The success rate targets account for the complexity of multi-tenant operations:

Test Category	Success Rate Target	Measurement Window	Escalation Threshold
Unit Tests	99.5%	Per commit	< 98%
Integration Tests	98%	Daily average	< 95%
Schema Migration Tests	99%	Per migration	< 97%
Performance Tests	95%	Weekly average	< 90%
End-to-End Tests	90%	Per release	< 85%
Performance Test Thresholds

Performance Metric	Target	Warning Threshold	Critical Threshold
Schema Creation Time	< 30 seconds	> 25 seconds	> 45 seconds
NFe Processing Latency	< 200ms 95th percentile	> 150ms	> 500ms
Connection Pool Acquisition	< 100ms	> 80ms	> 200ms
Migration Deployment	< 60 seconds per tenant	> 45 seconds	> 120 seconds
Memory Usage	< 512MB per tenant	> 400MB	> 1GB
Quality Gates

The quality gates ensure multi-tenant system reliability:

No

Yes

No

Yes

No

Yes

No

Yes

No

Yes

No

Yes

Code Commit

Unit Test Coverage > 95%?

Block Commit

Integration Tests Pass?

Performance Within Limits?

Security Scan Clean?

Allow Merge

Release Candidate

All Quality Gates Pass?

Block Release

Schema Migration Tests Pass?

Approve Release

Documentation Requirements

Documentation Type	Coverage Requirement	Update Frequency	Quality Standard
API Documentation	100% public APIs	Per API change	Examples included
Test Documentation	All test scenarios	Per test addition	Clear test purpose
Schema Documentation	All NFe tables	Per schema change	Complete field descriptions
Deployment Documentation	All procedures	Per process change	Step-by-step instructions
6.6.4 Test Execution Flow
6.6.4.1 Test Execution Architecture
The test execution flow addresses the complexity of multi-tenant schema testing where tenant isolation must be maintained throughout the testing process.

Yes

No

Test Execution Start

Environment Setup

Database Initialization

Tenant Schema Creation

Test Data Population

Unit Test Execution

Integration Test Execution

Schema Migration Tests

Performance Tests

End-to-End Tests

Test Results Aggregation

Coverage Analysis

Quality Gate Validation

All Gates Pass?

Environment Cleanup

Failure Analysis

Test Completion

Error Reporting

Test Failure

6.6.4.2 Test Environment Architecture
The test environment architecture supports isolated multi-tenant testing scenarios:

Test Execution

Tenant Test Schemas

Test Environment

Test Controller

PostgreSQL Test Instance

Schema Manager

Test Data Generator

Tenant Schema 1

Tenant Schema 2

Tenant Schema N

Unit Tests

Integration Tests

Migration Tests

Performance Tests

6.6.4.3 Test Data Flow Diagram
The test data flow ensures proper tenant isolation and data management throughout the testing process:

Test Suite
PostgreSQL
Schema Manager
Data Generator
Test Controller
Test Suite
PostgreSQL
Schema Manager
Data Generator
Test Controller
Complete tenant isolation maintained
Schema-qualified queries required
Initialize Test Environment
Create Test Database
Create Tenant Schemas
Generate Test Data
Insert Tenant-Specific Data
Execute Unit Tests
Query Tenant Schema 1
Return Test Results
Execute Integration Tests
Multi-Tenant Operations
Validate Isolation
Execute Migration Tests
Apply Schema Changes
Update Tenant Schemas
Confirm Updates
Cleanup Environment
Drop Test Schemas
Clean Test Data
6.6.5 Testing Tools And Frameworks
6.6.5.1 Core Testing Framework Stack
Tool Category	Tool Name	Version	Purpose	Integration Method
Test Framework	Rust built-in testing	Latest stable	Core test execution	Native Rust support
Test Runner	cargo-nextest	Latest	Enhanced test execution	`cargo nextest run`
Database Testing	testcontainers-rs	Latest	PostgreSQL test instances	Docker integration
Mocking Framework	mockall	Latest	Service mocking	Trait-based mocking
Coverage Analysis	cargo-tarpaulin	Latest	Code coverage measurement	CI/CD integration
6.6.5.2 Multi-tenant Testing Patterns
Schema-Per-Tenant Test Pattern

// Example test pattern for schema-per-tenant operations
#[cfg(test)]
mod multi_tenant_tests {
    use super::*;
    use testcontainers::{clients, images::postgres::Postgres};
    
    #[tokio::test]
    async fn test_tenant_isolation() {
        let docker = clients::Cli::default();
        let postgres = docker.run(Postgres::default());
        
        // Create multiple tenant schemas
        let tenant1_schema = create_tenant_schema("tenant_1").await?;
        let tenant2_schema = create_tenant_schema("tenant_2").await?;
        
        // Verify complete isolation
        assert_tenant_isolation(&tenant1_schema, &tenant2_schema).await;
    }
}
Dynamic Schema Testing Pattern

The testing approach addresses the challenge that many compile time guarantees are lost when using dynamic schemas, requiring comprehensive runtime validation.

Test Pattern	Implementation	Validation Method	Coverage Area
Schema Creation	Dynamic schema generation	Structure verification	Schema completeness
Query Validation	Runtime query testing	Result verification	Query correctness
Migration Testing	Version-controlled changes	Consistency checking	Schema evolution
Isolation Testing	Cross-tenant access attempts	Access denial verification	Security boundaries
6.6.5.3 Test Environment Configuration
Development Environment Setup

# Cargo.toml test dependencies
[dev-dependencies]
tokio-test = "0.4"
testcontainers = "0.15"
testcontainers-modules = { version = "0.3", features = ["postgres"] }
mockall = "0.12"
cargo-nextest = "0.9"
assert_cmd = "2.0"
predicates = "3.0"
CI/CD Environment Configuration

Environment Variable	Purpose	Example Value	Required
`DATABASE_URL`	Test database connection	`postgres://test:test@localhost/test`	Yes
`RUST_TEST_THREADS`	Parallel test execution	`4`	No
`CARGO_TERM_COLOR`	Test output coloring	`always`	No
`RUST_BACKTRACE`	Error debugging	`1`	No
The Testing Strategy for the NFe multi-tenant system provides a comprehensive framework for ensuring system reliability, security, and performance across schema-per-tenant operations. The strategy addresses the unique challenges of testing dynamic schema operations with Diesel ORM, multi-tenant isolation requirements, and the complexity of Brazilian NFe compliance validation. Through carefully designed unit tests, integration tests, and end-to-end testing scenarios, the system ensures robust operation across hundreds or thousands of tenant schemas while maintaining data isolation and regulatory compliance.

6.1 Service Components
6.2 Scalability Design
6.3 Resilience Patterns
6.2 Database Design
6.3 Integration Architecture
6.4 Security Architecture
6.5 Monitoring And Observability
6.6 Testing Strategy
7. User Interface Design
No user interface required.

The NFe multi-tenant management system is designed as a backend API service focused on schema-per-tenant database operations and Brazilian fiscal document processing. The system provides RESTful APIs for tenant registration, schema management, and NFe document operations, but does not include a user interface component.

The primary interaction points are:

API Endpoints: RESTful APIs for tenant management, NFe processing, and schema operations
Administrative Tools: Command-line interfaces for schema migration and system maintenance
Monitoring Interfaces: Integration with external monitoring systems (Grafana, Prometheus) for observability
Database Management: Direct database access for schema validation and maintenance operations
All user interactions occur through programmatic API calls, making this a headless service designed for integration with external applications and systems that require NFe processing capabilities within a multi-tenant architecture.

8. Infrastructure
8.1 Deployment Environment
8.1.1 Target Environment Assessment
The NFe multi-tenant system with schema-per-tenant architecture requires a robust deployment environment that supports gradual rollout across tenants and addresses the unique challenges where technologies like Docker and Kubernetes are being used to simplify the deployment and management of multi-tenant databases. By containerizing each tenant's database instance, developers can easily spin up or down instances as required, reducing complexity.

Environment Type Selection

Environment Type	Suitability	Justification	Implementation Approach
Cloud-Native (Primary)	Excellent	Storing each tenant's data in a separate database instance in the cloud is becoming more popular. This approach allows for easy scaling up or down as needed, providing flexibility and cost-effectiveness	Kubernetes with PostgreSQL operators
Hybrid Cloud	Good	Regulatory compliance for Brazilian NFe requirements	On-premises PostgreSQL with cloud orchestration
On-Premises	Limited	Higher operational overhead for schema-per-tenant	Traditional VM-based deployment
Multi-Cloud	Complex	Vendor lock-in avoidance	Cross-cloud Kubernetes federation
Geographic Distribution Requirements

The system must comply with Brazilian data sovereignty requirements for NFe processing while supporting distributed deployment patterns:

Primary Region: Brazil (São Paulo) for NFe compliance and SEFAZ integration
Secondary Region: Brazil (Rio de Janeiro) for disaster recovery
Edge Locations: Regional data centers for improved latency
Compliance Zones: Dedicated infrastructure for regulated data processing
Resource Requirements Analysis

Based on the schema-per-tenant architecture where there's a clear difference between having 1 tenant with 10 million rows of data and having 1,000 tenants with 10 million rows each. That single month-end report is now going to run 1,000 times over:

Resource Category	Specification	Scaling Factor	Monitoring Threshold
Compute (CPU)	4-8 vCPUs per 100 tenants	Linear scaling with tenant count	> 80% utilization
Memory (RAM)	16-32 GB per 100 tenants	Schema metadata caching requirements	> 85% utilization
Storage (SSD)	1-5 TB per 100 tenants	NFe document volume and retention	> 80% capacity
Network	10 Gbps minimum	Multi-tenant query patterns	> 70% bandwidth
Compliance and Regulatory Requirements

The Brazilian NFe ecosystem imposes specific infrastructure requirements:

Data Residency: All NFe data must remain within Brazilian borders
Encryption Standards: AES-256 encryption for data at rest and in transit
Audit Logging: Complete audit trails for all fiscal document operations
Backup Retention: 5-year minimum retention for NFe documents
High Availability: 99.9% uptime requirement for fiscal document processing
8.1.2 Environment Management
The environment management strategy addresses the complexity of schema-per-tenant deployments where whether tenant-schema is bad for migrations/schema changes really depends on how you're doing them. It's bad for rolling out a universal schema change quickly, but good for deploying schema changes as a gradual rollout across tenants.

Infrastructure as Code (IaC) Approach

Infrastructure as Code

Terraform Core

Kubernetes Manifests

Helm Charts

PostgreSQL Cluster

Network Configuration

Security Groups

Namespace Management

RBAC Configuration

Resource Quotas

NFe Application

PostgreSQL Operator

Monitoring Stack

IaC Component	Technology	Purpose	Tenant Impact
Infrastructure Provisioning	Terraform 1.6+	Cloud resource management	Tenant-aware resource allocation
Container Orchestration	Kubernetes 1.28+	Application deployment	Schema-per-tenant isolation
Application Packaging	Helm 3.12+	Multi-tenant application deployment	Tenant-specific configurations
Configuration Management	Kustomize	Environment-specific customization	Per-environment tenant settings
Configuration Management Strategy

The configuration management addresses the challenge that when I build a docker image I have given the following commands FROM rust RUN apt update RUN apt install -y libpq-dev RUN cargo install diesel_cli --no-default-features --features postgres requires careful dependency management:

# Example Kubernetes ConfigMap for multi-tenant NFe system
apiVersion: v1
kind: ConfigMap
metadata:
  name: nfe-config
  namespace: nfe-system
data:
  DATABASE_URL: "postgres://nfe_user:${DB_PASSWORD}@postgres-cluster:5432/nfe_db"
  RUST_LOG: "info"
  MAX_TENANTS_PER_INSTANCE: "500"
  SCHEMA_CREATION_TIMEOUT: "30s"
  MIGRATION_BATCH_SIZE: "10"
Environment Promotion Strategy

Environment	Purpose	Tenant Count	Promotion Criteria
Development	Feature development	5-10 test tenants	Automated tests pass
Staging	Integration testing	50-100 test tenants	Performance benchmarks met
Pre-Production	Load testing	200-500 test tenants	Security scans clean
Production	Live NFe processing	1000+ active tenants	Manual approval required
Backup and Disaster Recovery Plans

The backup strategy addresses your disaster recovery plan must encompass all the steps to restore data as quickly as possible after an incident. This ensures that your database is not just backed up but also recoverable in a timely and efficient manner:

Monitoring
DR Site
Backup Storage
Primary PostgreSQL
Backup Scheduler
Monitoring
DR Site
Backup Storage
Primary PostgreSQL
Backup Scheduler
Per-tenant schema isolation maintained
Cross-region replication for compliance
Initiate Schema-Level Backup
Stream WAL Files
Create Base Backup per Schema
Replicate to DR Site
Confirm Backup Integrity
Report Backup Status
8.2 Cloud Services
8.2.1 Cloud Provider Selection And Justification
The cloud provider selection prioritizes Brazilian data sovereignty requirements and PostgreSQL schema-per-tenant optimization capabilities.

Primary Cloud Provider: AWS (São Paulo Region)

Service Category	AWS Service	Justification	Multi-Tenant Benefit
Compute	EKS (Elastic Kubernetes Service)	Managed Kubernetes with PostgreSQL operators	Tenant-aware pod scheduling
Database	RDS PostgreSQL / Aurora PostgreSQL	RDS provides automated backups that capture the entire DB instance and transaction logs. This allows you to restore to any point within the retention period, up to the last five minutes of database use	Schema-level backup granularity
Storage	EBS GP3 / EFS	High-performance storage for PostgreSQL	Per-tenant storage allocation
Networking	VPC with Private Subnets	Network isolation for tenant data	Schema-level network policies
Secondary Cloud Provider: Google Cloud (São Paulo Region)

Service Category	GCP Service	Purpose	Implementation
Disaster Recovery	Cloud SQL PostgreSQL	Google Cloud SQL performs automated daily backups of your PostgreSQL database, and these are retained for a configurable amount of time. You can also configure the backup start time and set up binary logging for more granular point-in-time recovery	Cross-cloud DR strategy
Container Registry	Artifact Registry	Multi-region container image storage	Rust application images
Monitoring	Cloud Operations	Centralized logging and monitoring	Multi-tenant observability
8.2.2 Core Services Required
Kubernetes Infrastructure Services

# PostgreSQL Operator for Schema-per-Tenant Management
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: nfe-postgres-cluster
spec:
  instances: 3
  postgresql:
    parameters:
      max_connections: "1000"
      shared_preload_libraries: "pg_stat_statements"
      log_statement: "all"
  storage:
    size: 1Ti
    storageClass: fast-ssd
  monitoring:
    enabled: true
Service Type	Implementation	Version	Multi-Tenant Configuration
PostgreSQL Operator	CloudNativePG	1.21+	Schema-per-tenant automation
Ingress Controller	NGINX Ingress	1.8+	Tenant-aware routing
Service Mesh	Istio	1.19+	Inter-service security
Monitoring	Prometheus + Grafana	Latest	Tenant-specific dashboards
8.2.3 High Availability Design
The high availability design addresses high availability relies on redundancy – having a backup server ready to take over in case of failure. All servers must be synchronized with the latest data to achieve high availability and fast disaster recovery. This is where replication matters. Whether physically or logically, maintaining data consistency across multiple servers ensures a smooth transition in case of failure.

Multi-Zone PostgreSQL Cluster Architecture

Load Balancer

Primary PostgreSQL Zone A

Standby PostgreSQL Zone B

Standby PostgreSQL Zone C

Tenant Schema 1-333

Tenant Schema 334-666

Tenant Schema 667-1000

WAL Streaming

Backup Storage

Schema-Level Backups

WAL Archive

Availability Targets

Component	Availability Target	Recovery Time Objective	Recovery Point Objective
NFe Application	99.95%	< 5 minutes	< 1 minute
PostgreSQL Cluster	99.99%	< 2 minutes	< 30 seconds
Schema Creation Service	99.9%	< 10 minutes	< 5 minutes
Monitoring Systems	99.95%	< 3 minutes	Real-time
8.2.4 Cost Optimization Strategy
The cost optimization strategy considers that resources such as CPU, memory, and disk space are not shared between tenants. With a single PostgreSQL server and multiple databases, there is a risk that some connections may remain underutilized while other tenants are unable to access the database due to a lack of available connections. This can occur when a client creates a connection pool against a specific database.

Resource Optimization Matrix

Optimization Area	Strategy	Expected Savings	Implementation
Compute Resources	Right-sizing based on tenant usage	25-30%	Kubernetes HPA with custom metrics
Storage Costs	Tiered storage for NFe retention	40-50%	Automated lifecycle policies
Network Costs	Regional data locality	15-20%	Tenant-aware data placement
Backup Costs	Incremental schema-level backups	35-45%	Differential backup strategies
Cost Monitoring and Alerting

Cost Monitoring

Per-Tenant Cost Allocation

Resource Utilization Tracking

Optimization Recommendations

Schema Storage Costs

Compute Usage per Tenant

Network Transfer Costs

CPU Utilization Alerts

Memory Usage Patterns

Storage Growth Trends

Right-sizing Suggestions

Reserved Instance Opportunities

Cleanup Recommendations

8.3 Containerization
8.3.1 Container Platform Selection
The containerization strategy addresses the Rust and Diesel-specific requirements where we'll explore how to make testing in Axum, to deploy our Rust backend using Docker, ensuring that it's ready for production use. Along the way, we'll delve into the fundamentals of structuring a modular, maintainable codebase and guide you through the process of containerizing your application with Docker for easy deployment.

Docker and Kubernetes Selection Rationale

Platform Component	Technology	Version	Multi-Tenant Justification
Container Runtime	containerd	1.7+	Efficient Rust binary execution
Container Platform	Docker	24.0+	containerized basic rust TODO application using actix web, postgres, diesel ORM
Orchestration	Kubernetes	1.28+	Schema-per-tenant pod isolation
Registry	AWS ECR / Harbor	Latest	Multi-region image distribution
8.3.2 Base Image Strategy
The base image strategy optimizes for Rust application deployment with PostgreSQL connectivity requirements:

Multi-Stage Dockerfile for NFe Application

# Build Stage - Based on search results for Rust + Diesel + PostgreSQL
FROM rust:1.86.0 as builder

#### Install PostgreSQL development libraries
RUN apt-get update && \
    apt-get install -y \
        libpq-dev \
        pkg-config \
        && rm -rf /var/lib/apt/lists/*

#### Install Diesel CLI for migrations
RUN cargo install diesel_cli --no-default-features --features postgres

WORKDIR /app

#### Copy dependency files first for better caching
COPY Cargo.toml Cargo.lock ./
COPY migrations/ ./migrations/

#### Create dummy main.rs for dependency caching
RUN mkdir -p src && \
    echo "fn main() {}" > src/main.rs && \
    cargo build --release && \
    rm -rf src

#### Copy actual source code
COPY src/ ./src/
COPY .env ./

#### Build the application
RUN touch src/main.rs && \
    cargo build --release

#### Runtime Stage
FROM debian:bookworm-slim

#### Install runtime dependencies for PostgreSQL
RUN apt-get update && \
    apt-get install -y \
        libpq5 \
        ca-certificates \
        && rm -rf /var/lib/apt/lists/*

#### Create non-root user
RUN useradd -r -s /bin/false nfe-user

#### Copy binary from builder stage
COPY --from=builder /app/target/release/nfe-system /usr/local/bin/
COPY --from=builder /app/migrations /app/migrations

#### Set ownership and permissions
RUN chown -R nfe-user:nfe-user /app
USER nfe-user

EXPOSE 8080
CMD ["nfe-system"]
Base Image Specifications

Image Type	Base Image	Size Target	Security Features
Builder Image	rust:1.86.0	N/A (build-time only)	Latest security patches
Runtime Image	debian:bookworm-slim	< 100MB	Minimal attack surface
PostgreSQL Client	libpq5	< 10MB	Official Debian packages
Diesel CLI	Custom build	< 50MB	Embedded migrations support
8.3.3 Image Versioning Approach
The versioning strategy supports schema-per-tenant deployment patterns with gradual rollout capabilities:

Semantic Versioning for Multi-Tenant Deployments

Version Component	Format	Purpose	Example
Major Version	X.0.0	Breaking schema changes	2.0.0 (NFe 5.0 support)
Minor Version	X.Y.0	New tenant features	1.5.0 (new tax calculations)
Patch Version	X.Y.Z	Bug fixes, security updates	1.4.3 (connection pool fix)
Build Metadata	X.Y.Z+build	CI/CD build information	1.4.3+20241201.1
Container Image Tagging Strategy

# Example image tags for different deployment scenarios
images:
  production: "nfe-system:1.4.3"
  staging: "nfe-system:1.5.0-rc.1"
  development: "nfe-system:main-20241201.1"
  schema-migration: "nfe-system:1.4.3-migration"
8.3.4 Build Optimization Techniques
The build optimization addresses Rust-specific compilation requirements and multi-tenant deployment efficiency:

Rust Build Optimization

Optimization Technique	Implementation	Benefit	Multi-Tenant Impact
Dependency Caching	Multi-stage Docker builds	70% faster builds	Faster tenant onboarding
Target Caching	Persistent build cache	50% faster incremental builds	Rapid schema updates
Binary Stripping	`strip` command in Dockerfile	30% smaller images	Faster container startup
Parallel Compilation	`CARGO_BUILD_JOBS` environment	40% faster builds	Reduced CI/CD time
Container Layer Optimization

# Optimized layer structure for caching
COPY Cargo.toml Cargo.lock ./          # Dependencies (rarely change)
COPY migrations/ ./migrations/          # Schema definitions (change occasionally)
COPY src/ ./src/                       # Application code (change frequently)
8.3.5 Security Scanning Requirements
The security scanning strategy ensures compliance with Brazilian NFe security requirements:

Multi-Layer Security Scanning

Scan Type	Tool	Frequency	Compliance Requirement
Base Image Vulnerabilities	Trivy	Every build	Zero critical vulnerabilities
Dependency Scanning	cargo-audit	Daily	No known security advisories
Container Configuration	Hadolint	Every build	CIS Docker Benchmark compliance
Runtime Security	Falco	Continuous	Behavioral anomaly detection
Security Scan Integration

# GitHub Actions security scanning workflow
name: Security Scan
on: [push, pull_request]
jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'nfe-system:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
      - name: Run cargo audit
        run: cargo audit
8.4 Orchestration
8.4.1 Orchestration Platform Selection
Kubernetes is selected as the orchestration platform due to its mature ecosystem for PostgreSQL management and multi-tenant application deployment patterns that support schema-per-tenant architectures.

Kubernetes Distribution Selection

Distribution	Suitability	Justification	Multi-Tenant Benefits
Amazon EKS	Excellent	Managed service with AWS integration	Native PostgreSQL RDS integration
Google GKE	Good	Strong PostgreSQL operator support	running PostgreSQL on Kubernetes for High Availability, Disaster Recovery, and Scaling
Azure AKS	Good	Enterprise compliance features	Built-in monitoring and logging
Self-Managed	Limited	Higher operational overhead	Full control over tenant isolation
8.4.2 Cluster Architecture
The cluster architecture supports schema-per-tenant isolation while maintaining operational efficiency:

Multi-Tenant Kubernetes Cluster Design

Worker Nodes - Zone C

Worker Nodes - Zone B

Worker Nodes - Zone A

Control Plane

API Server

etcd Cluster

Controller Manager

Scheduler

NFe App Pods 1-3

PostgreSQL Primary

Monitoring Agents

NFe App Pods 4-6

PostgreSQL Standby

Backup Services

NFe App Pods 7-9

PostgreSQL Standby

Migration Services

Cluster Specifications

Component	Specification	Scaling Strategy	Tenant Isolation
Control Plane	3 masters across AZs	Managed by cloud provider	RBAC-based access control
Worker Nodes	6-12 nodes per cluster	Horizontal pod autoscaling	Namespace-based isolation
Node Pools	Dedicated pools for workload types	Node auto-scaling	Tenant-aware scheduling
Network Policy	Calico/Cilium CNI	Micro-segmentation	Schema-level network isolation
8.4.3 Service Deployment Strategy
The deployment strategy addresses the gradual rollout requirements for schema-per-tenant systems:

Deployment Patterns for Multi-Tenant NFe System

Deployment Pattern	Use Case	Implementation	Tenant Impact
Blue-Green	Major version updates	Istio traffic splitting	Zero-downtime schema migrations
Canary	Feature rollouts	Flagger progressive delivery	good for deploying schema changes as a gradual rollout across tenants
Rolling Update	Regular updates	Kubernetes native	Minimal tenant disruption
Per-Tenant	Critical tenant isolation	Dedicated namespaces	Complete tenant separation
Helm Chart Structure for NFe System

# values.yaml for multi-tenant NFe deployment
global:
  environment: production
  region: sa-east-1

nfeApplication:
  replicaCount: 6
  image:
    repository: nfe-system
    tag: "1.4.3"
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2000m
      memory: 4Gi

postgresql:
  enabled: true
  architecture: replication
  primary:
    persistence:
      size: 1Ti
  readReplicas:
    replicaCount: 2

tenantManagement:
  maxTenantsPerInstance: 500
  schemaCreationTimeout: 30s
  migrationBatchSize: 10
8.4.4 Auto-scaling Configuration
The auto-scaling configuration addresses the variable load patterns of multi-tenant NFe processing:

Horizontal Pod Autoscaler (HPA) Configuration

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nfe-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nfe-application
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: active_tenant_connections
      target:
        type: AverageValue
        averageValue: "100"
Vertical Pod Autoscaler (VPA) for PostgreSQL

Resource Type	Scaling Policy	Minimum	Maximum	Update Mode
CPU	Based on query load	2 cores	16 cores	Auto
Memory	Based on connection count	4 GB	64 GB	Auto
Storage	Based on tenant growth	100 GB	10 TB	Manual approval
8.4.5 Resource Allocation Policies
The resource allocation addresses the challenge that resources such as CPU, memory, and disk space are not shared between tenants, with risk that some connections may remain underutilized while other tenants are unable to access the database:

Resource Quotas and Limits

apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-resource-quota
  namespace: nfe-tenants
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    requests.storage: 1Ti
    limits.cpu: "100"
    limits.memory: 200Gi
    persistentvolumeclaims: "50"
    count/pods: "200"
Quality of Service Classes

QoS Class	Tenant Type	Resource Guarantee	Use Case
Guaranteed	Premium tenants	CPU/Memory requests = limits	Critical NFe processing
Burstable	Standard tenants	CPU/Memory requests < limits	Normal NFe operations
BestEffort	Development tenants	No resource requests/limits	Testing and development
8.5 Ci/cd Pipeline
8.5.1 Build Pipeline
The build pipeline addresses Rust-specific compilation requirements and multi-tenant deployment patterns:

Source Control Triggers and Workflow

No

Yes

Git Push/PR

Trigger CI Pipeline

Checkout Code

Rust Toolchain Setup

Dependency Caching

Cargo Build

Run Tests

Security Scans

All Checks Pass?

Fail Build

Build Docker Image

Push to Registry

Trigger Deployment

Build Environment Requirements

Component	Specification	Purpose	Multi-Tenant Consideration
Rust Toolchain	1.86.0+	Diesel requires Rust 1.86.0 or later. If you're following along with this guide, make sure you're using at least that version of Rust by running rustup update stable	Schema compilation validation
PostgreSQL Client	libpq-dev	sudo apt install libpq //for the PostgreSQL backend	Database connectivity testing
Diesel CLI	Latest	diesel_cli is a tool which is used to interact with your database schema. Running Database Migrations scripts. Generating Rust Code for Your Database Schema	Schema migration validation
Docker	24.0+	Container image building	Multi-tenant image optimization
Dependency Management Strategy

# GitHub Actions workflow for Rust + Diesel + PostgreSQL
name: Build and Test
on: [push, pull_request]

jobs:
  build:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4
      
      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: 1.86.0
          
      - name: Install PostgreSQL client
        run: sudo apt-get install libpq-dev
        
      - name: Install Diesel CLI
        run: cargo install diesel_cli --no-default-features --features postgres
        
      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          
      - name: Run tests
        run: cargo test
        env:
          DATABASE_URL: postgres://postgres:postgres@localhost/test_db
8.5.2 Deployment Pipeline
The deployment pipeline implements gradual rollout strategies suitable for schema-per-tenant architectures:

Deployment Strategy Implementation

Stage	Environment	Validation	Tenant Impact
Development	Dev cluster	Unit tests, integration tests	5-10 test tenants
Staging	Staging cluster	Performance tests, schema validation	50-100 test tenants
Canary	Production cluster	5% traffic, monitoring	50-100 production tenants
Production	Production cluster	Full rollout	All production tenants
Environment Promotion Workflow

Monitoring
Production
Canary
Staging
Development
Monitoring
Production
Canary
Staging
Development
Multi-tenant schema validation
Gradual tenant rollout
Promote after tests pass
Run schema migration tests
Deploy to 5% of tenants
Collect metrics
Validate success criteria
Full production rollout
Continuous monitoring
Rollback Procedures

The rollback strategy addresses the complexity of schema-per-tenant deployments:

Rollback Trigger	Automated Response	Manual Intervention	Recovery Time
Application Errors	Automatic rollback to previous version	None required	< 5 minutes
Schema Migration Failure	Per-tenant rollback isolation	DBA review required	< 15 minutes
Performance Degradation	Traffic shifting to previous version	Performance analysis	< 10 minutes
Security Issues	Immediate rollback and isolation	Security team involvement	< 2 minutes
8.5.3 Post-deployment Validation
The validation strategy ensures multi-tenant system integrity after deployments:

Automated Validation Checks

# Post-deployment validation script
apiVersion: batch/v1
kind: Job
metadata:
  name: post-deployment-validation
spec:
  template:
    spec:
      containers:
      - name: validator
        image: nfe-system:latest
        command: ["/bin/sh"]
        args:
        - -c
        - |
          # Validate schema creation
          ./validate-schema-creation.sh
          
          # Test tenant isolation
          ./test-tenant-isolation.sh
          
          # Verify NFe processing
          ./test-nfe-processing.sh
          
          # Check monitoring endpoints
          ./validate-monitoring.sh
      restartPolicy: Never
Validation Criteria

Validation Type	Success Criteria	Timeout	Failure Action
Schema Creation	< 30 seconds per tenant	60 seconds	Rollback deployment
Tenant Isolation	Zero cross-tenant data access	120 seconds	Security alert
NFe Processing	< 5 seconds processing time	300 seconds	Performance investigation
Monitoring Health	All metrics endpoints responding	60 seconds	Alert operations team
8.5.4 Release Management Process
The release management addresses the unique challenges of multi-tenant schema evolution:

Release Planning Matrix

Release Type	Frequency	Tenant Impact	Approval Required
Hotfix	As needed	Minimal	Automated
Minor Release	Bi-weekly	Feature additions	Team lead approval
Major Release	Quarterly	Schema changes	Architecture review
Security Release	Immediate	Security patches	Security team approval
Feature Flag Management

// Example feature flag implementation for gradual tenant rollout
use feature_flags::FeatureFlag;

pub struct TenantFeatureManager {
    flags: HashMap<String, FeatureFlag>,
}

impl TenantFeatureManager {
    pub fn is_enabled(&self, tenant_id: &str, feature: &str) -> bool {
        self.flags
            .get(feature)
            .map(|flag| flag.is_enabled_for_tenant(tenant_id))
            .unwrap_or(false)
    }
    
    pub fn enable_for_tenant_percentage(&mut self, feature: &str, percentage: u8) {
        // Gradual rollout implementation
    }
}
8.6 Infrastructure Monitoring
8.6.1 Resource Monitoring Approach
The monitoring strategy addresses the unique requirements of schema-per-tenant architecture where barman show-backup servername Backup-ID provides detailed backup information including disk usage, WAL compression ratio, and replication status for PostgreSQL instances.

Multi-Tenant Resource Monitoring Architecture

Infrastructure Monitoring

Kubernetes Metrics

PostgreSQL Metrics

Application Metrics

Network Metrics

Node Resource Usage

Pod Performance

Cluster Health

Schema-Level Metrics

Connection Pool Status

Query Performance

Tenant-Specific Metrics

NFe Processing Rates

Schema Creation Times

Inter-Service Communication

External API Latency

Bandwidth Utilization

Resource Monitoring Specifications

Metric Category	Collection Method	Retention Period	Alert Thresholds
CPU Utilization	Prometheus node-exporter	30 days	> 80% for 5 minutes
Memory Usage	cAdvisor	30 days	> 85% for 3 minutes
Disk I/O	PostgreSQL pg_stat_io	7 days	> 90% utilization
Network Traffic	CNI metrics	14 days	> 70% bandwidth
8.6.2 Performance Metrics Collection
The performance metrics collection focuses on schema-per-tenant specific indicators:

PostgreSQL Schema-Level Monitoring

# Prometheus configuration for PostgreSQL schema monitoring
- job_name: 'postgres-exporter'
  static_configs:
    - targets: ['postgres-exporter:9187']
  metrics_path: /metrics
  params:
    collect[]:
      - pg_stat_database
      - pg_stat_user_tables
      - pg_stat_activity
  relabel_configs:
    - source_labels: [__address__]
      target_label: instance
    - source_labels: [datname]
      target_label: tenant_schema
Application Performance Metrics

Metric Name	Type	Labels	Purpose
`nfe_schema_creation_duration`	Histogram	tenant_id, status	Track schema creation performance
`nfe_tenant_connections_active`	Gauge	tenant_id, pool_name	Monitor connection pool usage
`nfe_document_processing_time`	Histogram	tenant_id, document_type	NFe processing performance
`nfe_migration_success_rate`	Counter	tenant_id, migration_version	Track migration success
8.6.3 Cost Monitoring And Optimization
The cost monitoring addresses the resource allocation challenges in schema-per-tenant deployments:

Cost Allocation by Tenant

Cost Monitoring

Compute Costs

Storage Costs

Network Costs

Backup Costs

Per-Tenant CPU Usage

Memory Allocation

Pod Resource Consumption

Schema Storage Size

WAL Archive Volume

Backup Storage Usage

Inter-AZ Traffic

External API Calls

CDN Usage

Schema-Level Backups

Cross-Region Replication

Long-term Retention

Cost Optimization Strategies

Cost Category	Optimization Strategy	Expected Savings	Implementation
Compute	Right-sizing based on tenant usage patterns	25-30%	Kubernetes VPA + custom metrics
Storage	Automated tiering for NFe retention	40-50%	Lifecycle policies + compression
Backup	Incremental schema-level backups	35-45%	Barman stands as a widely used open-source tool dedicated to managing backup and disaster recovery operations for PostgreSQL databases
Network	Regional data locality optimization	15-20%	Intelligent tenant placement
8.6.4 Security Monitoring
The security monitoring ensures compliance with Brazilian NFe security requirements:

Security Event Monitoring

Security Event	Detection Method	Response Time	Escalation
Unauthorized schema access	PostgreSQL audit logs	< 1 minute	Security team alert
Suspicious query patterns	Query analysis	< 5 minutes	Automated blocking
Failed authentication attempts	Application logs	< 2 minutes	Rate limiting
Data exfiltration attempts	Network monitoring	< 30 seconds	Immediate isolation
Compliance Monitoring Dashboard

# Grafana dashboard configuration for compliance monitoring
apiVersion: v1
kind: ConfigMap
metadata:
  name: compliance-dashboard
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "NFe Compliance Monitoring",
        "panels": [
          {
            "title": "Schema Access Violations",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(postgres_unauthorized_access_total[5m]))"
              }
            ]
          },
          {
            "title": "Backup Compliance Status",
            "type": "table",
            "targets": [
              {
                "expr": "barman_backup_status by (tenant_schema)"
              }
            ]
          }
        ]
      }
    }
8.6.5 Compliance Auditing
The compliance auditing ensures adherence to Brazilian fiscal regulations and data protection requirements:

Audit Trail Requirements

Audit Category	Data Collected	Retention Period	Access Control
Schema Operations	Creation, modification, deletion	7 years	DBA and compliance only
NFe Processing	Document creation, status changes	5 years	Tenant and audit access
User Access	Authentication, authorization events	3 years	Security team access
System Changes	Configuration, deployment events	2 years	Operations team access
Automated Compliance Reporting

Notification Service
Audit Storage
Compliance Analyzer
Data Collector
Report Scheduler
Notification Service
Audit Storage
Compliance Analyzer
Data Collector
Report Scheduler
Schema-level audit aggregation
Brazilian NFe compliance validation
Trigger Daily Compliance Report
Gather Audit Data
Return Audit Events
Process Compliance Data
Check Compliance Rules
Send Compliance Status
Generate Compliance Report
8.7 Infrastructure Cost Estimates
8.7.1 Monthly Cost Breakdown
The cost estimates are based on a production deployment supporting 1,000 active tenants with schema-per-tenant architecture:

AWS Infrastructure Costs (São Paulo Region)

Service Category	Service	Configuration	Monthly Cost (USD)	Annual Cost (USD)
Compute	EKS Cluster	6 m5.2xlarge nodes	$1,440	$17,280
Database	RDS PostgreSQL	db.r6g.2xlarge Multi-AZ	$1,200	$14,400
Storage	EBS GP3	10TB for schemas + backups	$800	$9,600
Networking	VPC + Load Balancer	Standard configuration	$200	$2,400
**Total Base Infrastructure**			**$3,640**	**$43,680**
Additional Services

Service	Purpose	Monthly Cost (USD)	Annual Cost (USD)
Container Registry	Image storage	$50	$600
Monitoring (CloudWatch)	Metrics and logs	$300	$3,600
Backup Storage (S3)	Schema-level backups	$400	$4,800
Data Transfer	Cross-AZ and internet	$150	$1,800
**Total Additional Services**		**$900**	**$10,800**
Total Monthly Infrastructure Cost: $4,540 USD
Total Annual Infrastructure Cost: $54,480 USD

8.7.2 Scaling Cost Projections
The scaling projections account for tenant growth and resource optimization:

Cost per Tenant Analysis

Tenant Count	Monthly Infrastructure Cost	Cost per Tenant	Optimization Savings
500 tenants	$3,200	$6.40	Baseline
1,000 tenants	$4,540	$4.54	29% efficiency gain
2,000 tenants	$7,200	$3.60	44% efficiency gain
5,000 tenants	$15,000	$3.00	53% efficiency gain
Growth Projection Model

Current: 1,000 Tenants

Year 1: 2,500 Tenants

Infrastructure Scaling

Year 2: 5,000 Tenants

Cost Optimization

Year 3: 10,000 Tenants

Reserved Instances

Multi-Region Expansion

Spot Instance Usage

Cost per Tenant: $2.50

8.7.3 Cost Optimization Opportunities
Reserved Instance Savings

Resource Type	On-Demand Cost	Reserved (1-year)	Reserved (3-year)	Savings
EKS Compute Nodes	$1,440/month	$1,008/month	$864/month	30-40%
RDS PostgreSQL	$1,200/month	$840/month	$720/month	30-40%
EBS Storage	$800/month	$640/month	$560/month	20-30%
Automated Cost Optimization

Optimization Strategy	Implementation	Expected Savings	Timeline
Right-sizing	Kubernetes VPA + monitoring	25% compute costs	3 months
Storage tiering	Automated lifecycle policies	40% storage costs	6 months
Spot instances	Mixed instance types	60% compute costs	9 months
Regional optimization	Data locality improvements	15% network costs	12 months
8.8 External Dependencies
8.8.1 Third-party Services
The external dependencies support the Brazilian NFe ecosystem and multi-tenant operations:

Critical External Services

Service Category	Provider	Purpose	SLA Requirement
SEFAZ Integration	Brazilian Government	NFe authorization and validation	99.5% availability
Certificate Authority	ICP-Brasil	Digital certificate management	99.9% availability
Tax Rate Services	Receita Federal	Real-time tax rate updates	99.0% availability
Address Validation	Correios API	Brazilian address validation	95.0% availability
Service Integration Architecture

NFe Multi-Tenant System

SEFAZ Webservices

ICP-Brasil CA

Tax Rate APIs

Address Validation

NFe Authorization

Status Consultation

Cancellation Services

Certificate Validation

Certificate Renewal

ICMS Rates

IPI Rates

PIS/COFINS Rates

CEP Validation

Municipality Codes

8.8.2 Resource Sizing Guidelines
The resource sizing addresses the specific requirements of schema-per-tenant PostgreSQL deployments:

PostgreSQL Resource Sizing

Tenant Count	CPU Cores	Memory (GB)	Storage (TB)	Connection Limit
100 tenants	4 cores	16 GB	1 TB	200 connections
500 tenants	8 cores	32 GB	5 TB	500 connections
1,000 tenants	16 cores	64 GB	10 TB	1,000 connections
2,000 tenants	32 cores	128 GB	20 TB	2,000 connections
Application Resource Sizing

Component	CPU Request	CPU Limit	Memory Request	Memory Limit
NFe Application Pod	500m	2000m	1Gi	4Gi
Schema Migration Job	1000m	4000m	2Gi	8Gi
Monitoring Agent	100m	500m	256Mi	1Gi
Backup Service	500m	1000m	512Mi	2Gi
8.8.3 Maintenance Procedures
The maintenance procedures ensure system reliability and compliance with Brazilian NFe requirements:

Scheduled Maintenance Windows

Maintenance Type	Frequency	Duration	Tenant Impact
PostgreSQL Updates	Monthly	2-4 hours	Rolling updates, minimal downtime
Kubernetes Upgrades	Quarterly	4-6 hours	Blue-green deployment
Security Patches	As needed	1-2 hours	Immediate deployment
Schema Migrations	Bi-weekly	Variable	gradual rollout across tenants
Disaster Recovery Procedures

The disaster recovery strategy addresses disaster recovery (DR) is the maintaining or reestablishing of vital infrastructure and systems after a natural or human-induced disaster. Fault tolerance – A fault-tolerant design enables a system to continue its operations, possibly at a reduced level, rather than failing when some part of the system malfunctions:

Recovery Team
Backup System
DR Coordinator
Monitoring System
Disaster Event
Recovery Team
Backup System
DR Coordinator
Monitoring System
Disaster Event
Schema-per-tenant recovery isolation
Brazilian compliance validation
System Failure Detected
Trigger DR Procedures
Initiate Schema Recovery
Provide Recovery Data
Execute Recovery Plan
Validate System Health
Confirm Recovery Success
Recovery Time Objectives

Failure Scenario	Detection Time	Recovery Time	Data Loss (RPO)
Single node failure	< 1 minute	< 5 minutes	< 30 seconds
Database corruption	< 5 minutes	< 30 minutes	< 5 minutes
Regional outage	< 10 minutes	< 2 hours	< 15 minutes
Complete disaster	< 30 minutes	< 8 hours	< 1 hour
The Infrastructure architecture for the NFe multi-tenant system provides a comprehensive foundation for deploying and managing schema-per-tenant PostgreSQL operations at scale. The cloud-native approach leverages Kubernetes orchestration, automated CI/CD pipelines, and robust monitoring to ensure reliable, compliant, and cost-effective operations for Brazilian fiscal document processing across hundreds or thousands of tenant schemas.

9. Appendices
9.1 Tenant Schema Creation Implementation
9.1.1 Dynamic Nfe Schema Creation Strategy
Based on the provided NFe 4.00 SQL schema and the multi-tenant architecture requirements, the optimal approach for creating tenant-specific schemas involves dynamic schema creation where you don't actually know the schema you're interacting with until runtime, while still being able to use Diesel's query builder, though many compile time guarantees are lost and we cannot verify that the tables/columns you ask for actually exist, or that the types you state are correct.

Tenant Registration and Schema Creation Flow

Tenant Registry
PostgreSQL
Schema Creator
Connection Validator
Tenant API
Tenant Registry
PostgreSQL
Schema Creator
Connection Validator
Tenant API
loop
[NFe Schema Objects]
Validate Tenant Connection
Test Connection String
Connection Status
Connection Validated
Create Tenant Schema
BEGIN TRANSACTION
CREATE SCHEMA tenant_uuid
CREATE EXTENSION uuid-ossp
CREATE TYPE enums
CREATE TABLE with schema prefix
CREATE INDEX with schema prefix
COMMIT TRANSACTION
Schema Created Successfully
Register Tenant Metadata
Tenant Registration Complete
9.1.2 Rust Implementation Architecture
The implementation leverages Diesel which requires Rust 1.86.0 or later, ensuring compatibility with the latest Rust stable version and addresses the challenge that tenant-schema is bad for rolling out a universal schema change quickly, but good for deploying schema changes as a gradual rollout across tenants.

Core Implementation Components

Component	Technology	Purpose	Multi-Tenant Consideration
Schema Template	Embedded SQL	NFe 4.00 complete structure	Schema-qualified table creation
Connection Manager	Diesel + r2d2	Tenant-specific connections	Resources such as CPU, memory, and disk space are not shared between tenants, with risk that some connections may remain underutilized while other tenants are unable to access the database due to connection pool limitations
Dynamic Schema Service	diesel-dynamic-schema	Runtime schema operations	Table function creation with explicit select clause requirements
Migration Engine	Diesel migrations	Schema evolution	Gradual tenant rollout capability
Tenant Schema Creation Implementation

use diesel::prelude::*;
use diesel::pg::PgConnection;
use diesel_dynamic_schema::table;
use uuid::Uuid;

pub struct TenantSchemaCreator {
    connection: PgConnection,
}

impl TenantSchemaCreator {
    pub fn create_tenant_schema(&mut self, tenant_id: Uuid) -> Result<String, SchemaCreationError> {
        let schema_name = format!("tenant_{}", tenant_id.to_string().replace("-", "_"));
        
        // Begin transaction for atomic schema creation
        self.connection.transaction::<_, SchemaCreationError, _>(|| {
            // Create schema namespace
            diesel::sql_query(format!("CREATE SCHEMA {}", schema_name))
                .execute(&mut self.connection)?;
            
            // Enable UUID extension in schema
            diesel::sql_query(format!("CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\" SCHEMA {}", schema_name))
                .execute(&mut self.connection)?;
            
            // Create all NFe enums and types
            self.create_nfe_enums(&schema_name)?;
            
            // Create all NFe tables
            self.create_nfe_tables(&schema_name)?;
            
            // Create indexes
            self.create_nfe_indexes(&schema_name)?;
            
            // Validate schema completeness
            self.validate_schema_structure(&schema_name)?;
            
            Ok(schema_name)
        })
    }
    
    fn create_nfe_enums(&mut self, schema_name: &str) -> Result<(), SchemaCreationError> {
        // Create UF type
        diesel::sql_query(format!(
            "CREATE TYPE {}.uf_type AS ENUM ('AC', 'AL', 'AM', 'AP', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MG', 'MS', 'MT', 'PA', 'PB', 'PE', 'PI', 'PR', 'RJ', 'RN', 'RO', 'RR', 'RS', 'SC', 'SE', 'SP', 'TO', 'EX')",
            schema_name
        )).execute(&mut self.connection)?;
        
        // Create all other NFe enums following the same pattern
        self.create_status_nfe_enum(schema_name)?;
        self.create_tax_enums(schema_name)?;
        
        Ok(())
    }
    
    fn create_nfe_tables(&mut self, schema_name: &str) -> Result<(), SchemaCreationError> {
        // Create empresas table
        diesel::sql_query(format!(
            "CREATE TABLE {}.empresas (
                id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
                cnpj VARCHAR(14) NOT NULL UNIQUE,
                cpf VARCHAR(11),
                razao_social VARCHAR(60) NOT NULL,
                -- ... complete NFe empresas structure
                created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
            )",
            schema_name
        )).execute(&mut self.connection)?;
        
        // Create all other NFe tables with schema qualification
        self.create_pessoas_table(schema_name)?;
        self.create_produtos_table(schema_name)?;
        self.create_nfe_table(schema_name)?;
        self.create_nfe_item_table(schema_name)?;
        self.create_tax_tables(schema_name)?;
        
        Ok(())
    }
}
9.1.3 Connection Validation And Pool Management
The connection management strategy addresses shared-table works better for connection pooling, as all connections can use the same pool, but schema-per-tenant requires specialized handling.

Tenant Connection Validation

pub struct TenantConnectionValidator {
    base_connection_string: String,
}

impl TenantConnectionValidator {
    pub async fn validate_tenant_connection(&self, tenant_config: &TenantConfig) -> Result<(), ValidationError> {
        // Test basic connectivity
        let mut connection = PgConnection::establish(&tenant_config.database_url)
            .map_err(|e| ValidationError::ConnectionFailed(e.to_string()))?;
        
        // Verify PostgreSQL version compatibility
        let version: String = diesel::sql_query("SELECT version()")
            .get_result(&mut connection)?;
        
        if !self.is_compatible_version(&version) {
            return Err(ValidationError::IncompatibleVersion(version));
        }
        
        // Test schema creation permissions
        let test_schema = format!("test_schema_{}", Uuid::new_v4());
        diesel::sql_query(format!("CREATE SCHEMA {}", test_schema))
            .execute(&mut connection)?;
        diesel::sql_query(format!("DROP SCHEMA {}", test_schema))
            .execute(&mut connection)?;
        
        Ok(())
    }
}
9.1.4 Dynamic Schema Operations With Diesel
The implementation addresses the table function used to create new Diesel tables, noting that you must always provide an explicit select clause when using this crate.

Runtime Schema Access Pattern

use diesel_dynamic_schema::table;

pub struct TenantDataAccess {
    schema_name: String,
    connection_pool: Pool<ConnectionManager<PgConnection>>,
}

impl TenantDataAccess {
    pub fn get_nfe_by_chave(&self, chave_acesso: &str) -> Result<NfeData, DatabaseError> {
        // Create dynamic table reference with schema qualification
        let nfe_table = table(&format!("{}.nfe", self.schema_name));
        let chave_column = nfe_table.column::<diesel::sql_types::Text, _>("chave_acesso");
        let id_column = nfe_table.column::<diesel::sql_types::Uuid, _>("id");
        let status_column = nfe_table.column::<diesel::sql_types::Text, _>("status");
        
        let mut conn = self.connection_pool.get()?;
        
        // Explicit select clause required for dynamic schema operations
        nfe_table
            .select((id_column, chave_column, status_column))
            .filter(chave_column.eq(chave_acesso))
            .first::<(Uuid, String, String)>(&mut conn)
            .map(|(id, chave, status)| NfeData { id, chave_acesso: chave, status })
    }
    
    pub fn create_nfe(&self, nfe_data: &NewNfeData) -> Result<NfeData, DatabaseError> {
        let nfe_table = table(&format!("{}.nfe", self.schema_name));
        let mut conn = self.connection_pool.get()?;
        
        // Insert with schema-qualified table reference
        diesel::insert_into(nfe_table)
            .values(nfe_data)
            .returning((
                nfe_table.column::<diesel::sql_types::Uuid, _>("id"),
                nfe_table.column::<diesel::sql_types::Text, _>("chave_acesso"),
                nfe_table.column::<diesel::sql_types::Text, _>("status")
            ))
            .get_result::<(Uuid, String, String)>(&mut conn)
            .map(|(id, chave, status)| NfeData { id, chave_acesso: chave, status })
    }
}
9.1.5 Migration Strategy For Multi-tenant Schemas
The migration approach leverages Diesel CLI tool's --diff-schema option on the diesel migration generate command that allows generating migrations based on the current schema definition and database.

Gradual Migration Deployment

pub struct TenantMigrationEngine {
    migration_scripts: Vec<EmbeddedMigration>,
}

impl TenantMigrationEngine {
    pub async fn deploy_migration_to_tenants(
        &self, 
        migration_id: &str, 
        tenant_batch: Vec<TenantId>
    ) -> Result<MigrationReport, MigrationError> {
        let mut results = Vec::new();
        
        for tenant_id in tenant_batch {
            let result = self.deploy_to_single_tenant(migration_id, tenant_id).await;
            results.push((tenant_id, result));
            
            // Stop on first failure to prevent cascade issues
            if results.last().unwrap().1.is_err() {
                break;
            }
        }
        
        Ok(MigrationReport { results })
    }
    
    async fn deploy_to_single_tenant(
        &self, 
        migration_id: &str, 
        tenant_id: TenantId
    ) -> Result<(), MigrationError> {
        let schema_name = format!("tenant_{}", tenant_id);
        let mut connection = self.get_tenant_connection(tenant_id).await?;
        
        connection.transaction::<_, MigrationError, _>(|| {
            // Apply migration with schema qualification
            for migration in &self.migration_scripts {
                let schema_qualified_sql = migration.sql.replace("public.", &format!("{}.", schema_name));
                diesel::sql_query(schema_qualified_sql).execute(&mut connection)?;
            }
            
            // Update migration version tracking
            self.update_tenant_migration_version(tenant_id, migration_id)?;
            
            Ok(())
        })
    }
}
9.2 Glossary
9.2.1 Multi-tenancy Terms
Term	Definition
**Schema-per-Tenant**	A shared database, but each tenant gets a separate schema, balancing isolation and scalability well
**Tenant Isolation**	The fundamental requirement that one tenant should not be able to see or interact with any other tenant's data
**Dynamic Schema**	Schema interaction where you don't actually know the schema you're interacting with until runtime
**Connection Pool**	Resource management system where connections may remain underutilized while other tenants are unable to access the database due to pool limitations
9.2.2 Nfe-specific Terms
Term	Definition
**NFe (Nota Fiscal Eletrônica)**	Brazilian electronic invoice system mandated by government for fiscal document processing
**SEFAZ**	State Treasury Department responsible for NFe authorization and validation
**Chave de Acesso**	44-character unique access key for each NFe document
**CNPJ/CPF**	Brazilian company (CNPJ) and individual (CPF) tax identification numbers
9.2.3 Technical Terms
Term	Definition
**Diesel ORM**	High level query builder that lets you think about problems in Rust, not SQL, with zero-cost abstractions and reusable code design
**diesel-dynamic-schema**	Crate providing tools for runtime schema cases while using Diesel's query builder, though many compile time guarantees are lost
**Schema Qualification**	Table function creation requiring explicit select clauses when using dynamic schema operations
**Migration Rollout**	Gradual deployment of schema changes across tenants rather than universal changes
9.3 Acronyms
9.3.1 Technology Acronyms
Acronym	Expansion	Context
**ORM**	Object-Relational Mapping	Software that aims to solve the issue of using SQL directly by letting you map objects in your code to SQL
**CLI**	Command Line Interface	Tool used to interact with database schema, running database migration scripts and generating Rust code for database schema
**UUID**	Universally Unique Identifier	Primary key generation for tenant and entity identification
**CRUD**	Create, Read, Update, Delete	Basic database operations
**API**	Application Programming Interface	RESTful endpoints for tenant and NFe operations
**JWT**	JSON Web Token	Authentication token format with tenant context
**TLS**	Transport Layer Security	Encryption protocol for secure connections
**RBAC**	Role-Based Access Control	Authorization framework for multi-tenant access
9.3.2 Brazilian Fiscal Acronyms
Acronym	Expansion	Context
**NFe**	Nota Fiscal Eletrônica	Brazilian electronic invoice system
**SEFAZ**	Secretaria da Fazenda	State Treasury Department
**CNPJ**	Cadastro Nacional da Pessoa Jurídica	Brazilian company tax ID (14 digits)
**CPF**	Cadastro de Pessoas Físicas	Brazilian individual tax ID (11 digits)
**ICMS**	Imposto sobre Circulação de Mercadorias e Serviços	State tax on goods and services
**IPI**	Imposto sobre Produtos Industrializados	Federal tax on industrialized products
**PIS**	Programa de Integração Social	Social integration program tax
**COFINS**	Contribuição para o Financiamento da Seguridade Social	Social security financing contribution
**NCM**	Nomenclatura Comum do Mercosul	Mercosur common nomenclature for products
**CFOP**	Código Fiscal de Operações e Prestações	Fiscal code for operations and services
9.3.3 Infrastructure Acronyms
Acronym	Expansion	Context
**CI/CD**	Continuous Integration/Continuous Deployment	Automated build and deployment pipeline
**SLA**	Service Level Agreement	Performance and availability commitments
**RTO**	Recovery Time Objective	Maximum acceptable downtime
**RPO**	Recovery Point Objective	Maximum acceptable data loss
**MTTR**	Mean Time To Resolution	Average incident resolution time
**MTTD**	Mean Time To Detection	Average incident detection time
**HA**	High Availability	System uptime and redundancy design
**DR**	Disaster Recovery	Business continuity and data recovery procedures
9.4 Performance Considerations
9.4.1 Schema-per-tenant Performance Characteristics
The performance implications of schema-per-tenant architecture require careful consideration, as there are specific issues with large numbers of tables, namely administrative tasks like backup and VACUUM, though by "large numbers of tables" we're talking 10s or 100s of thousands, not a few hundred, and those issues don't affect SELECT queries.

Performance Optimization Matrix

Optimization Area	Implementation	Expected Benefit	Multi-Tenant Impact
Query Performance	Schema-qualified queries	Eliminates search_path overhead	Performance issues were with the application, not queries on individual tables
Connection Pooling	Tenant-specific pools	Reduced connection contention	Better for connection pooling when all connections use the same pool
Index Optimization	Per-tenant index strategies	Smaller indexes on tenant data	Indexes on smaller tables tend to be lighter than indexes on larger tables
Administrative Tasks	Parallel operations	Reduced maintenance windows	Problems with VACUUM and other PostgreSQL administrative operations which scale poorly across 1000's of tables
9.4.2 Scaling Characteristics
The scaling behavior differs significantly based on tenant distribution patterns, where shared-table works better for situations when you have a lot of tenants with very little data, like social media mobile applications with thousands of abandoned accounts, while schema-per-tenant excels in different scenarios.

Tenant Distribution Analysis

Tenant Profile	Schema-per-Tenant Suitability	Performance Characteristics
Few Large Tenants (< 100)	Excellent	Smaller tables with lighter indexes
Medium Tenants (100-1000)	Good	Balanced resource utilization
Many Small Tenants (> 1000)	Limited	Shared-table approach more suitable
9.5 Compliance And Regulatory Requirements
9.5.1 Brazilian Nfe Compliance Framework
The NFe system must comply with comprehensive Brazilian fiscal regulations, requiring specific data structures, retention policies, and audit capabilities as defined in the NFe 4.00 specification.

Compliance Requirements Matrix

Compliance Area	Requirement	Implementation	Validation Method
Data Retention	5-year minimum for NFe documents	Schema-level archival policies	Automated compliance reporting
Audit Trails	Complete operation logging	Immutable audit logs per tenant	Regulatory audit procedures
Data Sovereignty	Brazilian data residency	Geographic infrastructure constraints	Data location verification
Access Control	Role-based permissions	Schema-level access restrictions	Permission audit trails
9.5.2 Security And Privacy Controls
The multi-tenant architecture implements comprehensive security measures to ensure tenant data isolation and regulatory compliance.

Security Control Framework

Security Layer	Implementation	Tenant Isolation Method	Compliance Standard
Database Level	PostgreSQL schema isolation	Complete schema separation	Brazilian data protection laws
Application Level	Tenant-aware access control	JWT token validation	Industry security standards
Network Level	Encrypted connections	TLS 1.3 encryption	International security protocols
Audit Level	Comprehensive logging	Per-tenant audit trails	Fiscal audit requirements
9.6 Operational Procedures
9.6.1 Tenant Onboarding Process
The tenant onboarding process ensures proper validation, schema creation, and system integration while maintaining security and compliance standards.

Onboarding Workflow Steps

Tenant Registration Validation

Connection string verification
Database permissions testing
Compliance requirements validation
Schema Creation Process

Atomic schema generation
Complete NFe 4.00 structure deployment
Index and constraint creation
Connection Pool Initialization

Tenant-specific pool configuration
Resource allocation and limits
Health monitoring setup
Validation and Testing

Schema structure verification
Basic operation testing
Performance baseline establishment
9.6.2 Maintenance And Monitoring Procedures
Regular maintenance procedures ensure system reliability and performance across all tenant schemas.

Maintenance Schedule

Task	Frequency	Scope	Automation Level
Schema Health Checks	Daily	All tenant schemas	Fully automated
Performance Monitoring	Continuous	Per-tenant metrics	Automated with alerts
Backup Verification	Weekly	Schema-level backups	Automated with reporting
Migration Deployment	As needed	Gradual tenant rollout	Semi-automated with approval
The Appendices section provides comprehensive technical information supporting the NFe multi-tenant system implementation, covering dynamic schema creation strategies, performance considerations, compliance requirements, and operational procedures essential for successful deployment and maintenance of the schema-per-tenant architecture.